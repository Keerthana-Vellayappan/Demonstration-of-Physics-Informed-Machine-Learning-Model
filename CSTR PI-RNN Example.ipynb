{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "832feb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551708c1",
   "metadata": {},
   "source": [
    "# EarlyStopping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d0fd46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23b1aa92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'using cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tells whether the model is running on CPU or GPU\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('using GPU:', torch.cuda.get_device_name()) if torch.cuda.is_available() else 'using cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424399e1",
   "metadata": {},
   "source": [
    "# Specifying parameters for CSTR and other constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53772680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter values for a second-order reaction taking place in a CSTR\n",
    "\n",
    "T_0 = 300 # inlet temperature\n",
    "\n",
    "V = 1 # volume of reacting liquid in the reactor\n",
    "\n",
    "k_0 = 8.46 * (np.power(10,6)) # pre-exponential constant\n",
    "\n",
    "C_p = 0.231 # heat capacity of reacting liquid\n",
    "\n",
    "rho_L = 1000 # density of reacting liquid\n",
    "\n",
    "Q_s = 0.0 # steady-state heat input rate\n",
    "\n",
    "T_s = 402 # steady-state reactor temperature\n",
    "\n",
    "F = 5 # volumetric flow rate\n",
    "\n",
    "E = 5 * (np.power(10,4)) # activation energy\n",
    "\n",
    "delta_H = -1.15 * (np.power(10,4)) # enthalpy of reaction\n",
    "\n",
    "R = 8.314 # ideal gas constant\n",
    "\n",
    "C_A0s = 4 #  steady-state inlet concentration of A\n",
    "\n",
    "C_As = 1.95 # steady-state reactor concentration of A\n",
    "\n",
    "t_final = 0.05 #0.01 # end time for numerical simulation\n",
    "\n",
    "t_step = 0.01 #1e-3 # integration time step h_c\n",
    "\n",
    "P = np.array([[1060, 22], [22, 0.52]]) # a positive definite matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbebe71",
   "metadata": {},
   "source": [
    "# Euler method RK45 (use Scipy or this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8b034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that uses Euler method to return the values of concentration and temperature of the next time-step\n",
    "\n",
    "def CSTR_simulation_Euler(F, V, C_A0, k_0, E, R, T_0, delta_H, rho_L, C_p, Q, C_As, T_s, t_final, t_step, C_A_initial, T_initial):\n",
    "    \n",
    "    def dCAdt(C_A, T):\n",
    "        return F / V * (C_A0 - C_A) - k_0 * np.exp(-E / (R * T)) * C_A**2\n",
    "    def dTdt(C_A, T):\n",
    "        return F / V * (T_0 - T) - delta_H / (rho_L * C_p) * k_0 * np.exp(-E / (R * T)) * C_A**2 + Q / (rho_L * C_p * V)\n",
    "    \n",
    "    C_A = C_A_initial + C_As\n",
    "    T = T_initial + T_s\n",
    "    dCAdt1 = dCAdt(C_A, T)\n",
    "    dTdt1 = dTdt(C_A, T)\n",
    "    \n",
    "    C_A_2_1 = C_A + dCAdt1 * t_step / 2\n",
    "    T_2_1 = T + dTdt1 * t_step / 2\n",
    "    dCAdt2_1 = dCAdt(C_A_2_1, T_2_1)\n",
    "    dTdt2_1 = dTdt(C_A_2_1, T_2_1)\n",
    "    \n",
    "    C_A_2_2 = C_A + dCAdt2_1 * t_step / 2\n",
    "    T_2_2 = T + dTdt2_1 * t_step / 2\n",
    "    dCAdt2_2 = dCAdt(C_A_2_2, T_2_2)\n",
    "    dTdt2_2 = dTdt(C_A_2_2, T_2_2)\n",
    "    \n",
    "    C_A_3 = C_A + dCAdt2_2 * t_step\n",
    "    T_3 = T + dTdt2_2 * t_step\n",
    "    dCAdt3 = dCAdt(C_A_3, T_3)\n",
    "    dTdt3 =  dTdt(C_A_3, T_3)\n",
    "    \n",
    "    dCAdt2 = (dCAdt2_1 + dCAdt2_2) / 2\n",
    "    dTdt2 = (dTdt2_1 + dTdt2_2) / 2\n",
    "    \n",
    "    C_A_3 = C_A + t_step / 6 * (dCAdt1 + 4*dCAdt2 + dCAdt3)     \n",
    "    T_3 = T + t_step / 6 * (dTdt1 + 4*dTdt2 + dTdt3)\n",
    "\n",
    "    return C_A_3 - C_As, T_3 - T_s  # in deviation form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee7570",
   "metadata": {},
   "source": [
    "# Data generation (PI-RNN) collocation points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a002e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating inputs and initial states for CSTR, all expressed in deviation form\n",
    "\n",
    "u1_physics_list = np.linspace(-3.5, 3.5, 10) # u1 is the inlet concentration of species A\n",
    "u2_physics_list = np.linspace(-5e5, 5e5, 10) # u2 is the heat input rate\n",
    "T_physics_initial = np.linspace(300, 500, 10) - T_s # inlet temperature\n",
    "CA_physics_initial = np.linspace(0, 5, 10) - C_As # inlet concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5db1f3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of initial conditions: 28\n",
      "shape of x_physics_original is (28, 2)\n"
     ]
    }
   ],
   "source": [
    "# sieve out initial states that lie outside of stability region\n",
    "\n",
    "T_physics_start = list()\n",
    "CA_physics_start = list()\n",
    "\n",
    "for T in T_physics_initial:\n",
    "    for CA in CA_physics_initial:\n",
    "        x = np.array([CA, T])\n",
    "        if x @ P @ x < 1000:\n",
    "            CA_physics_start.append(CA)\n",
    "            T_physics_start.append(T)\n",
    "            \n",
    "print(\"number of initial conditions: {}\".format(len(CA_physics_start)))\n",
    "\n",
    "# convert to np.arrays\n",
    "CA_physics_start = np.array([CA_physics_start])\n",
    "T_physics_start = np.array([T_physics_start])\n",
    "x_physics_original = np.concatenate((CA_physics_start.T, T_physics_start.T), axis=1)  # every row is a pair of initial states within stability region\n",
    "print(\"shape of x_physics_original is {}\".format(x_physics_original.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "608f72a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 out of 10\n",
      "2 out of 10\n",
      "3 out of 10\n",
      "4 out of 10\n",
      "5 out of 10\n",
      "6 out of 10\n",
      "7 out of 10\n",
      "8 out of 10\n",
      "9 out of 10\n",
      "10 out of 10\n"
     ]
    }
   ],
   "source": [
    "# get X and y data for physics-informed loss\n",
    "\n",
    "CA_physics_output = list()\n",
    "T_physics_output = list()\n",
    "CA_physics_input = list()\n",
    "T_physics_input = list()\n",
    "CA0_physics_input = list()\n",
    "Q_physics_input = list()\n",
    "\n",
    "for num_id, u1 in enumerate(u1_physics_list):\n",
    "    print(f\"{num_id + 1} out of {u1_physics_list.shape[0]}\")    #just to count and keep track\n",
    "    C_A0 = u1 + C_A0s\n",
    "    \n",
    "    for u2 in u2_physics_list:\n",
    "        Q = u2 + Q_s\n",
    "        \n",
    "        for C_A_initial, T_initial in x_physics_original:\n",
    "            CA0_physics_input.append(u1)\n",
    "            Q_physics_input.append(u2)\n",
    "            CA_physics_input.append(C_A_initial)\n",
    "            T_physics_input.append(T_initial)\n",
    "            C_A_list = [C_A_initial]\n",
    "            T_list = [T_initial]\n",
    "            \n",
    "                        \n",
    "            for _ in range(int(t_final / t_step)):\n",
    "                CA_next, T_next = CSTR_simulation_Euler(F, V, C_A0, k_0, E, R, T_0, delta_H, rho_L, C_p, Q, C_As, T_s, t_final, t_step, C_A_initial, T_initial)\n",
    "                C_A_list.append(CA_next)\n",
    "                T_list.append(T_next)\n",
    "                C_A_initial = CA_next\n",
    "                T_initial = T_next\n",
    "            \n",
    "            CA_physics_output.append(C_A_list)\n",
    "            T_physics_output.append(T_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36fe5111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_physics_input_temp shape is (2800, 6, 4)\n",
      "RNN_physics_output shape is (2800, 6, 2)\n",
      "X_train shape is (1680, 6, 4), X_val shape is (560, 6, 4), X_test shape is (560, 6, 4)\n",
      "y_train shape is (1680, 6, 2), y_val shape is (560, 6, 2), y_test shape is (560, 6, 2)\n",
      "tensor([0.0421, 0.9233])\n"
     ]
    }
   ],
   "source": [
    "# collate input for RNN for physics loss\n",
    "\n",
    "CA0_physics_input = np.array(CA0_physics_input)\n",
    "CA0_physics_input = CA0_physics_input.reshape(-1,1,1)\n",
    "\n",
    "\n",
    "Q_physics_input = np.array(Q_physics_input)\n",
    "Q_physics_input = Q_physics_input.reshape(-1,1,1)\n",
    "\n",
    "\n",
    "CA_physics_input = np.array(CA_physics_input)\n",
    "CA_physics_input = CA_physics_input.reshape(-1,1,1)\n",
    "\n",
    "\n",
    "T_physics_input = np.array(T_physics_input)\n",
    "T_physics_input = T_physics_input.reshape(-1,1,1)\n",
    "\n",
    "\n",
    "RNN_physics_input_temp = np.concatenate((CA0_physics_input, Q_physics_input, CA_physics_input, T_physics_input), axis=2)\n",
    "\n",
    "\"\"\"\n",
    "    the input to RNN is in the shape [number of samples x timestep x variables], and the input variables are same for every\n",
    "    time step\n",
    "\"\"\"\n",
    "\n",
    "RNN_physics_input_temp = RNN_physics_input_temp.repeat(6, axis=1)\n",
    "print(\"RNN_physics_input_temp shape is {}\".format(RNN_physics_input_temp.shape))\n",
    "\n",
    "############################## collate output for RNN ####################################################\n",
    "\n",
    "CA_physics_output = np.array(CA_physics_output)\n",
    "CA_physics_output = CA_physics_output.reshape(-1, 6, 1)\n",
    "\n",
    "T_physics_output = np.array(T_physics_output)\n",
    "T_physics_output = T_physics_output.reshape(-1, 6, 1)\n",
    "\n",
    "RNN_physics_output = np.concatenate((CA_physics_output, T_physics_output), axis=2)\n",
    "print(\"RNN_physics_output shape is {}\".format(RNN_physics_output.shape))  # output shape: number of samples x timestep x variables\n",
    "\n",
    "############################# Split Train, Test, and Validation dataset ##################################\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(RNN_physics_input_temp, RNN_physics_output, test_size=0.2, random_state=123)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=123) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "print(f\"X_train shape is {X_train.shape}, X_val shape is {X_val.shape}, X_test shape is {X_test.shape}\")\n",
    "print(f\"y_train shape is {y_train.shape}, y_val shape is {y_val.shape}, y_test shape is {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32825d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mean and standard deviation to standardize both the input and output data\n",
    "\n",
    "mean_CA0 = np.mean(X_train[:, 0, 0].reshape(-1))\n",
    "std_CA0 = np.std(X_train[:, 0, 0].reshape(-1))\n",
    "\n",
    "mean_Q = np.mean(X_train[:, 0, 1].reshape(-1))\n",
    "std_Q = np.std(X_train[:, 0, 1].reshape(-1))\n",
    "\n",
    "mean_CA_input = np.mean(X_train[:, 0, -2].reshape(-1))\n",
    "std_CA_input = np.std(X_train[:, 0, -2].reshape(-1))\n",
    "\n",
    "mean_T_input = np.mean(X_train[:, 0, -1].reshape(-1))\n",
    "std_T_input = np.std(X_train[:, 0, -1].reshape(-1))\n",
    "\n",
    "# mean and standard deviation of the input data is used to scale the output data\n",
    "\n",
    "mean_y = np.concatenate((mean_CA_input.reshape(-1), mean_T_input.reshape(-1)))\n",
    "std_y = np.concatenate((std_CA_input.reshape(-1), std_T_input.reshape(-1)))\n",
    "\n",
    "mean_y = torch.from_numpy(mean_y).float()\n",
    "std_y = torch.from_numpy(std_y).float()\n",
    "\n",
    "print(mean_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82fe45e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_physics shape is: torch.Size([1680, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "# scale the input of the Traininng, Testing, and Validation dataset\n",
    "\n",
    "X_physics_train = (X_train - [mean_CA0, mean_Q, mean_CA_input, mean_T_input]) / [std_CA0, std_Q, std_CA_input, std_T_input]\n",
    "X_physics_train = torch.from_numpy(X_physics_train).float()\n",
    "\n",
    "X_physics_val = (X_val - [mean_CA0, mean_Q, mean_CA_input, mean_T_input]) / [std_CA0, std_Q, std_CA_input, std_T_input]\n",
    "X_physics_val = torch.from_numpy(X_physics_val).float()\n",
    "\n",
    "X_physics_test = (X_test - [mean_CA0, mean_Q, mean_CA_input, mean_T_input]) / [std_CA0, std_Q, std_CA_input, std_T_input]\n",
    "X_physics_test = torch.from_numpy(X_physics_test).float()\n",
    "\n",
    "y_physics_test = torch.from_numpy(y_test).float().view(-1, 6, 2)\n",
    "# y_physics = (y_physics - mean_y) / (std_y)\n",
    "\n",
    "print(f'X_physics shape is: {X_physics_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954379a",
   "metadata": {},
   "source": [
    "# Preparing dataset for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0704ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_physics_train = TensorDataset(X_physics_train)\n",
    "dataloader_physics_train = DataLoader(dataset_physics_train, batch_size=2048, shuffle=True)\n",
    "\n",
    "dataset_physics_val = TensorDataset(X_physics_val)\n",
    "dataloader_physics_val = DataLoader(dataset_physics_val, batch_size=2048, shuffle=True)\n",
    "\n",
    "dataset_physics_test = TensorDataset(X_physics_test)\n",
    "dataloader_physics_test = DataLoader(dataset_physics_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9897beae",
   "metadata": {},
   "source": [
    "# Defining RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fc02715",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"Defines a RNN network\"\n",
    "    \n",
    "    def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):\n",
    "        super(RNN, self).__init__()\n",
    "        self.layers = N_LAYERS\n",
    "        \n",
    "        if isinstance(N_HIDDEN, list):\n",
    "            self.rnn = nn.LSTM(N_INPUT, \n",
    "                                N_HIDDEN[0], \n",
    "                                batch_first=True)\n",
    "            \n",
    "            self.rnn1 = nn.ModuleList(\n",
    "                [nn.LSTM(N_HIDDEN[i], \n",
    "                        N_HIDDEN[i+1],\n",
    "                       batch_first=True) for i in range(N_LAYERS - 1)]\n",
    "            )\n",
    "            \n",
    "            self.output_layer = nn.Linear(N_HIDDEN[-1], N_OUTPUT)\n",
    "            \n",
    "            self.list_flag = True\n",
    "            \n",
    "        else:\n",
    "            self.rnn = nn.LSTM(N_INPUT, \n",
    "                                N_HIDDEN,\n",
    "                                N_LAYERS,\n",
    "                                batch_first=True,\n",
    "                                dropout=0.1)\n",
    "            \n",
    "            self.output_layer = nn.Linear(N_HIDDEN, N_OUTPUT)\n",
    "            \n",
    "            self.list_flag = False\n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        if self.list_flag:\n",
    "            for i in range(self.layers - 1):\n",
    "                x, _ = self.rnn1[i](x)\n",
    "                \n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4463dcc5",
   "metadata": {},
   "source": [
    "# Physics-informed RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17ffc6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, patience, n_epochs):\n",
    "    \n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for batch, x_batch in enumerate(dataloader_physics_train, 1):\n",
    "            # clear the gradients of all optimized variables\n",
    "            x_batch = x_batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            NN_output = model_PINN(x_batch)        \n",
    "\n",
    "            loss1 = torch.mean((NN_output[:, 0, :] - x_batch[:, 0, -2:])**2)  # use mean squared error\n",
    "\n",
    "            # compute the \"physics loss\"\n",
    "            \n",
    "            C_A0 = x_batch[:, :, 0] * std_CA0 + mean_CA0 + C_A0s\n",
    "            Q = x_batch[:, :, 1] * std_Q + mean_Q + Q_s\n",
    "\n",
    "            NN_output = NN_output * std_y.to(device) + mean_y.to(device) + torch.from_numpy(np.array([C_As, T_s])).float().to(device)\n",
    "\n",
    "            dCA_first = (NN_output[:, 1:2, 0] - NN_output[:, 0:1, 0]) / (t_step)\n",
    "            dT_first = (NN_output[:, 1:2, 1] - NN_output[:, 0:1, 1]) / (t_step)\n",
    "\n",
    "            dCA_center = (NN_output[:, 2:, 0] - NN_output[:, :-2, 0]) / (2 * t_step)\n",
    "            dT_center = (NN_output[:, 2:, 1] - NN_output[:, :-2, 1]) / (2 * t_step)\n",
    "\n",
    "            dCA_last = (NN_output[:, -1:, 0] - NN_output[:, -2:-1, 0]) / (t_step)\n",
    "            dT_last = (NN_output[:, -1:, 1] - NN_output[:, -2:-1, 1]) / (t_step)\n",
    "\n",
    "\n",
    "            dCA = torch.cat((dCA_first, dCA_center, dCA_last), 1)\n",
    "            dT = torch.cat((dT_first, dT_center, dT_last), 1)\n",
    "\n",
    "\n",
    "            # Physics-based Concentration loss\n",
    "            loss3 = dCA - F / V * (C_A0 - NN_output[:, :, 0]) + k_0 * torch.exp(-E / (R * NN_output[:, :, 1])) * NN_output[:, :, 0]**2\n",
    "            loss3 = torch.mean(loss3**2)\n",
    "             \n",
    "            # Physics-based Temperature loss    \n",
    "            loss4 = dT - F / V * (T_0 - NN_output[:, :, 1]) + delta_H / (rho_L * C_p) * k_0 * torch.exp(-E / (R * NN_output[:, :, 1])) * NN_output[:, :, 0]**2 - Q / (rho_L * C_p * V)\n",
    "            loss4 = torch.mean(loss4**2)\n",
    "\n",
    "            # backpropagate joint loss using appropriate scaling factors\n",
    "            loss = 1e3 * loss1 + 1e-1 * loss3 + 1e-5 * loss4 # add all loss terms together\n",
    "            \n",
    "            \n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for val_batch in dataloader_physics_val:\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            val_batch = val_batch[0].to(device)\n",
    "            NN_output = model(val_batch)\n",
    "            \n",
    "            loss1 = torch.mean((NN_output[:, 0, :] - val_batch[:, 0, -2:])**2)  # use mean squared error\n",
    "\n",
    "            # compute the \"physics loss\"\n",
    "            \n",
    "            C_A0 = val_batch[:, :, 0] * std_CA0 + mean_CA0 + C_A0s\n",
    "            Q = val_batch[:, :, 1] * std_Q + mean_Q + Q_s\n",
    "\n",
    "            NN_output = NN_output * std_y.to(device) + mean_y.to(device) + torch.from_numpy(np.array([C_As, T_s])).float().to(device)\n",
    "\n",
    "            dCA_first = (NN_output[:, 1:2, 0] - NN_output[:, 0:1, 0]) / (t_step)\n",
    "            dT_first = (NN_output[:, 1:2, 1] - NN_output[:, 0:1, 1]) / (t_step)\n",
    "\n",
    "            dCA_center = (NN_output[:, 2:, 0] - NN_output[:, :-2, 0]) / (2 * t_step)\n",
    "            dT_center = (NN_output[:, 2:, 1] - NN_output[:, :-2, 1]) / (2 * t_step)\n",
    "\n",
    "            dCA_last = (NN_output[:, -1:, 0] - NN_output[:, -2:-1, 0]) / (t_step)\n",
    "            dT_last = (NN_output[:, -1:, 1] - NN_output[:, -2:-1, 1]) / (t_step)\n",
    "\n",
    "\n",
    "            dCA = torch.cat((dCA_first, dCA_center, dCA_last), 1)\n",
    "            dT = torch.cat((dT_first, dT_center, dT_last), 1)\n",
    "\n",
    "\n",
    "            # Physics-based Concentration loss\n",
    "            loss3 = dCA - F / V * (C_A0 - NN_output[:, :, 0]) + k_0 * torch.exp(-E / (R * NN_output[:, :, 1])) * NN_output[:, :, 0]**2\n",
    "            loss3 = torch.mean(loss3**2)\n",
    "\n",
    "            # Physics-based Temperature loss\n",
    "            loss4 = dT - F / V * (T_0 - NN_output[:, :, 1]) + delta_H / (rho_L * C_p) * k_0 * torch.exp(-E / (R * NN_output[:, :, 1])) * NN_output[:, :, 0]**2 - Q / (rho_L * C_p * V)\n",
    "            loss4 = torch.mean(loss4**2)\n",
    "\n",
    "            # backpropagate joint loss using appropriate scaling factors\n",
    "            loss = 1e3 * loss1 + 1e-1 * loss3 + 1e-5 * loss4 # add all loss terms together\n",
    "           \n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(valid_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ee48f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): LSTM(4, 32, num_layers=3, batch_first=True, dropout=0.1)\n",
      "  (output_layer): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "[   1/1000] train_loss: 1043.76477 valid_loss: 998.94617\n",
      "Validation loss decreased (inf --> 998.946167).  Saving model ...\n",
      "[   2/1000] train_loss: 1042.38171 valid_loss: 997.61823\n",
      "Validation loss decreased (998.946167 --> 997.618225).  Saving model ...\n",
      "[   3/1000] train_loss: 1041.02954 valid_loss: 996.36328\n",
      "Validation loss decreased (997.618225 --> 996.363281).  Saving model ...\n",
      "[   4/1000] train_loss: 1039.76758 valid_loss: 995.17444\n",
      "Validation loss decreased (996.363281 --> 995.174438).  Saving model ...\n",
      "[   5/1000] train_loss: 1038.64307 valid_loss: 994.04431\n",
      "Validation loss decreased (995.174438 --> 994.044312).  Saving model ...\n",
      "[   6/1000] train_loss: 1037.51196 valid_loss: 992.96570\n",
      "Validation loss decreased (994.044312 --> 992.965698).  Saving model ...\n",
      "[   7/1000] train_loss: 1036.39685 valid_loss: 991.93146\n",
      "Validation loss decreased (992.965698 --> 991.931458).  Saving model ...\n",
      "[   8/1000] train_loss: 1035.34985 valid_loss: 990.93469\n",
      "Validation loss decreased (991.931458 --> 990.934692).  Saving model ...\n",
      "[   9/1000] train_loss: 1034.37048 valid_loss: 989.96912\n",
      "Validation loss decreased (990.934692 --> 989.969116).  Saving model ...\n",
      "[  10/1000] train_loss: 1033.39868 valid_loss: 989.02820\n",
      "Validation loss decreased (989.969116 --> 989.028198).  Saving model ...\n",
      "[  11/1000] train_loss: 1032.49048 valid_loss: 988.10510\n",
      "Validation loss decreased (989.028198 --> 988.105103).  Saving model ...\n",
      "[  12/1000] train_loss: 1031.49463 valid_loss: 987.19403\n",
      "Validation loss decreased (988.105103 --> 987.194031).  Saving model ...\n",
      "[  13/1000] train_loss: 1030.60254 valid_loss: 986.28760\n",
      "Validation loss decreased (987.194031 --> 986.287598).  Saving model ...\n",
      "[  14/1000] train_loss: 1029.70178 valid_loss: 985.37927\n",
      "Validation loss decreased (986.287598 --> 985.379272).  Saving model ...\n",
      "[  15/1000] train_loss: 1028.77429 valid_loss: 984.46136\n",
      "Validation loss decreased (985.379272 --> 984.461365).  Saving model ...\n",
      "[  16/1000] train_loss: 1027.85974 valid_loss: 983.52667\n",
      "Validation loss decreased (984.461365 --> 983.526672).  Saving model ...\n",
      "[  17/1000] train_loss: 1026.80444 valid_loss: 982.56586\n",
      "Validation loss decreased (983.526672 --> 982.565857).  Saving model ...\n",
      "[  18/1000] train_loss: 1025.93884 valid_loss: 981.57025\n",
      "Validation loss decreased (982.565857 --> 981.570251).  Saving model ...\n",
      "[  19/1000] train_loss: 1024.90747 valid_loss: 980.53040\n",
      "Validation loss decreased (981.570251 --> 980.530396).  Saving model ...\n",
      "[  20/1000] train_loss: 1023.70844 valid_loss: 979.43622\n",
      "Validation loss decreased (980.530396 --> 979.436218).  Saving model ...\n",
      "[  21/1000] train_loss: 1022.49744 valid_loss: 978.27649\n",
      "Validation loss decreased (979.436218 --> 978.276489).  Saving model ...\n",
      "[  22/1000] train_loss: 1021.28992 valid_loss: 977.04059\n",
      "Validation loss decreased (978.276489 --> 977.040588).  Saving model ...\n",
      "[  23/1000] train_loss: 1020.04010 valid_loss: 975.71790\n",
      "Validation loss decreased (977.040588 --> 975.717896).  Saving model ...\n",
      "[  24/1000] train_loss: 1018.61157 valid_loss: 974.29651\n",
      "Validation loss decreased (975.717896 --> 974.296509).  Saving model ...\n",
      "[  25/1000] train_loss: 1017.15430 valid_loss: 972.76544\n",
      "Validation loss decreased (974.296509 --> 972.765442).  Saving model ...\n",
      "[  26/1000] train_loss: 1015.53619 valid_loss: 971.11255\n",
      "Validation loss decreased (972.765442 --> 971.112549).  Saving model ...\n",
      "[  27/1000] train_loss: 1013.76843 valid_loss: 969.32568\n",
      "Validation loss decreased (971.112549 --> 969.325684).  Saving model ...\n",
      "[  28/1000] train_loss: 1011.68927 valid_loss: 967.39185\n",
      "Validation loss decreased (969.325684 --> 967.391846).  Saving model ...\n",
      "[  29/1000] train_loss: 1009.95917 valid_loss: 965.29834\n",
      "Validation loss decreased (967.391846 --> 965.298340).  Saving model ...\n",
      "[  30/1000] train_loss: 1007.57111 valid_loss: 963.03052\n",
      "Validation loss decreased (965.298340 --> 963.030518).  Saving model ...\n",
      "[  31/1000] train_loss: 1005.23743 valid_loss: 960.57336\n",
      "Validation loss decreased (963.030518 --> 960.573364).  Saving model ...\n",
      "[  32/1000] train_loss: 1002.70300 valid_loss: 957.91119\n",
      "Validation loss decreased (960.573364 --> 957.911194).  Saving model ...\n",
      "[  33/1000] train_loss: 1000.13330 valid_loss: 955.02777\n",
      "Validation loss decreased (957.911194 --> 955.027771).  Saving model ...\n",
      "[  34/1000] train_loss: 997.23419 valid_loss: 951.90503\n",
      "Validation loss decreased (955.027771 --> 951.905029).  Saving model ...\n",
      "[  35/1000] train_loss: 993.67310 valid_loss: 948.52277\n",
      "Validation loss decreased (951.905029 --> 948.522766).  Saving model ...\n",
      "[  36/1000] train_loss: 990.04828 valid_loss: 944.86060\n",
      "Validation loss decreased (948.522766 --> 944.860596).  Saving model ...\n",
      "[  37/1000] train_loss: 986.61804 valid_loss: 940.89850\n",
      "Validation loss decreased (944.860596 --> 940.898499).  Saving model ...\n",
      "[  38/1000] train_loss: 982.35309 valid_loss: 936.61505\n",
      "Validation loss decreased (940.898499 --> 936.615051).  Saving model ...\n",
      "[  39/1000] train_loss: 977.94507 valid_loss: 931.98810\n",
      "Validation loss decreased (936.615051 --> 931.988098).  Saving model ...\n",
      "[  40/1000] train_loss: 973.22357 valid_loss: 926.99597\n",
      "Validation loss decreased (931.988098 --> 926.995972).  Saving model ...\n",
      "[  41/1000] train_loss: 967.37927 valid_loss: 921.61377\n",
      "Validation loss decreased (926.995972 --> 921.613770).  Saving model ...\n",
      "[  42/1000] train_loss: 962.34967 valid_loss: 915.82104\n",
      "Validation loss decreased (921.613770 --> 915.821045).  Saving model ...\n",
      "[  43/1000] train_loss: 956.51337 valid_loss: 909.59479\n",
      "Validation loss decreased (915.821045 --> 909.594788).  Saving model ...\n",
      "[  44/1000] train_loss: 949.92352 valid_loss: 902.91199\n",
      "Validation loss decreased (909.594788 --> 902.911987).  Saving model ...\n",
      "[  45/1000] train_loss: 943.76721 valid_loss: 895.75018\n",
      "Validation loss decreased (902.911987 --> 895.750183).  Saving model ...\n",
      "[  46/1000] train_loss: 936.54633 valid_loss: 888.08173\n",
      "Validation loss decreased (895.750183 --> 888.081726).  Saving model ...\n",
      "[  47/1000] train_loss: 928.08002 valid_loss: 879.87732\n",
      "Validation loss decreased (888.081726 --> 879.877319).  Saving model ...\n",
      "[  48/1000] train_loss: 919.71777 valid_loss: 871.10724\n",
      "Validation loss decreased (879.877319 --> 871.107239).  Saving model ...\n",
      "[  49/1000] train_loss: 910.51111 valid_loss: 861.74005\n",
      "Validation loss decreased (871.107239 --> 861.740051).  Saving model ...\n",
      "[  50/1000] train_loss: 901.08795 valid_loss: 851.74677\n",
      "Validation loss decreased (861.740051 --> 851.746765).  Saving model ...\n",
      "[  51/1000] train_loss: 890.58521 valid_loss: 841.09564\n",
      "Validation loss decreased (851.746765 --> 841.095642).  Saving model ...\n",
      "[  52/1000] train_loss: 879.79950 valid_loss: 829.75476\n",
      "Validation loss decreased (841.095642 --> 829.754761).  Saving model ...\n",
      "[  53/1000] train_loss: 868.73517 valid_loss: 817.69183\n",
      "Validation loss decreased (829.754761 --> 817.691833).  Saving model ...\n",
      "[  54/1000] train_loss: 856.08221 valid_loss: 804.87146\n",
      "Validation loss decreased (817.691833 --> 804.871460).  Saving model ...\n",
      "[  55/1000] train_loss: 842.36945 valid_loss: 791.25238\n",
      "Validation loss decreased (804.871460 --> 791.252380).  Saving model ...\n",
      "[  56/1000] train_loss: 829.13110 valid_loss: 776.79950\n",
      "Validation loss decreased (791.252380 --> 776.799500).  Saving model ...\n",
      "[  57/1000] train_loss: 813.11432 valid_loss: 761.46930\n",
      "Validation loss decreased (776.799500 --> 761.469299).  Saving model ...\n",
      "[  58/1000] train_loss: 796.85901 valid_loss: 745.22693\n",
      "Validation loss decreased (761.469299 --> 745.226929).  Saving model ...\n",
      "[  59/1000] train_loss: 781.96820 valid_loss: 728.04755\n",
      "Validation loss decreased (745.226929 --> 728.047546).  Saving model ...\n",
      "[  60/1000] train_loss: 761.82147 valid_loss: 709.90186\n",
      "Validation loss decreased (728.047546 --> 709.901855).  Saving model ...\n",
      "[  61/1000] train_loss: 744.76324 valid_loss: 690.77979\n",
      "Validation loss decreased (709.901855 --> 690.779785).  Saving model ...\n",
      "[  62/1000] train_loss: 726.60760 valid_loss: 670.67932\n",
      "Validation loss decreased (690.779785 --> 670.679321).  Saving model ...\n",
      "[  63/1000] train_loss: 705.21552 valid_loss: 649.60065\n",
      "Validation loss decreased (670.679321 --> 649.600647).  Saving model ...\n",
      "[  64/1000] train_loss: 684.38226 valid_loss: 627.55945\n",
      "Validation loss decreased (649.600647 --> 627.559448).  Saving model ...\n",
      "[  65/1000] train_loss: 663.25012 valid_loss: 604.58368\n",
      "Validation loss decreased (627.559448 --> 604.583679).  Saving model ...\n",
      "[  66/1000] train_loss: 638.24872 valid_loss: 580.71521\n",
      "Validation loss decreased (604.583679 --> 580.715210).  Saving model ...\n",
      "[  67/1000] train_loss: 611.26471 valid_loss: 556.00513\n",
      "Validation loss decreased (580.715210 --> 556.005127).  Saving model ...\n",
      "[  68/1000] train_loss: 590.22906 valid_loss: 530.53918\n",
      "Validation loss decreased (556.005127 --> 530.539185).  Saving model ...\n",
      "[  69/1000] train_loss: 564.06683 valid_loss: 504.41632\n",
      "Validation loss decreased (530.539185 --> 504.416321).  Saving model ...\n",
      "[  70/1000] train_loss: 537.42590 valid_loss: 477.75858\n",
      "Validation loss decreased (504.416321 --> 477.758575).  Saving model ...\n",
      "[  71/1000] train_loss: 511.81790 valid_loss: 450.71387\n",
      "Validation loss decreased (477.758575 --> 450.713867).  Saving model ...\n",
      "[  72/1000] train_loss: 480.11346 valid_loss: 423.44907\n",
      "Validation loss decreased (450.713867 --> 423.449066).  Saving model ...\n",
      "[  73/1000] train_loss: 455.22382 valid_loss: 396.15674\n",
      "Validation loss decreased (423.449066 --> 396.156738).  Saving model ...\n",
      "[  74/1000] train_loss: 425.16464 valid_loss: 369.05405\n",
      "Validation loss decreased (396.156738 --> 369.054047).  Saving model ...\n",
      "[  75/1000] train_loss: 400.52017 valid_loss: 342.37045\n",
      "Validation loss decreased (369.054047 --> 342.370453).  Saving model ...\n",
      "[  76/1000] train_loss: 370.50592 valid_loss: 316.35529\n",
      "Validation loss decreased (342.370453 --> 316.355286).  Saving model ...\n",
      "[  77/1000] train_loss: 348.90829 valid_loss: 291.25317\n",
      "Validation loss decreased (316.355286 --> 291.253174).  Saving model ...\n",
      "[  78/1000] train_loss: 323.50366 valid_loss: 267.29514\n",
      "Validation loss decreased (291.253174 --> 267.295135).  Saving model ...\n",
      "[  79/1000] train_loss: 296.63785 valid_loss: 244.69601\n",
      "Validation loss decreased (267.295135 --> 244.696014).  Saving model ...\n",
      "[  80/1000] train_loss: 275.21103 valid_loss: 223.62762\n",
      "Validation loss decreased (244.696014 --> 223.627625).  Saving model ...\n",
      "[  81/1000] train_loss: 254.52948 valid_loss: 204.22833\n",
      "Validation loss decreased (223.627625 --> 204.228333).  Saving model ...\n",
      "[  82/1000] train_loss: 236.18591 valid_loss: 186.61871\n",
      "Validation loss decreased (204.228333 --> 186.618713).  Saving model ...\n",
      "[  83/1000] train_loss: 215.65234 valid_loss: 170.88635\n",
      "Validation loss decreased (186.618713 --> 170.886353).  Saving model ...\n",
      "[  84/1000] train_loss: 201.93271 valid_loss: 157.09938\n",
      "Validation loss decreased (170.886353 --> 157.099380).  Saving model ...\n",
      "[  85/1000] train_loss: 187.79909 valid_loss: 145.30547\n",
      "Validation loss decreased (157.099380 --> 145.305466).  Saving model ...\n",
      "[  86/1000] train_loss: 173.54344 valid_loss: 135.51573\n",
      "Validation loss decreased (145.305466 --> 135.515732).  Saving model ...\n",
      "[  87/1000] train_loss: 165.98792 valid_loss: 127.66693\n",
      "Validation loss decreased (135.515732 --> 127.666931).  Saving model ...\n",
      "[  88/1000] train_loss: 159.27588 valid_loss: 121.60982\n",
      "Validation loss decreased (127.666931 --> 121.609818).  Saving model ...\n",
      "[  89/1000] train_loss: 150.62166 valid_loss: 117.12364\n",
      "Validation loss decreased (121.609818 --> 117.123642).  Saving model ...\n",
      "[  90/1000] train_loss: 147.68465 valid_loss: 113.90343\n",
      "Validation loss decreased (117.123642 --> 113.903435).  Saving model ...\n",
      "[  91/1000] train_loss: 144.62987 valid_loss: 111.65645\n",
      "Validation loss decreased (113.903435 --> 111.656448).  Saving model ...\n",
      "[  92/1000] train_loss: 141.21080 valid_loss: 110.09483\n",
      "Validation loss decreased (111.656448 --> 110.094826).  Saving model ...\n",
      "[  93/1000] train_loss: 138.52228 valid_loss: 108.98913\n",
      "Validation loss decreased (110.094826 --> 108.989128).  Saving model ...\n",
      "[  94/1000] train_loss: 140.40044 valid_loss: 108.13969\n",
      "Validation loss decreased (108.989128 --> 108.139694).  Saving model ...\n",
      "[  95/1000] train_loss: 136.66397 valid_loss: 107.38769\n",
      "Validation loss decreased (108.139694 --> 107.387688).  Saving model ...\n",
      "[  96/1000] train_loss: 135.38000 valid_loss: 106.59187\n",
      "Validation loss decreased (107.387688 --> 106.591866).  Saving model ...\n",
      "[  97/1000] train_loss: 134.10207 valid_loss: 105.68240\n",
      "Validation loss decreased (106.591866 --> 105.682396).  Saving model ...\n",
      "[  98/1000] train_loss: 133.26022 valid_loss: 104.60370\n",
      "Validation loss decreased (105.682396 --> 104.603699).  Saving model ...\n",
      "[  99/1000] train_loss: 129.38966 valid_loss: 103.33849\n",
      "Validation loss decreased (104.603699 --> 103.338493).  Saving model ...\n",
      "[ 100/1000] train_loss: 128.24220 valid_loss: 101.88354\n",
      "Validation loss decreased (103.338493 --> 101.883545).  Saving model ...\n",
      "[ 101/1000] train_loss: 125.29844 valid_loss: 100.27094\n",
      "Validation loss decreased (101.883545 --> 100.270935).  Saving model ...\n",
      "[ 102/1000] train_loss: 123.50372 valid_loss: 98.55748\n",
      "Validation loss decreased (100.270935 --> 98.557480).  Saving model ...\n",
      "[ 103/1000] train_loss: 122.88187 valid_loss: 96.76254\n",
      "Validation loss decreased (98.557480 --> 96.762535).  Saving model ...\n",
      "[ 104/1000] train_loss: 120.39863 valid_loss: 94.95634\n",
      "Validation loss decreased (96.762535 --> 94.956337).  Saving model ...\n",
      "[ 105/1000] train_loss: 115.14412 valid_loss: 93.20223\n",
      "Validation loss decreased (94.956337 --> 93.202232).  Saving model ...\n",
      "[ 106/1000] train_loss: 115.10397 valid_loss: 91.53968\n",
      "Validation loss decreased (93.202232 --> 91.539680).  Saving model ...\n",
      "[ 107/1000] train_loss: 110.71676 valid_loss: 89.99097\n",
      "Validation loss decreased (91.539680 --> 89.990974).  Saving model ...\n",
      "[ 108/1000] train_loss: 110.17313 valid_loss: 88.58091\n",
      "Validation loss decreased (89.990974 --> 88.580910).  Saving model ...\n",
      "[ 109/1000] train_loss: 107.23807 valid_loss: 87.32372\n",
      "Validation loss decreased (88.580910 --> 87.323723).  Saving model ...\n",
      "[ 110/1000] train_loss: 104.61892 valid_loss: 86.22210\n",
      "Validation loss decreased (87.323723 --> 86.222099).  Saving model ...\n",
      "[ 111/1000] train_loss: 106.20817 valid_loss: 85.27013\n",
      "Validation loss decreased (86.222099 --> 85.270134).  Saving model ...\n",
      "[ 112/1000] train_loss: 103.03499 valid_loss: 84.45425\n",
      "Validation loss decreased (85.270134 --> 84.454254).  Saving model ...\n",
      "[ 113/1000] train_loss: 101.73257 valid_loss: 83.76564\n",
      "Validation loss decreased (84.454254 --> 83.765640).  Saving model ...\n",
      "[ 114/1000] train_loss: 100.67832 valid_loss: 83.18710\n",
      "Validation loss decreased (83.765640 --> 83.187096).  Saving model ...\n",
      "[ 115/1000] train_loss: 100.74589 valid_loss: 82.69908\n",
      "Validation loss decreased (83.187096 --> 82.699081).  Saving model ...\n",
      "[ 116/1000] train_loss: 97.95193 valid_loss: 82.28362\n",
      "Validation loss decreased (82.699081 --> 82.283615).  Saving model ...\n",
      "[ 117/1000] train_loss: 97.74560 valid_loss: 81.91848\n",
      "Validation loss decreased (82.283615 --> 81.918480).  Saving model ...\n",
      "[ 118/1000] train_loss: 98.18353 valid_loss: 81.58813\n",
      "Validation loss decreased (81.918480 --> 81.588135).  Saving model ...\n",
      "[ 119/1000] train_loss: 99.06025 valid_loss: 81.27757\n",
      "Validation loss decreased (81.588135 --> 81.277565).  Saving model ...\n",
      "[ 120/1000] train_loss: 95.39452 valid_loss: 80.97770\n",
      "Validation loss decreased (81.277565 --> 80.977699).  Saving model ...\n",
      "[ 121/1000] train_loss: 95.57156 valid_loss: 80.67614\n",
      "Validation loss decreased (80.977699 --> 80.676140).  Saving model ...\n",
      "[ 122/1000] train_loss: 96.20651 valid_loss: 80.36894\n",
      "Validation loss decreased (80.676140 --> 80.368942).  Saving model ...\n",
      "[ 123/1000] train_loss: 95.00391 valid_loss: 80.04938\n",
      "Validation loss decreased (80.368942 --> 80.049377).  Saving model ...\n",
      "[ 124/1000] train_loss: 94.62228 valid_loss: 79.71854\n",
      "Validation loss decreased (80.049377 --> 79.718536).  Saving model ...\n",
      "[ 125/1000] train_loss: 94.14976 valid_loss: 79.37506\n",
      "Validation loss decreased (79.718536 --> 79.375061).  Saving model ...\n",
      "[ 126/1000] train_loss: 93.38614 valid_loss: 79.02311\n",
      "Validation loss decreased (79.375061 --> 79.023109).  Saving model ...\n",
      "[ 127/1000] train_loss: 93.11652 valid_loss: 78.66620\n",
      "Validation loss decreased (79.023109 --> 78.666199).  Saving model ...\n",
      "[ 128/1000] train_loss: 93.10758 valid_loss: 78.30283\n",
      "Validation loss decreased (78.666199 --> 78.302834).  Saving model ...\n",
      "[ 129/1000] train_loss: 92.76368 valid_loss: 77.94041\n",
      "Validation loss decreased (78.302834 --> 77.940414).  Saving model ...\n",
      "[ 130/1000] train_loss: 92.74352 valid_loss: 77.58188\n",
      "Validation loss decreased (77.940414 --> 77.581879).  Saving model ...\n",
      "[ 131/1000] train_loss: 91.61953 valid_loss: 77.22979\n",
      "Validation loss decreased (77.581879 --> 77.229790).  Saving model ...\n",
      "[ 132/1000] train_loss: 90.08445 valid_loss: 76.88357\n",
      "Validation loss decreased (77.229790 --> 76.883568).  Saving model ...\n",
      "[ 133/1000] train_loss: 89.98305 valid_loss: 76.54523\n",
      "Validation loss decreased (76.883568 --> 76.545227).  Saving model ...\n",
      "[ 134/1000] train_loss: 89.84930 valid_loss: 76.21472\n",
      "Validation loss decreased (76.545227 --> 76.214722).  Saving model ...\n",
      "[ 135/1000] train_loss: 89.41608 valid_loss: 75.89349\n",
      "Validation loss decreased (76.214722 --> 75.893494).  Saving model ...\n",
      "[ 136/1000] train_loss: 88.92777 valid_loss: 75.57630\n",
      "Validation loss decreased (75.893494 --> 75.576302).  Saving model ...\n",
      "[ 137/1000] train_loss: 88.35535 valid_loss: 75.26572\n",
      "Validation loss decreased (75.576302 --> 75.265717).  Saving model ...\n",
      "[ 138/1000] train_loss: 88.48682 valid_loss: 74.96076\n",
      "Validation loss decreased (75.265717 --> 74.960762).  Saving model ...\n",
      "[ 139/1000] train_loss: 88.52975 valid_loss: 74.65845\n",
      "Validation loss decreased (74.960762 --> 74.658455).  Saving model ...\n",
      "[ 140/1000] train_loss: 87.83109 valid_loss: 74.35712\n",
      "Validation loss decreased (74.658455 --> 74.357124).  Saving model ...\n",
      "[ 141/1000] train_loss: 86.53598 valid_loss: 74.05560\n",
      "Validation loss decreased (74.357124 --> 74.055595).  Saving model ...\n",
      "[ 142/1000] train_loss: 87.46085 valid_loss: 73.75420\n",
      "Validation loss decreased (74.055595 --> 73.754196).  Saving model ...\n",
      "[ 143/1000] train_loss: 85.80042 valid_loss: 73.44746\n",
      "Validation loss decreased (73.754196 --> 73.447456).  Saving model ...\n",
      "[ 144/1000] train_loss: 86.42021 valid_loss: 73.13042\n",
      "Validation loss decreased (73.447456 --> 73.130417).  Saving model ...\n",
      "[ 145/1000] train_loss: 85.60234 valid_loss: 72.80122\n",
      "Validation loss decreased (73.130417 --> 72.801224).  Saving model ...\n",
      "[ 146/1000] train_loss: 85.26662 valid_loss: 72.46191\n",
      "Validation loss decreased (72.801224 --> 72.461906).  Saving model ...\n",
      "[ 147/1000] train_loss: 85.52127 valid_loss: 72.11576\n",
      "Validation loss decreased (72.461906 --> 72.115761).  Saving model ...\n",
      "[ 148/1000] train_loss: 83.49284 valid_loss: 71.76286\n",
      "Validation loss decreased (72.115761 --> 71.762863).  Saving model ...\n",
      "[ 149/1000] train_loss: 82.69323 valid_loss: 71.41096\n",
      "Validation loss decreased (71.762863 --> 71.410957).  Saving model ...\n",
      "[ 150/1000] train_loss: 83.83781 valid_loss: 71.05856\n",
      "Validation loss decreased (71.410957 --> 71.058556).  Saving model ...\n",
      "[ 151/1000] train_loss: 83.47286 valid_loss: 70.70716\n",
      "Validation loss decreased (71.058556 --> 70.707161).  Saving model ...\n",
      "[ 152/1000] train_loss: 83.35658 valid_loss: 70.35594\n",
      "Validation loss decreased (70.707161 --> 70.355942).  Saving model ...\n",
      "[ 153/1000] train_loss: 82.34819 valid_loss: 70.00780\n",
      "Validation loss decreased (70.355942 --> 70.007797).  Saving model ...\n",
      "[ 154/1000] train_loss: 81.12772 valid_loss: 69.66541\n",
      "Validation loss decreased (70.007797 --> 69.665405).  Saving model ...\n",
      "[ 155/1000] train_loss: 82.14445 valid_loss: 69.32458\n",
      "Validation loss decreased (69.665405 --> 69.324585).  Saving model ...\n",
      "[ 156/1000] train_loss: 81.41940 valid_loss: 68.98795\n",
      "Validation loss decreased (69.324585 --> 68.987953).  Saving model ...\n",
      "[ 157/1000] train_loss: 80.40884 valid_loss: 68.65272\n",
      "Validation loss decreased (68.987953 --> 68.652718).  Saving model ...\n",
      "[ 158/1000] train_loss: 80.64609 valid_loss: 68.31831\n",
      "Validation loss decreased (68.652718 --> 68.318306).  Saving model ...\n",
      "[ 159/1000] train_loss: 79.87605 valid_loss: 67.98596\n",
      "Validation loss decreased (68.318306 --> 67.985962).  Saving model ...\n",
      "[ 160/1000] train_loss: 79.75359 valid_loss: 67.65243\n",
      "Validation loss decreased (67.985962 --> 67.652428).  Saving model ...\n",
      "[ 161/1000] train_loss: 80.47823 valid_loss: 67.32671\n",
      "Validation loss decreased (67.652428 --> 67.326714).  Saving model ...\n",
      "[ 162/1000] train_loss: 78.71638 valid_loss: 67.00404\n",
      "Validation loss decreased (67.326714 --> 67.004044).  Saving model ...\n",
      "[ 163/1000] train_loss: 78.62222 valid_loss: 66.68424\n",
      "Validation loss decreased (67.004044 --> 66.684242).  Saving model ...\n",
      "[ 164/1000] train_loss: 78.78728 valid_loss: 66.36996\n",
      "Validation loss decreased (66.684242 --> 66.369965).  Saving model ...\n",
      "[ 165/1000] train_loss: 76.50220 valid_loss: 66.06195\n",
      "Validation loss decreased (66.369965 --> 66.061951).  Saving model ...\n",
      "[ 166/1000] train_loss: 78.42375 valid_loss: 65.75957\n",
      "Validation loss decreased (66.061951 --> 65.759575).  Saving model ...\n",
      "[ 167/1000] train_loss: 77.39982 valid_loss: 65.46403\n",
      "Validation loss decreased (65.759575 --> 65.464027).  Saving model ...\n",
      "[ 168/1000] train_loss: 75.83194 valid_loss: 65.17082\n",
      "Validation loss decreased (65.464027 --> 65.170822).  Saving model ...\n",
      "[ 169/1000] train_loss: 76.48159 valid_loss: 64.88140\n",
      "Validation loss decreased (65.170822 --> 64.881401).  Saving model ...\n",
      "[ 170/1000] train_loss: 77.33759 valid_loss: 64.59473\n",
      "Validation loss decreased (64.881401 --> 64.594727).  Saving model ...\n",
      "[ 171/1000] train_loss: 76.16489 valid_loss: 64.31367\n",
      "Validation loss decreased (64.594727 --> 64.313667).  Saving model ...\n",
      "[ 172/1000] train_loss: 76.19781 valid_loss: 64.03979\n",
      "Validation loss decreased (64.313667 --> 64.039787).  Saving model ...\n",
      "[ 173/1000] train_loss: 75.76802 valid_loss: 63.77369\n",
      "Validation loss decreased (64.039787 --> 63.773689).  Saving model ...\n",
      "[ 174/1000] train_loss: 74.32954 valid_loss: 63.51300\n",
      "Validation loss decreased (63.773689 --> 63.513000).  Saving model ...\n",
      "[ 175/1000] train_loss: 74.90916 valid_loss: 63.25634\n",
      "Validation loss decreased (63.513000 --> 63.256340).  Saving model ...\n",
      "[ 176/1000] train_loss: 75.74577 valid_loss: 63.00502\n",
      "Validation loss decreased (63.256340 --> 63.005020).  Saving model ...\n",
      "[ 177/1000] train_loss: 73.15823 valid_loss: 62.75947\n",
      "Validation loss decreased (63.005020 --> 62.759468).  Saving model ...\n",
      "[ 178/1000] train_loss: 75.61463 valid_loss: 62.51790\n",
      "Validation loss decreased (62.759468 --> 62.517902).  Saving model ...\n",
      "[ 179/1000] train_loss: 73.58797 valid_loss: 62.27400\n",
      "Validation loss decreased (62.517902 --> 62.274002).  Saving model ...\n",
      "[ 180/1000] train_loss: 73.83980 valid_loss: 62.03181\n",
      "Validation loss decreased (62.274002 --> 62.031811).  Saving model ...\n",
      "[ 181/1000] train_loss: 73.46651 valid_loss: 61.79291\n",
      "Validation loss decreased (62.031811 --> 61.792912).  Saving model ...\n",
      "[ 182/1000] train_loss: 72.57482 valid_loss: 61.55925\n",
      "Validation loss decreased (61.792912 --> 61.559254).  Saving model ...\n",
      "[ 183/1000] train_loss: 71.99142 valid_loss: 61.33046\n",
      "Validation loss decreased (61.559254 --> 61.330460).  Saving model ...\n",
      "[ 184/1000] train_loss: 72.14499 valid_loss: 61.10243\n",
      "Validation loss decreased (61.330460 --> 61.102432).  Saving model ...\n",
      "[ 185/1000] train_loss: 72.32445 valid_loss: 60.87635\n",
      "Validation loss decreased (61.102432 --> 60.876354).  Saving model ...\n",
      "[ 186/1000] train_loss: 71.19009 valid_loss: 60.65253\n",
      "Validation loss decreased (60.876354 --> 60.652527).  Saving model ...\n",
      "[ 187/1000] train_loss: 70.46386 valid_loss: 60.42630\n",
      "Validation loss decreased (60.652527 --> 60.426300).  Saving model ...\n",
      "[ 188/1000] train_loss: 70.54962 valid_loss: 60.20106\n",
      "Validation loss decreased (60.426300 --> 60.201061).  Saving model ...\n",
      "[ 189/1000] train_loss: 71.56206 valid_loss: 59.97807\n",
      "Validation loss decreased (60.201061 --> 59.978073).  Saving model ...\n",
      "[ 190/1000] train_loss: 71.67821 valid_loss: 59.75980\n",
      "Validation loss decreased (59.978073 --> 59.759804).  Saving model ...\n",
      "[ 191/1000] train_loss: 70.67591 valid_loss: 59.54366\n",
      "Validation loss decreased (59.759804 --> 59.543663).  Saving model ...\n",
      "[ 192/1000] train_loss: 70.69064 valid_loss: 59.33316\n",
      "Validation loss decreased (59.543663 --> 59.333160).  Saving model ...\n",
      "[ 193/1000] train_loss: 69.25183 valid_loss: 59.12433\n",
      "Validation loss decreased (59.333160 --> 59.124332).  Saving model ...\n",
      "[ 194/1000] train_loss: 68.67644 valid_loss: 58.92242\n",
      "Validation loss decreased (59.124332 --> 58.922424).  Saving model ...\n",
      "[ 195/1000] train_loss: 68.66061 valid_loss: 58.72284\n",
      "Validation loss decreased (58.922424 --> 58.722839).  Saving model ...\n",
      "[ 196/1000] train_loss: 68.79364 valid_loss: 58.52601\n",
      "Validation loss decreased (58.722839 --> 58.526009).  Saving model ...\n",
      "[ 197/1000] train_loss: 68.08145 valid_loss: 58.32602\n",
      "Validation loss decreased (58.526009 --> 58.326019).  Saving model ...\n",
      "[ 198/1000] train_loss: 68.94227 valid_loss: 58.12581\n",
      "Validation loss decreased (58.326019 --> 58.125809).  Saving model ...\n",
      "[ 199/1000] train_loss: 68.34655 valid_loss: 57.92274\n",
      "Validation loss decreased (58.125809 --> 57.922745).  Saving model ...\n",
      "[ 200/1000] train_loss: 67.36402 valid_loss: 57.72588\n",
      "Validation loss decreased (57.922745 --> 57.725883).  Saving model ...\n",
      "[ 201/1000] train_loss: 67.21496 valid_loss: 57.53666\n",
      "Validation loss decreased (57.725883 --> 57.536655).  Saving model ...\n",
      "[ 202/1000] train_loss: 66.22283 valid_loss: 57.35021\n",
      "Validation loss decreased (57.536655 --> 57.350212).  Saving model ...\n",
      "[ 203/1000] train_loss: 67.59022 valid_loss: 57.16613\n",
      "Validation loss decreased (57.350212 --> 57.166134).  Saving model ...\n",
      "[ 204/1000] train_loss: 67.42599 valid_loss: 56.98352\n",
      "Validation loss decreased (57.166134 --> 56.983517).  Saving model ...\n",
      "[ 205/1000] train_loss: 67.08192 valid_loss: 56.80595\n",
      "Validation loss decreased (56.983517 --> 56.805946).  Saving model ...\n",
      "[ 206/1000] train_loss: 67.18941 valid_loss: 56.62658\n",
      "Validation loss decreased (56.805946 --> 56.626579).  Saving model ...\n",
      "[ 207/1000] train_loss: 65.12067 valid_loss: 56.44798\n",
      "Validation loss decreased (56.626579 --> 56.447975).  Saving model ...\n",
      "[ 208/1000] train_loss: 67.18090 valid_loss: 56.26959\n",
      "Validation loss decreased (56.447975 --> 56.269588).  Saving model ...\n",
      "[ 209/1000] train_loss: 65.03034 valid_loss: 56.09311\n",
      "Validation loss decreased (56.269588 --> 56.093109).  Saving model ...\n",
      "[ 210/1000] train_loss: 65.33116 valid_loss: 55.91327\n",
      "Validation loss decreased (56.093109 --> 55.913273).  Saving model ...\n",
      "[ 211/1000] train_loss: 65.59116 valid_loss: 55.73637\n",
      "Validation loss decreased (55.913273 --> 55.736370).  Saving model ...\n",
      "[ 212/1000] train_loss: 63.88132 valid_loss: 55.55620\n",
      "Validation loss decreased (55.736370 --> 55.556202).  Saving model ...\n",
      "[ 213/1000] train_loss: 64.46093 valid_loss: 55.37715\n",
      "Validation loss decreased (55.556202 --> 55.377151).  Saving model ...\n",
      "[ 214/1000] train_loss: 64.17799 valid_loss: 55.20033\n",
      "Validation loss decreased (55.377151 --> 55.200325).  Saving model ...\n",
      "[ 215/1000] train_loss: 65.28317 valid_loss: 55.02845\n",
      "Validation loss decreased (55.200325 --> 55.028454).  Saving model ...\n",
      "[ 216/1000] train_loss: 64.46243 valid_loss: 54.85770\n",
      "Validation loss decreased (55.028454 --> 54.857697).  Saving model ...\n",
      "[ 217/1000] train_loss: 63.08363 valid_loss: 54.69057\n",
      "Validation loss decreased (54.857697 --> 54.690575).  Saving model ...\n",
      "[ 218/1000] train_loss: 62.89617 valid_loss: 54.52275\n",
      "Validation loss decreased (54.690575 --> 54.522747).  Saving model ...\n",
      "[ 219/1000] train_loss: 64.27375 valid_loss: 54.35769\n",
      "Validation loss decreased (54.522747 --> 54.357689).  Saving model ...\n",
      "[ 220/1000] train_loss: 62.08649 valid_loss: 54.19369\n",
      "Validation loss decreased (54.357689 --> 54.193691).  Saving model ...\n",
      "[ 221/1000] train_loss: 62.91558 valid_loss: 54.03455\n",
      "Validation loss decreased (54.193691 --> 54.034546).  Saving model ...\n",
      "[ 222/1000] train_loss: 64.21858 valid_loss: 53.88121\n",
      "Validation loss decreased (54.034546 --> 53.881214).  Saving model ...\n",
      "[ 223/1000] train_loss: 63.66947 valid_loss: 53.73166\n",
      "Validation loss decreased (53.881214 --> 53.731659).  Saving model ...\n",
      "[ 224/1000] train_loss: 63.06814 valid_loss: 53.58714\n",
      "Validation loss decreased (53.731659 --> 53.587139).  Saving model ...\n",
      "[ 225/1000] train_loss: 63.91542 valid_loss: 53.44021\n",
      "Validation loss decreased (53.587139 --> 53.440212).  Saving model ...\n",
      "[ 226/1000] train_loss: 62.67995 valid_loss: 53.28674\n",
      "Validation loss decreased (53.440212 --> 53.286743).  Saving model ...\n",
      "[ 227/1000] train_loss: 63.36649 valid_loss: 53.13744\n",
      "Validation loss decreased (53.286743 --> 53.137444).  Saving model ...\n",
      "[ 228/1000] train_loss: 63.05539 valid_loss: 52.98767\n",
      "Validation loss decreased (53.137444 --> 52.987667).  Saving model ...\n",
      "[ 229/1000] train_loss: 61.32750 valid_loss: 52.83886\n",
      "Validation loss decreased (52.987667 --> 52.838860).  Saving model ...\n",
      "[ 230/1000] train_loss: 61.97303 valid_loss: 52.69183\n",
      "Validation loss decreased (52.838860 --> 52.691830).  Saving model ...\n",
      "[ 231/1000] train_loss: 62.15980 valid_loss: 52.54753\n",
      "Validation loss decreased (52.691830 --> 52.547527).  Saving model ...\n",
      "[ 232/1000] train_loss: 61.40574 valid_loss: 52.40726\n",
      "Validation loss decreased (52.547527 --> 52.407265).  Saving model ...\n",
      "[ 233/1000] train_loss: 61.23083 valid_loss: 52.26836\n",
      "Validation loss decreased (52.407265 --> 52.268356).  Saving model ...\n",
      "[ 234/1000] train_loss: 61.35009 valid_loss: 52.12833\n",
      "Validation loss decreased (52.268356 --> 52.128330).  Saving model ...\n",
      "[ 235/1000] train_loss: 61.63512 valid_loss: 51.98528\n",
      "Validation loss decreased (52.128330 --> 51.985275).  Saving model ...\n",
      "[ 236/1000] train_loss: 60.33848 valid_loss: 51.84388\n",
      "Validation loss decreased (51.985275 --> 51.843876).  Saving model ...\n",
      "[ 237/1000] train_loss: 60.22828 valid_loss: 51.70300\n",
      "Validation loss decreased (51.843876 --> 51.703003).  Saving model ...\n",
      "[ 238/1000] train_loss: 61.06259 valid_loss: 51.56599\n",
      "Validation loss decreased (51.703003 --> 51.565987).  Saving model ...\n",
      "[ 239/1000] train_loss: 60.40700 valid_loss: 51.42925\n",
      "Validation loss decreased (51.565987 --> 51.429249).  Saving model ...\n",
      "[ 240/1000] train_loss: 59.99104 valid_loss: 51.29282\n",
      "Validation loss decreased (51.429249 --> 51.292824).  Saving model ...\n",
      "[ 241/1000] train_loss: 59.63641 valid_loss: 51.16074\n",
      "Validation loss decreased (51.292824 --> 51.160736).  Saving model ...\n",
      "[ 242/1000] train_loss: 59.38732 valid_loss: 51.03330\n",
      "Validation loss decreased (51.160736 --> 51.033302).  Saving model ...\n",
      "[ 243/1000] train_loss: 59.04100 valid_loss: 50.90496\n",
      "Validation loss decreased (51.033302 --> 50.904961).  Saving model ...\n",
      "[ 244/1000] train_loss: 59.13839 valid_loss: 50.77467\n",
      "Validation loss decreased (50.904961 --> 50.774666).  Saving model ...\n",
      "[ 245/1000] train_loss: 59.68353 valid_loss: 50.64426\n",
      "Validation loss decreased (50.774666 --> 50.644264).  Saving model ...\n",
      "[ 246/1000] train_loss: 58.41888 valid_loss: 50.51321\n",
      "Validation loss decreased (50.644264 --> 50.513214).  Saving model ...\n",
      "[ 247/1000] train_loss: 59.11695 valid_loss: 50.38592\n",
      "Validation loss decreased (50.513214 --> 50.385918).  Saving model ...\n",
      "[ 248/1000] train_loss: 59.34911 valid_loss: 50.26349\n",
      "Validation loss decreased (50.385918 --> 50.263489).  Saving model ...\n",
      "[ 249/1000] train_loss: 58.51063 valid_loss: 50.14380\n",
      "Validation loss decreased (50.263489 --> 50.143803).  Saving model ...\n",
      "[ 250/1000] train_loss: 57.84883 valid_loss: 50.02144\n",
      "Validation loss decreased (50.143803 --> 50.021442).  Saving model ...\n",
      "[ 251/1000] train_loss: 58.95279 valid_loss: 49.89951\n",
      "Validation loss decreased (50.021442 --> 49.899513).  Saving model ...\n",
      "[ 252/1000] train_loss: 59.24049 valid_loss: 49.77705\n",
      "Validation loss decreased (49.899513 --> 49.777046).  Saving model ...\n",
      "[ 253/1000] train_loss: 59.17819 valid_loss: 49.65497\n",
      "Validation loss decreased (49.777046 --> 49.654972).  Saving model ...\n",
      "[ 254/1000] train_loss: 58.73780 valid_loss: 49.53613\n",
      "Validation loss decreased (49.654972 --> 49.536133).  Saving model ...\n",
      "[ 255/1000] train_loss: 56.98236 valid_loss: 49.42023\n",
      "Validation loss decreased (49.536133 --> 49.420231).  Saving model ...\n",
      "[ 256/1000] train_loss: 57.86961 valid_loss: 49.30820\n",
      "Validation loss decreased (49.420231 --> 49.308197).  Saving model ...\n",
      "[ 257/1000] train_loss: 58.23399 valid_loss: 49.20015\n",
      "Validation loss decreased (49.308197 --> 49.200146).  Saving model ...\n",
      "[ 258/1000] train_loss: 56.10889 valid_loss: 49.09426\n",
      "Validation loss decreased (49.200146 --> 49.094261).  Saving model ...\n",
      "[ 259/1000] train_loss: 58.00598 valid_loss: 48.98415\n",
      "Validation loss decreased (49.094261 --> 48.984154).  Saving model ...\n",
      "[ 260/1000] train_loss: 56.69217 valid_loss: 48.86985\n",
      "Validation loss decreased (48.984154 --> 48.869846).  Saving model ...\n",
      "[ 261/1000] train_loss: 58.32719 valid_loss: 48.76016\n",
      "Validation loss decreased (48.869846 --> 48.760162).  Saving model ...\n",
      "[ 262/1000] train_loss: 55.86163 valid_loss: 48.64489\n",
      "Validation loss decreased (48.760162 --> 48.644886).  Saving model ...\n",
      "[ 263/1000] train_loss: 56.78983 valid_loss: 48.53125\n",
      "Validation loss decreased (48.644886 --> 48.531250).  Saving model ...\n",
      "[ 264/1000] train_loss: 56.83803 valid_loss: 48.42240\n",
      "Validation loss decreased (48.531250 --> 48.422401).  Saving model ...\n",
      "[ 265/1000] train_loss: 56.29393 valid_loss: 48.31751\n",
      "Validation loss decreased (48.422401 --> 48.317509).  Saving model ...\n",
      "[ 266/1000] train_loss: 57.52765 valid_loss: 48.21589\n",
      "Validation loss decreased (48.317509 --> 48.215893).  Saving model ...\n",
      "[ 267/1000] train_loss: 55.85392 valid_loss: 48.11520\n",
      "Validation loss decreased (48.215893 --> 48.115200).  Saving model ...\n",
      "[ 268/1000] train_loss: 57.32263 valid_loss: 48.01423\n",
      "Validation loss decreased (48.115200 --> 48.014229).  Saving model ...\n",
      "[ 269/1000] train_loss: 55.60247 valid_loss: 47.91270\n",
      "Validation loss decreased (48.014229 --> 47.912701).  Saving model ...\n",
      "[ 270/1000] train_loss: 55.90327 valid_loss: 47.81279\n",
      "Validation loss decreased (47.912701 --> 47.812794).  Saving model ...\n",
      "[ 271/1000] train_loss: 55.81788 valid_loss: 47.71704\n",
      "Validation loss decreased (47.812794 --> 47.717041).  Saving model ...\n",
      "[ 272/1000] train_loss: 55.36132 valid_loss: 47.62650\n",
      "Validation loss decreased (47.717041 --> 47.626495).  Saving model ...\n",
      "[ 273/1000] train_loss: 55.98689 valid_loss: 47.54014\n",
      "Validation loss decreased (47.626495 --> 47.540142).  Saving model ...\n",
      "[ 274/1000] train_loss: 55.74861 valid_loss: 47.45433\n",
      "Validation loss decreased (47.540142 --> 47.454330).  Saving model ...\n",
      "[ 275/1000] train_loss: 54.57616 valid_loss: 47.36271\n",
      "Validation loss decreased (47.454330 --> 47.362713).  Saving model ...\n",
      "[ 276/1000] train_loss: 55.20730 valid_loss: 47.27667\n",
      "Validation loss decreased (47.362713 --> 47.276672).  Saving model ...\n",
      "[ 277/1000] train_loss: 55.00264 valid_loss: 47.18678\n",
      "Validation loss decreased (47.276672 --> 47.186783).  Saving model ...\n",
      "[ 278/1000] train_loss: 54.99949 valid_loss: 47.09406\n",
      "Validation loss decreased (47.186783 --> 47.094063).  Saving model ...\n",
      "[ 279/1000] train_loss: 54.40689 valid_loss: 47.00198\n",
      "Validation loss decreased (47.094063 --> 47.001976).  Saving model ...\n",
      "[ 280/1000] train_loss: 54.75551 valid_loss: 46.91303\n",
      "Validation loss decreased (47.001976 --> 46.913029).  Saving model ...\n",
      "[ 281/1000] train_loss: 54.10698 valid_loss: 46.82654\n",
      "Validation loss decreased (46.913029 --> 46.826538).  Saving model ...\n",
      "[ 282/1000] train_loss: 54.27908 valid_loss: 46.73858\n",
      "Validation loss decreased (46.826538 --> 46.738583).  Saving model ...\n",
      "[ 283/1000] train_loss: 53.87787 valid_loss: 46.64982\n",
      "Validation loss decreased (46.738583 --> 46.649818).  Saving model ...\n",
      "[ 284/1000] train_loss: 54.39208 valid_loss: 46.56168\n",
      "Validation loss decreased (46.649818 --> 46.561676).  Saving model ...\n",
      "[ 285/1000] train_loss: 54.19483 valid_loss: 46.47924\n",
      "Validation loss decreased (46.561676 --> 46.479240).  Saving model ...\n",
      "[ 286/1000] train_loss: 54.16028 valid_loss: 46.39877\n",
      "Validation loss decreased (46.479240 --> 46.398773).  Saving model ...\n",
      "[ 287/1000] train_loss: 53.43311 valid_loss: 46.31866\n",
      "Validation loss decreased (46.398773 --> 46.318661).  Saving model ...\n",
      "[ 288/1000] train_loss: 53.96604 valid_loss: 46.23932\n",
      "Validation loss decreased (46.318661 --> 46.239323).  Saving model ...\n",
      "[ 289/1000] train_loss: 54.00226 valid_loss: 46.16230\n",
      "Validation loss decreased (46.239323 --> 46.162296).  Saving model ...\n",
      "[ 290/1000] train_loss: 53.77320 valid_loss: 46.08223\n",
      "Validation loss decreased (46.162296 --> 46.082226).  Saving model ...\n",
      "[ 291/1000] train_loss: 54.23732 valid_loss: 46.00164\n",
      "Validation loss decreased (46.082226 --> 46.001640).  Saving model ...\n",
      "[ 292/1000] train_loss: 54.13981 valid_loss: 45.92040\n",
      "Validation loss decreased (46.001640 --> 45.920403).  Saving model ...\n",
      "[ 293/1000] train_loss: 53.08815 valid_loss: 45.84068\n",
      "Validation loss decreased (45.920403 --> 45.840683).  Saving model ...\n",
      "[ 294/1000] train_loss: 53.50796 valid_loss: 45.76745\n",
      "Validation loss decreased (45.840683 --> 45.767452).  Saving model ...\n",
      "[ 295/1000] train_loss: 53.42317 valid_loss: 45.69624\n",
      "Validation loss decreased (45.767452 --> 45.696236).  Saving model ...\n",
      "[ 296/1000] train_loss: 53.42655 valid_loss: 45.62437\n",
      "Validation loss decreased (45.696236 --> 45.624367).  Saving model ...\n",
      "[ 297/1000] train_loss: 53.09032 valid_loss: 45.55457\n",
      "Validation loss decreased (45.624367 --> 45.554565).  Saving model ...\n",
      "[ 298/1000] train_loss: 53.36816 valid_loss: 45.47744\n",
      "Validation loss decreased (45.554565 --> 45.477444).  Saving model ...\n",
      "[ 299/1000] train_loss: 52.07272 valid_loss: 45.40337\n",
      "Validation loss decreased (45.477444 --> 45.403374).  Saving model ...\n",
      "[ 300/1000] train_loss: 52.90271 valid_loss: 45.33421\n",
      "Validation loss decreased (45.403374 --> 45.334206).  Saving model ...\n",
      "[ 301/1000] train_loss: 52.20498 valid_loss: 45.27211\n",
      "Validation loss decreased (45.334206 --> 45.272106).  Saving model ...\n",
      "[ 302/1000] train_loss: 52.54326 valid_loss: 45.21644\n",
      "Validation loss decreased (45.272106 --> 45.216442).  Saving model ...\n",
      "[ 303/1000] train_loss: 52.52155 valid_loss: 45.15321\n",
      "Validation loss decreased (45.216442 --> 45.153206).  Saving model ...\n",
      "[ 304/1000] train_loss: 53.23373 valid_loss: 45.09019\n",
      "Validation loss decreased (45.153206 --> 45.090187).  Saving model ...\n",
      "[ 305/1000] train_loss: 51.65488 valid_loss: 45.02663\n",
      "Validation loss decreased (45.090187 --> 45.026630).  Saving model ...\n",
      "[ 306/1000] train_loss: 52.65048 valid_loss: 44.96302\n",
      "Validation loss decreased (45.026630 --> 44.963024).  Saving model ...\n",
      "[ 307/1000] train_loss: 51.89668 valid_loss: 44.89571\n",
      "Validation loss decreased (44.963024 --> 44.895706).  Saving model ...\n",
      "[ 308/1000] train_loss: 52.40955 valid_loss: 44.82956\n",
      "Validation loss decreased (44.895706 --> 44.829559).  Saving model ...\n",
      "[ 309/1000] train_loss: 51.73612 valid_loss: 44.76802\n",
      "Validation loss decreased (44.829559 --> 44.768024).  Saving model ...\n",
      "[ 310/1000] train_loss: 51.71679 valid_loss: 44.70767\n",
      "Validation loss decreased (44.768024 --> 44.707668).  Saving model ...\n",
      "[ 311/1000] train_loss: 51.74339 valid_loss: 44.64312\n",
      "Validation loss decreased (44.707668 --> 44.643116).  Saving model ...\n",
      "[ 312/1000] train_loss: 51.11939 valid_loss: 44.58103\n",
      "Validation loss decreased (44.643116 --> 44.581028).  Saving model ...\n",
      "[ 313/1000] train_loss: 52.02890 valid_loss: 44.52209\n",
      "Validation loss decreased (44.581028 --> 44.522087).  Saving model ...\n",
      "[ 314/1000] train_loss: 51.35518 valid_loss: 44.46753\n",
      "Validation loss decreased (44.522087 --> 44.467533).  Saving model ...\n",
      "[ 315/1000] train_loss: 52.08876 valid_loss: 44.41138\n",
      "Validation loss decreased (44.467533 --> 44.411381).  Saving model ...\n",
      "[ 316/1000] train_loss: 51.71044 valid_loss: 44.35450\n",
      "Validation loss decreased (44.411381 --> 44.354504).  Saving model ...\n",
      "[ 317/1000] train_loss: 51.50494 valid_loss: 44.30024\n",
      "Validation loss decreased (44.354504 --> 44.300243).  Saving model ...\n",
      "[ 318/1000] train_loss: 51.82763 valid_loss: 44.24672\n",
      "Validation loss decreased (44.300243 --> 44.246719).  Saving model ...\n",
      "[ 319/1000] train_loss: 51.57573 valid_loss: 44.19190\n",
      "Validation loss decreased (44.246719 --> 44.191902).  Saving model ...\n",
      "[ 320/1000] train_loss: 51.86128 valid_loss: 44.14399\n",
      "Validation loss decreased (44.191902 --> 44.143993).  Saving model ...\n",
      "[ 321/1000] train_loss: 51.60663 valid_loss: 44.10126\n",
      "Validation loss decreased (44.143993 --> 44.101261).  Saving model ...\n",
      "[ 322/1000] train_loss: 51.22434 valid_loss: 44.05995\n",
      "Validation loss decreased (44.101261 --> 44.059952).  Saving model ...\n",
      "[ 323/1000] train_loss: 51.23600 valid_loss: 44.01365\n",
      "Validation loss decreased (44.059952 --> 44.013645).  Saving model ...\n",
      "[ 324/1000] train_loss: 51.38857 valid_loss: 43.96197\n",
      "Validation loss decreased (44.013645 --> 43.961971).  Saving model ...\n",
      "[ 325/1000] train_loss: 50.73638 valid_loss: 43.89977\n",
      "Validation loss decreased (43.961971 --> 43.899765).  Saving model ...\n",
      "[ 326/1000] train_loss: 50.79848 valid_loss: 43.84363\n",
      "Validation loss decreased (43.899765 --> 43.843628).  Saving model ...\n",
      "[ 327/1000] train_loss: 52.42278 valid_loss: 43.79794\n",
      "Validation loss decreased (43.843628 --> 43.797939).  Saving model ...\n",
      "[ 328/1000] train_loss: 51.49499 valid_loss: 43.75532\n",
      "Validation loss decreased (43.797939 --> 43.755318).  Saving model ...\n",
      "[ 329/1000] train_loss: 51.24457 valid_loss: 43.70967\n",
      "Validation loss decreased (43.755318 --> 43.709667).  Saving model ...\n",
      "[ 330/1000] train_loss: 51.09673 valid_loss: 43.66192\n",
      "Validation loss decreased (43.709667 --> 43.661919).  Saving model ...\n",
      "[ 331/1000] train_loss: 50.24794 valid_loss: 43.61883\n",
      "Validation loss decreased (43.661919 --> 43.618832).  Saving model ...\n",
      "[ 332/1000] train_loss: 51.00498 valid_loss: 43.57911\n",
      "Validation loss decreased (43.618832 --> 43.579105).  Saving model ...\n",
      "[ 333/1000] train_loss: 50.50505 valid_loss: 43.54074\n",
      "Validation loss decreased (43.579105 --> 43.540737).  Saving model ...\n",
      "[ 334/1000] train_loss: 51.24312 valid_loss: 43.50540\n",
      "Validation loss decreased (43.540737 --> 43.505402).  Saving model ...\n",
      "[ 335/1000] train_loss: 50.77831 valid_loss: 43.46624\n",
      "Validation loss decreased (43.505402 --> 43.466240).  Saving model ...\n",
      "[ 336/1000] train_loss: 49.98780 valid_loss: 43.41770\n",
      "Validation loss decreased (43.466240 --> 43.417702).  Saving model ...\n",
      "[ 337/1000] train_loss: 50.71152 valid_loss: 43.37178\n",
      "Validation loss decreased (43.417702 --> 43.371780).  Saving model ...\n",
      "[ 338/1000] train_loss: 50.26333 valid_loss: 43.32940\n",
      "Validation loss decreased (43.371780 --> 43.329399).  Saving model ...\n",
      "[ 339/1000] train_loss: 50.96799 valid_loss: 43.28701\n",
      "Validation loss decreased (43.329399 --> 43.287010).  Saving model ...\n",
      "[ 340/1000] train_loss: 49.33744 valid_loss: 43.24740\n",
      "Validation loss decreased (43.287010 --> 43.247402).  Saving model ...\n",
      "[ 341/1000] train_loss: 49.31270 valid_loss: 43.21210\n",
      "Validation loss decreased (43.247402 --> 43.212105).  Saving model ...\n",
      "[ 342/1000] train_loss: 50.99337 valid_loss: 43.18105\n",
      "Validation loss decreased (43.212105 --> 43.181046).  Saving model ...\n",
      "[ 343/1000] train_loss: 49.77319 valid_loss: 43.14528\n",
      "Validation loss decreased (43.181046 --> 43.145279).  Saving model ...\n",
      "[ 344/1000] train_loss: 49.66790 valid_loss: 43.09575\n",
      "Validation loss decreased (43.145279 --> 43.095753).  Saving model ...\n",
      "[ 345/1000] train_loss: 49.21792 valid_loss: 43.05668\n",
      "Validation loss decreased (43.095753 --> 43.056679).  Saving model ...\n",
      "[ 346/1000] train_loss: 50.97550 valid_loss: 43.03329\n",
      "Validation loss decreased (43.056679 --> 43.033287).  Saving model ...\n",
      "[ 347/1000] train_loss: 50.58262 valid_loss: 43.02374\n",
      "Validation loss decreased (43.033287 --> 43.023739).  Saving model ...\n",
      "[ 348/1000] train_loss: 50.32507 valid_loss: 43.01397\n",
      "Validation loss decreased (43.023739 --> 43.013969).  Saving model ...\n",
      "[ 349/1000] train_loss: 50.00783 valid_loss: 43.00432\n",
      "Validation loss decreased (43.013969 --> 43.004318).  Saving model ...\n",
      "[ 350/1000] train_loss: 49.61035 valid_loss: 42.96414\n",
      "Validation loss decreased (43.004318 --> 42.964142).  Saving model ...\n",
      "[ 351/1000] train_loss: 50.28839 valid_loss: 42.89417\n",
      "Validation loss decreased (42.964142 --> 42.894173).  Saving model ...\n",
      "[ 352/1000] train_loss: 49.90883 valid_loss: 42.83565\n",
      "Validation loss decreased (42.894173 --> 42.835648).  Saving model ...\n",
      "[ 353/1000] train_loss: 49.84388 valid_loss: 42.79890\n",
      "Validation loss decreased (42.835648 --> 42.798901).  Saving model ...\n",
      "[ 354/1000] train_loss: 49.19765 valid_loss: 42.77621\n",
      "Validation loss decreased (42.798901 --> 42.776211).  Saving model ...\n",
      "[ 355/1000] train_loss: 49.61004 valid_loss: 42.75066\n",
      "Validation loss decreased (42.776211 --> 42.750660).  Saving model ...\n",
      "[ 356/1000] train_loss: 48.65375 valid_loss: 42.71695\n",
      "Validation loss decreased (42.750660 --> 42.716946).  Saving model ...\n",
      "[ 357/1000] train_loss: 49.20644 valid_loss: 42.66884\n",
      "Validation loss decreased (42.716946 --> 42.668839).  Saving model ...\n",
      "[ 358/1000] train_loss: 49.52225 valid_loss: 42.62366\n",
      "Validation loss decreased (42.668839 --> 42.623661).  Saving model ...\n",
      "[ 359/1000] train_loss: 49.09126 valid_loss: 42.60033\n",
      "Validation loss decreased (42.623661 --> 42.600330).  Saving model ...\n",
      "[ 360/1000] train_loss: 49.71529 valid_loss: 42.58889\n",
      "Validation loss decreased (42.600330 --> 42.588886).  Saving model ...\n",
      "[ 361/1000] train_loss: 49.80212 valid_loss: 42.57677\n",
      "Validation loss decreased (42.588886 --> 42.576771).  Saving model ...\n",
      "[ 362/1000] train_loss: 48.69141 valid_loss: 42.55975\n",
      "Validation loss decreased (42.576771 --> 42.559753).  Saving model ...\n",
      "[ 363/1000] train_loss: 50.35407 valid_loss: 42.52687\n",
      "Validation loss decreased (42.559753 --> 42.526867).  Saving model ...\n",
      "[ 364/1000] train_loss: 48.53727 valid_loss: 42.48589\n",
      "Validation loss decreased (42.526867 --> 42.485893).  Saving model ...\n",
      "[ 365/1000] train_loss: 49.44254 valid_loss: 42.44887\n",
      "Validation loss decreased (42.485893 --> 42.448872).  Saving model ...\n",
      "[ 366/1000] train_loss: 48.62129 valid_loss: 42.41831\n",
      "Validation loss decreased (42.448872 --> 42.418312).  Saving model ...\n",
      "[ 367/1000] train_loss: 49.21538 valid_loss: 42.40162\n",
      "Validation loss decreased (42.418312 --> 42.401619).  Saving model ...\n",
      "[ 368/1000] train_loss: 49.61985 valid_loss: 42.37864\n",
      "Validation loss decreased (42.401619 --> 42.378635).  Saving model ...\n",
      "[ 369/1000] train_loss: 48.65161 valid_loss: 42.35182\n",
      "Validation loss decreased (42.378635 --> 42.351818).  Saving model ...\n",
      "[ 370/1000] train_loss: 49.92782 valid_loss: 42.30892\n",
      "Validation loss decreased (42.351818 --> 42.308918).  Saving model ...\n",
      "[ 371/1000] train_loss: 48.89587 valid_loss: 42.28137\n",
      "Validation loss decreased (42.308918 --> 42.281368).  Saving model ...\n",
      "[ 372/1000] train_loss: 49.44864 valid_loss: 42.26387\n",
      "Validation loss decreased (42.281368 --> 42.263870).  Saving model ...\n",
      "[ 373/1000] train_loss: 48.28355 valid_loss: 42.25100\n",
      "Validation loss decreased (42.263870 --> 42.250996).  Saving model ...\n",
      "[ 374/1000] train_loss: 48.47570 valid_loss: 42.23305\n",
      "Validation loss decreased (42.250996 --> 42.233047).  Saving model ...\n",
      "[ 375/1000] train_loss: 49.46986 valid_loss: 42.20726\n",
      "Validation loss decreased (42.233047 --> 42.207260).  Saving model ...\n",
      "[ 376/1000] train_loss: 48.87437 valid_loss: 42.17393\n",
      "Validation loss decreased (42.207260 --> 42.173931).  Saving model ...\n",
      "[ 377/1000] train_loss: 48.84702 valid_loss: 42.13805\n",
      "Validation loss decreased (42.173931 --> 42.138050).  Saving model ...\n",
      "[ 378/1000] train_loss: 48.94682 valid_loss: 42.11409\n",
      "Validation loss decreased (42.138050 --> 42.114090).  Saving model ...\n",
      "[ 379/1000] train_loss: 48.51529 valid_loss: 42.09902\n",
      "Validation loss decreased (42.114090 --> 42.099018).  Saving model ...\n",
      "[ 380/1000] train_loss: 48.05183 valid_loss: 42.08468\n",
      "Validation loss decreased (42.099018 --> 42.084682).  Saving model ...\n",
      "[ 381/1000] train_loss: 48.06038 valid_loss: 42.05581\n",
      "Validation loss decreased (42.084682 --> 42.055813).  Saving model ...\n",
      "[ 382/1000] train_loss: 49.54624 valid_loss: 42.02154\n",
      "Validation loss decreased (42.055813 --> 42.021538).  Saving model ...\n",
      "[ 383/1000] train_loss: 48.02522 valid_loss: 41.99758\n",
      "Validation loss decreased (42.021538 --> 41.997578).  Saving model ...\n",
      "[ 384/1000] train_loss: 49.02151 valid_loss: 41.99181\n",
      "Validation loss decreased (41.997578 --> 41.991806).  Saving model ...\n",
      "[ 385/1000] train_loss: 48.27463 valid_loss: 41.99020\n",
      "Validation loss decreased (41.991806 --> 41.990196).  Saving model ...\n",
      "[ 386/1000] train_loss: 47.96689 valid_loss: 41.98698\n",
      "Validation loss decreased (41.990196 --> 41.986977).  Saving model ...\n",
      "[ 387/1000] train_loss: 48.07624 valid_loss: 41.97602\n",
      "Validation loss decreased (41.986977 --> 41.976021).  Saving model ...\n",
      "[ 388/1000] train_loss: 48.53745 valid_loss: 41.93417\n",
      "Validation loss decreased (41.976021 --> 41.934166).  Saving model ...\n",
      "[ 389/1000] train_loss: 49.02781 valid_loss: 41.89296\n",
      "Validation loss decreased (41.934166 --> 41.892963).  Saving model ...\n",
      "[ 390/1000] train_loss: 47.77286 valid_loss: 41.85755\n",
      "Validation loss decreased (41.892963 --> 41.857548).  Saving model ...\n",
      "[ 391/1000] train_loss: 48.29359 valid_loss: 41.83462\n",
      "Validation loss decreased (41.857548 --> 41.834621).  Saving model ...\n",
      "[ 392/1000] train_loss: 47.40919 valid_loss: 41.81879\n",
      "Validation loss decreased (41.834621 --> 41.818794).  Saving model ...\n",
      "[ 393/1000] train_loss: 48.46252 valid_loss: 41.80644\n",
      "Validation loss decreased (41.818794 --> 41.806438).  Saving model ...\n",
      "[ 394/1000] train_loss: 48.00814 valid_loss: 41.78444\n",
      "Validation loss decreased (41.806438 --> 41.784443).  Saving model ...\n",
      "[ 395/1000] train_loss: 47.29945 valid_loss: 41.75598\n",
      "Validation loss decreased (41.784443 --> 41.755981).  Saving model ...\n",
      "[ 396/1000] train_loss: 49.39525 valid_loss: 41.74285\n",
      "Validation loss decreased (41.755981 --> 41.742847).  Saving model ...\n",
      "[ 397/1000] train_loss: 47.70698 valid_loss: 41.73833\n",
      "Validation loss decreased (41.742847 --> 41.738331).  Saving model ...\n",
      "[ 398/1000] train_loss: 48.62614 valid_loss: 41.72828\n",
      "Validation loss decreased (41.738331 --> 41.728283).  Saving model ...\n",
      "[ 399/1000] train_loss: 48.33160 valid_loss: 41.71137\n",
      "Validation loss decreased (41.728283 --> 41.711369).  Saving model ...\n",
      "[ 400/1000] train_loss: 47.40337 valid_loss: 41.68454\n",
      "Validation loss decreased (41.711369 --> 41.684536).  Saving model ...\n",
      "[ 401/1000] train_loss: 48.22441 valid_loss: 41.65829\n",
      "Validation loss decreased (41.684536 --> 41.658287).  Saving model ...\n",
      "[ 402/1000] train_loss: 48.03165 valid_loss: 41.63478\n",
      "Validation loss decreased (41.658287 --> 41.634785).  Saving model ...\n",
      "[ 403/1000] train_loss: 47.67757 valid_loss: 41.61720\n",
      "Validation loss decreased (41.634785 --> 41.617203).  Saving model ...\n",
      "[ 404/1000] train_loss: 47.51025 valid_loss: 41.60717\n",
      "Validation loss decreased (41.617203 --> 41.607166).  Saving model ...\n",
      "[ 405/1000] train_loss: 48.82719 valid_loss: 41.59311\n",
      "Validation loss decreased (41.607166 --> 41.593113).  Saving model ...\n",
      "[ 406/1000] train_loss: 48.11093 valid_loss: 41.57697\n",
      "Validation loss decreased (41.593113 --> 41.576969).  Saving model ...\n",
      "[ 407/1000] train_loss: 48.56681 valid_loss: 41.56149\n",
      "Validation loss decreased (41.576969 --> 41.561489).  Saving model ...\n",
      "[ 408/1000] train_loss: 48.03156 valid_loss: 41.54937\n",
      "Validation loss decreased (41.561489 --> 41.549374).  Saving model ...\n",
      "[ 409/1000] train_loss: 47.11318 valid_loss: 41.53655\n",
      "Validation loss decreased (41.549374 --> 41.536549).  Saving model ...\n",
      "[ 410/1000] train_loss: 47.22909 valid_loss: 41.52296\n",
      "Validation loss decreased (41.536549 --> 41.522964).  Saving model ...\n",
      "[ 411/1000] train_loss: 47.24323 valid_loss: 41.50688\n",
      "Validation loss decreased (41.522964 --> 41.506882).  Saving model ...\n",
      "[ 412/1000] train_loss: 46.92593 valid_loss: 41.49336\n",
      "Validation loss decreased (41.506882 --> 41.493359).  Saving model ...\n",
      "[ 413/1000] train_loss: 47.07827 valid_loss: 41.47211\n",
      "Validation loss decreased (41.493359 --> 41.472107).  Saving model ...\n",
      "[ 414/1000] train_loss: 46.96814 valid_loss: 41.44787\n",
      "Validation loss decreased (41.472107 --> 41.447872).  Saving model ...\n",
      "[ 415/1000] train_loss: 47.42877 valid_loss: 41.42984\n",
      "Validation loss decreased (41.447872 --> 41.429844).  Saving model ...\n",
      "[ 416/1000] train_loss: 47.55446 valid_loss: 41.40707\n",
      "Validation loss decreased (41.429844 --> 41.407070).  Saving model ...\n",
      "[ 417/1000] train_loss: 47.10497 valid_loss: 41.38199\n",
      "Validation loss decreased (41.407070 --> 41.381992).  Saving model ...\n",
      "[ 418/1000] train_loss: 47.23788 valid_loss: 41.35975\n",
      "Validation loss decreased (41.381992 --> 41.359753).  Saving model ...\n",
      "[ 419/1000] train_loss: 47.07740 valid_loss: 41.34436\n",
      "Validation loss decreased (41.359753 --> 41.344357).  Saving model ...\n",
      "[ 420/1000] train_loss: 46.87485 valid_loss: 41.32861\n",
      "Validation loss decreased (41.344357 --> 41.328613).  Saving model ...\n",
      "[ 421/1000] train_loss: 47.46251 valid_loss: 41.30843\n",
      "Validation loss decreased (41.328613 --> 41.308430).  Saving model ...\n",
      "[ 422/1000] train_loss: 46.93678 valid_loss: 41.28882\n",
      "Validation loss decreased (41.308430 --> 41.288818).  Saving model ...\n",
      "[ 423/1000] train_loss: 46.87136 valid_loss: 41.27196\n",
      "Validation loss decreased (41.288818 --> 41.271957).  Saving model ...\n",
      "[ 424/1000] train_loss: 46.90453 valid_loss: 41.26035\n",
      "Validation loss decreased (41.271957 --> 41.260353).  Saving model ...\n",
      "[ 425/1000] train_loss: 46.80201 valid_loss: 41.24646\n",
      "Validation loss decreased (41.260353 --> 41.246464).  Saving model ...\n",
      "[ 426/1000] train_loss: 47.04259 valid_loss: 41.22194\n",
      "Validation loss decreased (41.246464 --> 41.221935).  Saving model ...\n",
      "[ 427/1000] train_loss: 47.66465 valid_loss: 41.19228\n",
      "Validation loss decreased (41.221935 --> 41.192284).  Saving model ...\n",
      "[ 428/1000] train_loss: 47.60855 valid_loss: 41.16742\n",
      "Validation loss decreased (41.192284 --> 41.167419).  Saving model ...\n",
      "[ 429/1000] train_loss: 47.16349 valid_loss: 41.15486\n",
      "Validation loss decreased (41.167419 --> 41.154861).  Saving model ...\n",
      "[ 430/1000] train_loss: 47.13683 valid_loss: 41.14090\n",
      "Validation loss decreased (41.154861 --> 41.140896).  Saving model ...\n",
      "[ 431/1000] train_loss: 48.12368 valid_loss: 41.11654\n",
      "Validation loss decreased (41.140896 --> 41.116539).  Saving model ...\n",
      "[ 432/1000] train_loss: 47.01495 valid_loss: 41.09877\n",
      "Validation loss decreased (41.116539 --> 41.098770).  Saving model ...\n",
      "[ 433/1000] train_loss: 46.71402 valid_loss: 41.08134\n",
      "Validation loss decreased (41.098770 --> 41.081345).  Saving model ...\n",
      "[ 434/1000] train_loss: 46.43628 valid_loss: 41.06723\n",
      "Validation loss decreased (41.081345 --> 41.067230).  Saving model ...\n",
      "[ 435/1000] train_loss: 47.43649 valid_loss: 41.06658\n",
      "Validation loss decreased (41.067230 --> 41.066582).  Saving model ...\n",
      "[ 436/1000] train_loss: 46.86024 valid_loss: 41.06782\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 437/1000] train_loss: 46.18897 valid_loss: 41.05503\n",
      "Validation loss decreased (41.066582 --> 41.055035).  Saving model ...\n",
      "[ 438/1000] train_loss: 46.18927 valid_loss: 41.02980\n",
      "Validation loss decreased (41.055035 --> 41.029797).  Saving model ...\n",
      "[ 439/1000] train_loss: 46.89272 valid_loss: 41.01471\n",
      "Validation loss decreased (41.029797 --> 41.014706).  Saving model ...\n",
      "[ 440/1000] train_loss: 46.79335 valid_loss: 41.00170\n",
      "Validation loss decreased (41.014706 --> 41.001701).  Saving model ...\n",
      "[ 441/1000] train_loss: 46.33543 valid_loss: 40.98627\n",
      "Validation loss decreased (41.001701 --> 40.986267).  Saving model ...\n",
      "[ 442/1000] train_loss: 46.92678 valid_loss: 40.96978\n",
      "Validation loss decreased (40.986267 --> 40.969784).  Saving model ...\n",
      "[ 443/1000] train_loss: 47.10097 valid_loss: 40.95514\n",
      "Validation loss decreased (40.969784 --> 40.955139).  Saving model ...\n",
      "[ 444/1000] train_loss: 46.80082 valid_loss: 40.93931\n",
      "Validation loss decreased (40.955139 --> 40.939308).  Saving model ...\n",
      "[ 445/1000] train_loss: 47.25327 valid_loss: 40.91987\n",
      "Validation loss decreased (40.939308 --> 40.919868).  Saving model ...\n",
      "[ 446/1000] train_loss: 46.85722 valid_loss: 40.89810\n",
      "Validation loss decreased (40.919868 --> 40.898102).  Saving model ...\n",
      "[ 447/1000] train_loss: 46.63432 valid_loss: 40.87705\n",
      "Validation loss decreased (40.898102 --> 40.877052).  Saving model ...\n",
      "[ 448/1000] train_loss: 46.34418 valid_loss: 40.85628\n",
      "Validation loss decreased (40.877052 --> 40.856277).  Saving model ...\n",
      "[ 449/1000] train_loss: 46.69244 valid_loss: 40.83148\n",
      "Validation loss decreased (40.856277 --> 40.831482).  Saving model ...\n",
      "[ 450/1000] train_loss: 46.36810 valid_loss: 40.80881\n",
      "Validation loss decreased (40.831482 --> 40.808811).  Saving model ...\n",
      "[ 451/1000] train_loss: 47.17215 valid_loss: 40.78999\n",
      "Validation loss decreased (40.808811 --> 40.789986).  Saving model ...\n",
      "[ 452/1000] train_loss: 46.55330 valid_loss: 40.77315\n",
      "Validation loss decreased (40.789986 --> 40.773148).  Saving model ...\n",
      "[ 453/1000] train_loss: 46.32062 valid_loss: 40.75543\n",
      "Validation loss decreased (40.773148 --> 40.755432).  Saving model ...\n",
      "[ 454/1000] train_loss: 47.11231 valid_loss: 40.74023\n",
      "Validation loss decreased (40.755432 --> 40.740234).  Saving model ...\n",
      "[ 455/1000] train_loss: 46.42948 valid_loss: 40.72330\n",
      "Validation loss decreased (40.740234 --> 40.723297).  Saving model ...\n",
      "[ 456/1000] train_loss: 46.46671 valid_loss: 40.70650\n",
      "Validation loss decreased (40.723297 --> 40.706497).  Saving model ...\n",
      "[ 457/1000] train_loss: 46.61725 valid_loss: 40.69098\n",
      "Validation loss decreased (40.706497 --> 40.690979).  Saving model ...\n",
      "[ 458/1000] train_loss: 46.06391 valid_loss: 40.68058\n",
      "Validation loss decreased (40.690979 --> 40.680580).  Saving model ...\n",
      "[ 459/1000] train_loss: 46.24908 valid_loss: 40.66434\n",
      "Validation loss decreased (40.680580 --> 40.664345).  Saving model ...\n",
      "[ 460/1000] train_loss: 45.88910 valid_loss: 40.65095\n",
      "Validation loss decreased (40.664345 --> 40.650951).  Saving model ...\n",
      "[ 461/1000] train_loss: 45.56186 valid_loss: 40.63684\n",
      "Validation loss decreased (40.650951 --> 40.636837).  Saving model ...\n",
      "[ 462/1000] train_loss: 46.68202 valid_loss: 40.62139\n",
      "Validation loss decreased (40.636837 --> 40.621391).  Saving model ...\n",
      "[ 463/1000] train_loss: 46.10050 valid_loss: 40.60707\n",
      "Validation loss decreased (40.621391 --> 40.607071).  Saving model ...\n",
      "[ 464/1000] train_loss: 46.56847 valid_loss: 40.59002\n",
      "Validation loss decreased (40.607071 --> 40.590023).  Saving model ...\n",
      "[ 465/1000] train_loss: 46.72042 valid_loss: 40.57069\n",
      "Validation loss decreased (40.590023 --> 40.570686).  Saving model ...\n",
      "[ 466/1000] train_loss: 46.70855 valid_loss: 40.54403\n",
      "Validation loss decreased (40.570686 --> 40.544025).  Saving model ...\n",
      "[ 467/1000] train_loss: 46.24819 valid_loss: 40.51655\n",
      "Validation loss decreased (40.544025 --> 40.516548).  Saving model ...\n",
      "[ 468/1000] train_loss: 46.78404 valid_loss: 40.48524\n",
      "Validation loss decreased (40.516548 --> 40.485245).  Saving model ...\n",
      "[ 469/1000] train_loss: 46.07103 valid_loss: 40.46170\n",
      "Validation loss decreased (40.485245 --> 40.461700).  Saving model ...\n",
      "[ 470/1000] train_loss: 45.42974 valid_loss: 40.44282\n",
      "Validation loss decreased (40.461700 --> 40.442818).  Saving model ...\n",
      "[ 471/1000] train_loss: 46.14965 valid_loss: 40.42278\n",
      "Validation loss decreased (40.442818 --> 40.422783).  Saving model ...\n",
      "[ 472/1000] train_loss: 46.11466 valid_loss: 40.40261\n",
      "Validation loss decreased (40.422783 --> 40.402611).  Saving model ...\n",
      "[ 473/1000] train_loss: 45.27734 valid_loss: 40.38380\n",
      "Validation loss decreased (40.402611 --> 40.383804).  Saving model ...\n",
      "[ 474/1000] train_loss: 45.99276 valid_loss: 40.36408\n",
      "Validation loss decreased (40.383804 --> 40.364079).  Saving model ...\n",
      "[ 475/1000] train_loss: 46.38515 valid_loss: 40.34701\n",
      "Validation loss decreased (40.364079 --> 40.347012).  Saving model ...\n",
      "[ 476/1000] train_loss: 46.60312 valid_loss: 40.33034\n",
      "Validation loss decreased (40.347012 --> 40.330341).  Saving model ...\n",
      "[ 477/1000] train_loss: 45.49522 valid_loss: 40.31382\n",
      "Validation loss decreased (40.330341 --> 40.313816).  Saving model ...\n",
      "[ 478/1000] train_loss: 45.75583 valid_loss: 40.29986\n",
      "Validation loss decreased (40.313816 --> 40.299858).  Saving model ...\n",
      "[ 479/1000] train_loss: 45.88834 valid_loss: 40.28939\n",
      "Validation loss decreased (40.299858 --> 40.289387).  Saving model ...\n",
      "[ 480/1000] train_loss: 45.98064 valid_loss: 40.27381\n",
      "Validation loss decreased (40.289387 --> 40.273811).  Saving model ...\n",
      "[ 481/1000] train_loss: 46.00847 valid_loss: 40.25410\n",
      "Validation loss decreased (40.273811 --> 40.254105).  Saving model ...\n",
      "[ 482/1000] train_loss: 45.02518 valid_loss: 40.23697\n",
      "Validation loss decreased (40.254105 --> 40.236969).  Saving model ...\n",
      "[ 483/1000] train_loss: 45.48809 valid_loss: 40.22135\n",
      "Validation loss decreased (40.236969 --> 40.221348).  Saving model ...\n",
      "[ 484/1000] train_loss: 45.70693 valid_loss: 40.20340\n",
      "Validation loss decreased (40.221348 --> 40.203403).  Saving model ...\n",
      "[ 485/1000] train_loss: 44.85411 valid_loss: 40.18356\n",
      "Validation loss decreased (40.203403 --> 40.183563).  Saving model ...\n",
      "[ 486/1000] train_loss: 46.17099 valid_loss: 40.15921\n",
      "Validation loss decreased (40.183563 --> 40.159206).  Saving model ...\n",
      "[ 487/1000] train_loss: 45.40647 valid_loss: 40.13871\n",
      "Validation loss decreased (40.159206 --> 40.138710).  Saving model ...\n",
      "[ 488/1000] train_loss: 45.56793 valid_loss: 40.12072\n",
      "Validation loss decreased (40.138710 --> 40.120724).  Saving model ...\n",
      "[ 489/1000] train_loss: 45.62502 valid_loss: 40.09713\n",
      "Validation loss decreased (40.120724 --> 40.097134).  Saving model ...\n",
      "[ 490/1000] train_loss: 45.48466 valid_loss: 40.07407\n",
      "Validation loss decreased (40.097134 --> 40.074074).  Saving model ...\n",
      "[ 491/1000] train_loss: 46.01271 valid_loss: 40.05025\n",
      "Validation loss decreased (40.074074 --> 40.050255).  Saving model ...\n",
      "[ 492/1000] train_loss: 46.10823 valid_loss: 40.02424\n",
      "Validation loss decreased (40.050255 --> 40.024242).  Saving model ...\n",
      "[ 493/1000] train_loss: 45.76819 valid_loss: 40.00324\n",
      "Validation loss decreased (40.024242 --> 40.003239).  Saving model ...\n",
      "[ 494/1000] train_loss: 45.49618 valid_loss: 39.97904\n",
      "Validation loss decreased (40.003239 --> 39.979042).  Saving model ...\n",
      "[ 495/1000] train_loss: 45.73481 valid_loss: 39.95348\n",
      "Validation loss decreased (39.979042 --> 39.953476).  Saving model ...\n",
      "[ 496/1000] train_loss: 44.43755 valid_loss: 39.93291\n",
      "Validation loss decreased (39.953476 --> 39.932915).  Saving model ...\n",
      "[ 497/1000] train_loss: 45.12131 valid_loss: 39.91481\n",
      "Validation loss decreased (39.932915 --> 39.914814).  Saving model ...\n",
      "[ 498/1000] train_loss: 46.07832 valid_loss: 39.89067\n",
      "Validation loss decreased (39.914814 --> 39.890671).  Saving model ...\n",
      "[ 499/1000] train_loss: 45.27464 valid_loss: 39.86969\n",
      "Validation loss decreased (39.890671 --> 39.869690).  Saving model ...\n",
      "[ 500/1000] train_loss: 45.08045 valid_loss: 39.84446\n",
      "Validation loss decreased (39.869690 --> 39.844463).  Saving model ...\n",
      "[ 501/1000] train_loss: 45.19038 valid_loss: 39.81510\n",
      "Validation loss decreased (39.844463 --> 39.815098).  Saving model ...\n",
      "[ 502/1000] train_loss: 44.71627 valid_loss: 39.79424\n",
      "Validation loss decreased (39.815098 --> 39.794235).  Saving model ...\n",
      "[ 503/1000] train_loss: 45.85807 valid_loss: 39.77739\n",
      "Validation loss decreased (39.794235 --> 39.777390).  Saving model ...\n",
      "[ 504/1000] train_loss: 44.51854 valid_loss: 39.75770\n",
      "Validation loss decreased (39.777390 --> 39.757702).  Saving model ...\n",
      "[ 505/1000] train_loss: 45.30538 valid_loss: 39.73090\n",
      "Validation loss decreased (39.757702 --> 39.730896).  Saving model ...\n",
      "[ 506/1000] train_loss: 45.56021 valid_loss: 39.70614\n",
      "Validation loss decreased (39.730896 --> 39.706142).  Saving model ...\n",
      "[ 507/1000] train_loss: 44.72503 valid_loss: 39.68548\n",
      "Validation loss decreased (39.706142 --> 39.685478).  Saving model ...\n",
      "[ 508/1000] train_loss: 44.62618 valid_loss: 39.66328\n",
      "Validation loss decreased (39.685478 --> 39.663284).  Saving model ...\n",
      "[ 509/1000] train_loss: 44.62027 valid_loss: 39.63638\n",
      "Validation loss decreased (39.663284 --> 39.636375).  Saving model ...\n",
      "[ 510/1000] train_loss: 44.86214 valid_loss: 39.60551\n",
      "Validation loss decreased (39.636375 --> 39.605507).  Saving model ...\n",
      "[ 511/1000] train_loss: 44.46390 valid_loss: 39.58177\n",
      "Validation loss decreased (39.605507 --> 39.581772).  Saving model ...\n",
      "[ 512/1000] train_loss: 45.53445 valid_loss: 39.56043\n",
      "Validation loss decreased (39.581772 --> 39.560432).  Saving model ...\n",
      "[ 513/1000] train_loss: 45.04478 valid_loss: 39.53686\n",
      "Validation loss decreased (39.560432 --> 39.536861).  Saving model ...\n",
      "[ 514/1000] train_loss: 44.47874 valid_loss: 39.50986\n",
      "Validation loss decreased (39.536861 --> 39.509857).  Saving model ...\n",
      "[ 515/1000] train_loss: 44.92197 valid_loss: 39.48403\n",
      "Validation loss decreased (39.509857 --> 39.484028).  Saving model ...\n",
      "[ 516/1000] train_loss: 44.74525 valid_loss: 39.45297\n",
      "Validation loss decreased (39.484028 --> 39.452972).  Saving model ...\n",
      "[ 517/1000] train_loss: 44.01971 valid_loss: 39.41825\n",
      "Validation loss decreased (39.452972 --> 39.418251).  Saving model ...\n",
      "[ 518/1000] train_loss: 45.35418 valid_loss: 39.38712\n",
      "Validation loss decreased (39.418251 --> 39.387115).  Saving model ...\n",
      "[ 519/1000] train_loss: 44.91322 valid_loss: 39.35598\n",
      "Validation loss decreased (39.387115 --> 39.355980).  Saving model ...\n",
      "[ 520/1000] train_loss: 44.88659 valid_loss: 39.32511\n",
      "Validation loss decreased (39.355980 --> 39.325108).  Saving model ...\n",
      "[ 521/1000] train_loss: 45.03109 valid_loss: 39.29547\n",
      "Validation loss decreased (39.325108 --> 39.295471).  Saving model ...\n",
      "[ 522/1000] train_loss: 44.27891 valid_loss: 39.26801\n",
      "Validation loss decreased (39.295471 --> 39.268013).  Saving model ...\n",
      "[ 523/1000] train_loss: 44.15827 valid_loss: 39.24260\n",
      "Validation loss decreased (39.268013 --> 39.242599).  Saving model ...\n",
      "[ 524/1000] train_loss: 45.05698 valid_loss: 39.21448\n",
      "Validation loss decreased (39.242599 --> 39.214481).  Saving model ...\n",
      "[ 525/1000] train_loss: 44.66561 valid_loss: 39.18491\n",
      "Validation loss decreased (39.214481 --> 39.184914).  Saving model ...\n",
      "[ 526/1000] train_loss: 43.68258 valid_loss: 39.15675\n",
      "Validation loss decreased (39.184914 --> 39.156750).  Saving model ...\n",
      "[ 527/1000] train_loss: 43.64819 valid_loss: 39.13132\n",
      "Validation loss decreased (39.156750 --> 39.131321).  Saving model ...\n",
      "[ 528/1000] train_loss: 43.81646 valid_loss: 39.10554\n",
      "Validation loss decreased (39.131321 --> 39.105537).  Saving model ...\n",
      "[ 529/1000] train_loss: 44.43808 valid_loss: 39.07986\n",
      "Validation loss decreased (39.105537 --> 39.079861).  Saving model ...\n",
      "[ 530/1000] train_loss: 44.57473 valid_loss: 39.05286\n",
      "Validation loss decreased (39.079861 --> 39.052856).  Saving model ...\n",
      "[ 531/1000] train_loss: 43.86670 valid_loss: 39.01986\n",
      "Validation loss decreased (39.052856 --> 39.019859).  Saving model ...\n",
      "[ 532/1000] train_loss: 45.16999 valid_loss: 38.98352\n",
      "Validation loss decreased (39.019859 --> 38.983524).  Saving model ...\n",
      "[ 533/1000] train_loss: 44.38250 valid_loss: 38.96144\n",
      "Validation loss decreased (38.983524 --> 38.961437).  Saving model ...\n",
      "[ 534/1000] train_loss: 44.15522 valid_loss: 38.94600\n",
      "Validation loss decreased (38.961437 --> 38.945995).  Saving model ...\n",
      "[ 535/1000] train_loss: 43.90353 valid_loss: 38.91912\n",
      "Validation loss decreased (38.945995 --> 38.919121).  Saving model ...\n",
      "[ 536/1000] train_loss: 43.50430 valid_loss: 38.86501\n",
      "Validation loss decreased (38.919121 --> 38.865013).  Saving model ...\n",
      "[ 537/1000] train_loss: 45.03962 valid_loss: 38.80254\n",
      "Validation loss decreased (38.865013 --> 38.802540).  Saving model ...\n",
      "[ 538/1000] train_loss: 44.20004 valid_loss: 38.75500\n",
      "Validation loss decreased (38.802540 --> 38.755005).  Saving model ...\n",
      "[ 539/1000] train_loss: 44.26981 valid_loss: 38.71940\n",
      "Validation loss decreased (38.755005 --> 38.719402).  Saving model ...\n",
      "[ 540/1000] train_loss: 43.76447 valid_loss: 38.68829\n",
      "Validation loss decreased (38.719402 --> 38.688290).  Saving model ...\n",
      "[ 541/1000] train_loss: 44.63677 valid_loss: 38.65036\n",
      "Validation loss decreased (38.688290 --> 38.650360).  Saving model ...\n",
      "[ 542/1000] train_loss: 43.52678 valid_loss: 38.60936\n",
      "Validation loss decreased (38.650360 --> 38.609360).  Saving model ...\n",
      "[ 543/1000] train_loss: 44.96272 valid_loss: 38.56911\n",
      "Validation loss decreased (38.609360 --> 38.569107).  Saving model ...\n",
      "[ 544/1000] train_loss: 43.53185 valid_loss: 38.53748\n",
      "Validation loss decreased (38.569107 --> 38.537476).  Saving model ...\n",
      "[ 545/1000] train_loss: 42.86900 valid_loss: 38.50529\n",
      "Validation loss decreased (38.537476 --> 38.505291).  Saving model ...\n",
      "[ 546/1000] train_loss: 43.40407 valid_loss: 38.46841\n",
      "Validation loss decreased (38.505291 --> 38.468410).  Saving model ...\n",
      "[ 547/1000] train_loss: 43.92217 valid_loss: 38.42789\n",
      "Validation loss decreased (38.468410 --> 38.427895).  Saving model ...\n",
      "[ 548/1000] train_loss: 42.99024 valid_loss: 38.38202\n",
      "Validation loss decreased (38.427895 --> 38.382019).  Saving model ...\n",
      "[ 549/1000] train_loss: 43.58665 valid_loss: 38.33359\n",
      "Validation loss decreased (38.382019 --> 38.333591).  Saving model ...\n",
      "[ 550/1000] train_loss: 43.60937 valid_loss: 38.28671\n",
      "Validation loss decreased (38.333591 --> 38.286705).  Saving model ...\n",
      "[ 551/1000] train_loss: 43.54642 valid_loss: 38.24183\n",
      "Validation loss decreased (38.286705 --> 38.241825).  Saving model ...\n",
      "[ 552/1000] train_loss: 43.50835 valid_loss: 38.19704\n",
      "Validation loss decreased (38.241825 --> 38.197037).  Saving model ...\n",
      "[ 553/1000] train_loss: 43.12139 valid_loss: 38.15475\n",
      "Validation loss decreased (38.197037 --> 38.154747).  Saving model ...\n",
      "[ 554/1000] train_loss: 43.07651 valid_loss: 38.11477\n",
      "Validation loss decreased (38.154747 --> 38.114769).  Saving model ...\n",
      "[ 555/1000] train_loss: 43.21451 valid_loss: 38.07020\n",
      "Validation loss decreased (38.114769 --> 38.070202).  Saving model ...\n",
      "[ 556/1000] train_loss: 43.18680 valid_loss: 38.02664\n",
      "Validation loss decreased (38.070202 --> 38.026638).  Saving model ...\n",
      "[ 557/1000] train_loss: 43.25573 valid_loss: 37.98755\n",
      "Validation loss decreased (38.026638 --> 37.987549).  Saving model ...\n",
      "[ 558/1000] train_loss: 43.47364 valid_loss: 37.94596\n",
      "Validation loss decreased (37.987549 --> 37.945957).  Saving model ...\n",
      "[ 559/1000] train_loss: 42.92358 valid_loss: 37.89890\n",
      "Validation loss decreased (37.945957 --> 37.898903).  Saving model ...\n",
      "[ 560/1000] train_loss: 42.77292 valid_loss: 37.85416\n",
      "Validation loss decreased (37.898903 --> 37.854156).  Saving model ...\n",
      "[ 561/1000] train_loss: 42.80283 valid_loss: 37.80952\n",
      "Validation loss decreased (37.854156 --> 37.809525).  Saving model ...\n",
      "[ 562/1000] train_loss: 43.18738 valid_loss: 37.76645\n",
      "Validation loss decreased (37.809525 --> 37.766445).  Saving model ...\n",
      "[ 563/1000] train_loss: 42.45716 valid_loss: 37.72546\n",
      "Validation loss decreased (37.766445 --> 37.725460).  Saving model ...\n",
      "[ 564/1000] train_loss: 43.12742 valid_loss: 37.66688\n",
      "Validation loss decreased (37.725460 --> 37.666882).  Saving model ...\n",
      "[ 565/1000] train_loss: 42.54154 valid_loss: 37.60614\n",
      "Validation loss decreased (37.666882 --> 37.606144).  Saving model ...\n",
      "[ 566/1000] train_loss: 42.55978 valid_loss: 37.54864\n",
      "Validation loss decreased (37.606144 --> 37.548637).  Saving model ...\n",
      "[ 567/1000] train_loss: 42.86657 valid_loss: 37.49193\n",
      "Validation loss decreased (37.548637 --> 37.491932).  Saving model ...\n",
      "[ 568/1000] train_loss: 42.89117 valid_loss: 37.43639\n",
      "Validation loss decreased (37.491932 --> 37.436394).  Saving model ...\n",
      "[ 569/1000] train_loss: 42.56841 valid_loss: 37.38049\n",
      "Validation loss decreased (37.436394 --> 37.380493).  Saving model ...\n",
      "[ 570/1000] train_loss: 42.70902 valid_loss: 37.32523\n",
      "Validation loss decreased (37.380493 --> 37.325230).  Saving model ...\n",
      "[ 571/1000] train_loss: 42.45348 valid_loss: 37.27390\n",
      "Validation loss decreased (37.325230 --> 37.273895).  Saving model ...\n",
      "[ 572/1000] train_loss: 42.12777 valid_loss: 37.22306\n",
      "Validation loss decreased (37.273895 --> 37.223064).  Saving model ...\n",
      "[ 573/1000] train_loss: 42.20757 valid_loss: 37.17101\n",
      "Validation loss decreased (37.223064 --> 37.171013).  Saving model ...\n",
      "[ 574/1000] train_loss: 41.85199 valid_loss: 37.12287\n",
      "Validation loss decreased (37.171013 --> 37.122868).  Saving model ...\n",
      "[ 575/1000] train_loss: 41.75719 valid_loss: 37.07063\n",
      "Validation loss decreased (37.122868 --> 37.070629).  Saving model ...\n",
      "[ 576/1000] train_loss: 41.58600 valid_loss: 37.01202\n",
      "Validation loss decreased (37.070629 --> 37.012016).  Saving model ...\n",
      "[ 577/1000] train_loss: 42.16867 valid_loss: 36.94449\n",
      "Validation loss decreased (37.012016 --> 36.944489).  Saving model ...\n",
      "[ 578/1000] train_loss: 41.90267 valid_loss: 36.87457\n",
      "Validation loss decreased (36.944489 --> 36.874569).  Saving model ...\n",
      "[ 579/1000] train_loss: 42.26635 valid_loss: 36.80001\n",
      "Validation loss decreased (36.874569 --> 36.800014).  Saving model ...\n",
      "[ 580/1000] train_loss: 42.26955 valid_loss: 36.72287\n",
      "Validation loss decreased (36.800014 --> 36.722874).  Saving model ...\n",
      "[ 581/1000] train_loss: 41.34348 valid_loss: 36.65014\n",
      "Validation loss decreased (36.722874 --> 36.650139).  Saving model ...\n",
      "[ 582/1000] train_loss: 41.72675 valid_loss: 36.58364\n",
      "Validation loss decreased (36.650139 --> 36.583645).  Saving model ...\n",
      "[ 583/1000] train_loss: 41.07958 valid_loss: 36.52436\n",
      "Validation loss decreased (36.583645 --> 36.524357).  Saving model ...\n",
      "[ 584/1000] train_loss: 40.64288 valid_loss: 36.45900\n",
      "Validation loss decreased (36.524357 --> 36.459003).  Saving model ...\n",
      "[ 585/1000] train_loss: 40.80295 valid_loss: 36.38337\n",
      "Validation loss decreased (36.459003 --> 36.383373).  Saving model ...\n",
      "[ 586/1000] train_loss: 41.63712 valid_loss: 36.31931\n",
      "Validation loss decreased (36.383373 --> 36.319313).  Saving model ...\n",
      "[ 587/1000] train_loss: 41.90042 valid_loss: 36.26943\n",
      "Validation loss decreased (36.319313 --> 36.269428).  Saving model ...\n",
      "[ 588/1000] train_loss: 41.82650 valid_loss: 36.20041\n",
      "Validation loss decreased (36.269428 --> 36.200413).  Saving model ...\n",
      "[ 589/1000] train_loss: 40.62445 valid_loss: 36.10829\n",
      "Validation loss decreased (36.200413 --> 36.108288).  Saving model ...\n",
      "[ 590/1000] train_loss: 41.09304 valid_loss: 36.01908\n",
      "Validation loss decreased (36.108288 --> 36.019081).  Saving model ...\n",
      "[ 591/1000] train_loss: 41.24155 valid_loss: 35.93484\n",
      "Validation loss decreased (36.019081 --> 35.934837).  Saving model ...\n",
      "[ 592/1000] train_loss: 40.80835 valid_loss: 35.85717\n",
      "Validation loss decreased (35.934837 --> 35.857170).  Saving model ...\n",
      "[ 593/1000] train_loss: 40.84521 valid_loss: 35.78328\n",
      "Validation loss decreased (35.857170 --> 35.783279).  Saving model ...\n",
      "[ 594/1000] train_loss: 40.90408 valid_loss: 35.71038\n",
      "Validation loss decreased (35.783279 --> 35.710381).  Saving model ...\n",
      "[ 595/1000] train_loss: 40.34917 valid_loss: 35.64462\n",
      "Validation loss decreased (35.710381 --> 35.644623).  Saving model ...\n",
      "[ 596/1000] train_loss: 40.36900 valid_loss: 35.56619\n",
      "Validation loss decreased (35.644623 --> 35.566193).  Saving model ...\n",
      "[ 597/1000] train_loss: 40.49174 valid_loss: 35.47221\n",
      "Validation loss decreased (35.566193 --> 35.472206).  Saving model ...\n",
      "[ 598/1000] train_loss: 40.79865 valid_loss: 35.36671\n",
      "Validation loss decreased (35.472206 --> 35.366711).  Saving model ...\n",
      "[ 599/1000] train_loss: 40.59834 valid_loss: 35.26999\n",
      "Validation loss decreased (35.366711 --> 35.269985).  Saving model ...\n",
      "[ 600/1000] train_loss: 40.62085 valid_loss: 35.15820\n",
      "Validation loss decreased (35.269985 --> 35.158203).  Saving model ...\n",
      "[ 601/1000] train_loss: 40.24765 valid_loss: 35.04352\n",
      "Validation loss decreased (35.158203 --> 35.043522).  Saving model ...\n",
      "[ 602/1000] train_loss: 40.04745 valid_loss: 34.94424\n",
      "Validation loss decreased (35.043522 --> 34.944241).  Saving model ...\n",
      "[ 603/1000] train_loss: 39.74977 valid_loss: 34.85147\n",
      "Validation loss decreased (34.944241 --> 34.851471).  Saving model ...\n",
      "[ 604/1000] train_loss: 40.86119 valid_loss: 34.75017\n",
      "Validation loss decreased (34.851471 --> 34.750172).  Saving model ...\n",
      "[ 605/1000] train_loss: 40.34610 valid_loss: 34.64177\n",
      "Validation loss decreased (34.750172 --> 34.641773).  Saving model ...\n",
      "[ 606/1000] train_loss: 39.68690 valid_loss: 34.53619\n",
      "Validation loss decreased (34.641773 --> 34.536194).  Saving model ...\n",
      "[ 607/1000] train_loss: 39.36457 valid_loss: 34.43446\n",
      "Validation loss decreased (34.536194 --> 34.434456).  Saving model ...\n",
      "[ 608/1000] train_loss: 39.09726 valid_loss: 34.33399\n",
      "Validation loss decreased (34.434456 --> 34.333992).  Saving model ...\n",
      "[ 609/1000] train_loss: 39.64486 valid_loss: 34.23820\n",
      "Validation loss decreased (34.333992 --> 34.238197).  Saving model ...\n",
      "[ 610/1000] train_loss: 39.72732 valid_loss: 34.13689\n",
      "Validation loss decreased (34.238197 --> 34.136887).  Saving model ...\n",
      "[ 611/1000] train_loss: 39.05953 valid_loss: 34.00645\n",
      "Validation loss decreased (34.136887 --> 34.006454).  Saving model ...\n",
      "[ 612/1000] train_loss: 38.66304 valid_loss: 33.87138\n",
      "Validation loss decreased (34.006454 --> 33.871376).  Saving model ...\n",
      "[ 613/1000] train_loss: 39.36974 valid_loss: 33.74340\n",
      "Validation loss decreased (33.871376 --> 33.743397).  Saving model ...\n",
      "[ 614/1000] train_loss: 38.78896 valid_loss: 33.61873\n",
      "Validation loss decreased (33.743397 --> 33.618732).  Saving model ...\n",
      "[ 615/1000] train_loss: 39.32280 valid_loss: 33.49492\n",
      "Validation loss decreased (33.618732 --> 33.494923).  Saving model ...\n",
      "[ 616/1000] train_loss: 39.03568 valid_loss: 33.37405\n",
      "Validation loss decreased (33.494923 --> 33.374050).  Saving model ...\n",
      "[ 617/1000] train_loss: 38.88553 valid_loss: 33.23922\n",
      "Validation loss decreased (33.374050 --> 33.239216).  Saving model ...\n",
      "[ 618/1000] train_loss: 38.17903 valid_loss: 33.11551\n",
      "Validation loss decreased (33.239216 --> 33.115505).  Saving model ...\n",
      "[ 619/1000] train_loss: 38.17681 valid_loss: 32.99385\n",
      "Validation loss decreased (33.115505 --> 32.993855).  Saving model ...\n",
      "[ 620/1000] train_loss: 38.34676 valid_loss: 32.86511\n",
      "Validation loss decreased (32.993855 --> 32.865108).  Saving model ...\n",
      "[ 621/1000] train_loss: 37.55619 valid_loss: 32.72598\n",
      "Validation loss decreased (32.865108 --> 32.725975).  Saving model ...\n",
      "[ 622/1000] train_loss: 38.66838 valid_loss: 32.57009\n",
      "Validation loss decreased (32.725975 --> 32.570091).  Saving model ...\n",
      "[ 623/1000] train_loss: 37.20159 valid_loss: 32.41028\n",
      "Validation loss decreased (32.570091 --> 32.410278).  Saving model ...\n",
      "[ 624/1000] train_loss: 37.75015 valid_loss: 32.25487\n",
      "Validation loss decreased (32.410278 --> 32.254868).  Saving model ...\n",
      "[ 625/1000] train_loss: 37.56867 valid_loss: 32.10500\n",
      "Validation loss decreased (32.254868 --> 32.105000).  Saving model ...\n",
      "[ 626/1000] train_loss: 37.60105 valid_loss: 31.95127\n",
      "Validation loss decreased (32.105000 --> 31.951269).  Saving model ...\n",
      "[ 627/1000] train_loss: 37.31318 valid_loss: 31.79379\n",
      "Validation loss decreased (31.951269 --> 31.793791).  Saving model ...\n",
      "[ 628/1000] train_loss: 37.53149 valid_loss: 31.63697\n",
      "Validation loss decreased (31.793791 --> 31.636969).  Saving model ...\n",
      "[ 629/1000] train_loss: 36.43835 valid_loss: 31.47349\n",
      "Validation loss decreased (31.636969 --> 31.473492).  Saving model ...\n",
      "[ 630/1000] train_loss: 37.27395 valid_loss: 31.30103\n",
      "Validation loss decreased (31.473492 --> 31.301027).  Saving model ...\n",
      "[ 631/1000] train_loss: 36.56849 valid_loss: 31.12695\n",
      "Validation loss decreased (31.301027 --> 31.126945).  Saving model ...\n",
      "[ 632/1000] train_loss: 36.70383 valid_loss: 30.94818\n",
      "Validation loss decreased (31.126945 --> 30.948183).  Saving model ...\n",
      "[ 633/1000] train_loss: 36.37872 valid_loss: 30.76817\n",
      "Validation loss decreased (30.948183 --> 30.768171).  Saving model ...\n",
      "[ 634/1000] train_loss: 36.71354 valid_loss: 30.58882\n",
      "Validation loss decreased (30.768171 --> 30.588821).  Saving model ...\n",
      "[ 635/1000] train_loss: 36.28714 valid_loss: 30.40023\n",
      "Validation loss decreased (30.588821 --> 30.400229).  Saving model ...\n",
      "[ 636/1000] train_loss: 35.89991 valid_loss: 30.20603\n",
      "Validation loss decreased (30.400229 --> 30.206028).  Saving model ...\n",
      "[ 637/1000] train_loss: 36.30602 valid_loss: 30.00726\n",
      "Validation loss decreased (30.206028 --> 30.007263).  Saving model ...\n",
      "[ 638/1000] train_loss: 36.25881 valid_loss: 29.80639\n",
      "Validation loss decreased (30.007263 --> 29.806395).  Saving model ...\n",
      "[ 639/1000] train_loss: 35.26468 valid_loss: 29.60437\n",
      "Validation loss decreased (29.806395 --> 29.604366).  Saving model ...\n",
      "[ 640/1000] train_loss: 34.79953 valid_loss: 29.39783\n",
      "Validation loss decreased (29.604366 --> 29.397833).  Saving model ...\n",
      "[ 641/1000] train_loss: 34.91101 valid_loss: 29.18513\n",
      "Validation loss decreased (29.397833 --> 29.185127).  Saving model ...\n",
      "[ 642/1000] train_loss: 35.25454 valid_loss: 28.96731\n",
      "Validation loss decreased (29.185127 --> 28.967312).  Saving model ...\n",
      "[ 643/1000] train_loss: 35.14666 valid_loss: 28.74696\n",
      "Validation loss decreased (28.967312 --> 28.746964).  Saving model ...\n",
      "[ 644/1000] train_loss: 34.44466 valid_loss: 28.52371\n",
      "Validation loss decreased (28.746964 --> 28.523710).  Saving model ...\n",
      "[ 645/1000] train_loss: 33.98029 valid_loss: 28.29922\n",
      "Validation loss decreased (28.523710 --> 28.299215).  Saving model ...\n",
      "[ 646/1000] train_loss: 33.75699 valid_loss: 28.07158\n",
      "Validation loss decreased (28.299215 --> 28.071581).  Saving model ...\n",
      "[ 647/1000] train_loss: 34.28371 valid_loss: 27.84462\n",
      "Validation loss decreased (28.071581 --> 27.844622).  Saving model ...\n",
      "[ 648/1000] train_loss: 33.56855 valid_loss: 27.61744\n",
      "Validation loss decreased (27.844622 --> 27.617435).  Saving model ...\n",
      "[ 649/1000] train_loss: 33.67511 valid_loss: 27.37866\n",
      "Validation loss decreased (27.617435 --> 27.378662).  Saving model ...\n",
      "[ 650/1000] train_loss: 33.17645 valid_loss: 27.12625\n",
      "Validation loss decreased (27.378662 --> 27.126253).  Saving model ...\n",
      "[ 651/1000] train_loss: 32.92947 valid_loss: 26.87540\n",
      "Validation loss decreased (27.126253 --> 26.875402).  Saving model ...\n",
      "[ 652/1000] train_loss: 32.95475 valid_loss: 26.62039\n",
      "Validation loss decreased (26.875402 --> 26.620388).  Saving model ...\n",
      "[ 653/1000] train_loss: 32.58957 valid_loss: 26.36579\n",
      "Validation loss decreased (26.620388 --> 26.365786).  Saving model ...\n",
      "[ 654/1000] train_loss: 32.83572 valid_loss: 26.10905\n",
      "Validation loss decreased (26.365786 --> 26.109045).  Saving model ...\n",
      "[ 655/1000] train_loss: 32.04367 valid_loss: 25.84874\n",
      "Validation loss decreased (26.109045 --> 25.848742).  Saving model ...\n",
      "[ 656/1000] train_loss: 32.22972 valid_loss: 25.57815\n",
      "Validation loss decreased (25.848742 --> 25.578152).  Saving model ...\n",
      "[ 657/1000] train_loss: 31.82339 valid_loss: 25.30168\n",
      "Validation loss decreased (25.578152 --> 25.301678).  Saving model ...\n",
      "[ 658/1000] train_loss: 32.09031 valid_loss: 25.02935\n",
      "Validation loss decreased (25.301678 --> 25.029348).  Saving model ...\n",
      "[ 659/1000] train_loss: 30.71234 valid_loss: 24.74558\n",
      "Validation loss decreased (25.029348 --> 24.745579).  Saving model ...\n",
      "[ 660/1000] train_loss: 30.96030 valid_loss: 24.45581\n",
      "Validation loss decreased (24.745579 --> 24.455809).  Saving model ...\n",
      "[ 661/1000] train_loss: 30.57050 valid_loss: 24.16399\n",
      "Validation loss decreased (24.455809 --> 24.163986).  Saving model ...\n",
      "[ 662/1000] train_loss: 30.72676 valid_loss: 23.87102\n",
      "Validation loss decreased (24.163986 --> 23.871016).  Saving model ...\n",
      "[ 663/1000] train_loss: 30.09881 valid_loss: 23.57547\n",
      "Validation loss decreased (23.871016 --> 23.575474).  Saving model ...\n",
      "[ 664/1000] train_loss: 30.83606 valid_loss: 23.27242\n",
      "Validation loss decreased (23.575474 --> 23.272425).  Saving model ...\n",
      "[ 665/1000] train_loss: 31.05478 valid_loss: 22.96850\n",
      "Validation loss decreased (23.272425 --> 22.968504).  Saving model ...\n",
      "[ 666/1000] train_loss: 30.00646 valid_loss: 22.65861\n",
      "Validation loss decreased (22.968504 --> 22.658607).  Saving model ...\n",
      "[ 667/1000] train_loss: 29.08238 valid_loss: 22.34969\n",
      "Validation loss decreased (22.658607 --> 22.349691).  Saving model ...\n",
      "[ 668/1000] train_loss: 28.64206 valid_loss: 22.03648\n",
      "Validation loss decreased (22.349691 --> 22.036480).  Saving model ...\n",
      "[ 669/1000] train_loss: 28.51581 valid_loss: 21.70827\n",
      "Validation loss decreased (22.036480 --> 21.708269).  Saving model ...\n",
      "[ 670/1000] train_loss: 28.31106 valid_loss: 21.38020\n",
      "Validation loss decreased (21.708269 --> 21.380201).  Saving model ...\n",
      "[ 671/1000] train_loss: 28.57386 valid_loss: 21.05594\n",
      "Validation loss decreased (21.380201 --> 21.055937).  Saving model ...\n",
      "[ 672/1000] train_loss: 27.63261 valid_loss: 20.72992\n",
      "Validation loss decreased (21.055937 --> 20.729919).  Saving model ...\n",
      "[ 673/1000] train_loss: 27.50463 valid_loss: 20.40548\n",
      "Validation loss decreased (20.729919 --> 20.405483).  Saving model ...\n",
      "[ 674/1000] train_loss: 26.82085 valid_loss: 20.07551\n",
      "Validation loss decreased (20.405483 --> 20.075512).  Saving model ...\n",
      "[ 675/1000] train_loss: 27.01876 valid_loss: 19.73990\n",
      "Validation loss decreased (20.075512 --> 19.739902).  Saving model ...\n",
      "[ 676/1000] train_loss: 26.76651 valid_loss: 19.40388\n",
      "Validation loss decreased (19.739902 --> 19.403879).  Saving model ...\n",
      "[ 677/1000] train_loss: 27.57421 valid_loss: 19.07367\n",
      "Validation loss decreased (19.403879 --> 19.073669).  Saving model ...\n",
      "[ 678/1000] train_loss: 26.07423 valid_loss: 18.74070\n",
      "Validation loss decreased (19.073669 --> 18.740698).  Saving model ...\n",
      "[ 679/1000] train_loss: 25.58654 valid_loss: 18.40768\n",
      "Validation loss decreased (18.740698 --> 18.407682).  Saving model ...\n",
      "[ 680/1000] train_loss: 26.75980 valid_loss: 18.08057\n",
      "Validation loss decreased (18.407682 --> 18.080574).  Saving model ...\n",
      "[ 681/1000] train_loss: 25.33863 valid_loss: 17.74091\n",
      "Validation loss decreased (18.080574 --> 17.740911).  Saving model ...\n",
      "[ 682/1000] train_loss: 25.97608 valid_loss: 17.39627\n",
      "Validation loss decreased (17.740911 --> 17.396275).  Saving model ...\n",
      "[ 683/1000] train_loss: 25.23240 valid_loss: 17.05723\n",
      "Validation loss decreased (17.396275 --> 17.057228).  Saving model ...\n",
      "[ 684/1000] train_loss: 25.14658 valid_loss: 16.72359\n",
      "Validation loss decreased (17.057228 --> 16.723595).  Saving model ...\n",
      "[ 685/1000] train_loss: 24.37626 valid_loss: 16.39638\n",
      "Validation loss decreased (16.723595 --> 16.396376).  Saving model ...\n",
      "[ 686/1000] train_loss: 24.04189 valid_loss: 16.07113\n",
      "Validation loss decreased (16.396376 --> 16.071133).  Saving model ...\n",
      "[ 687/1000] train_loss: 24.05737 valid_loss: 15.74659\n",
      "Validation loss decreased (16.071133 --> 15.746587).  Saving model ...\n",
      "[ 688/1000] train_loss: 23.74756 valid_loss: 15.42069\n",
      "Validation loss decreased (15.746587 --> 15.420689).  Saving model ...\n",
      "[ 689/1000] train_loss: 23.31454 valid_loss: 15.08777\n",
      "Validation loss decreased (15.420689 --> 15.087767).  Saving model ...\n",
      "[ 690/1000] train_loss: 23.38196 valid_loss: 14.75795\n",
      "Validation loss decreased (15.087767 --> 14.757953).  Saving model ...\n",
      "[ 691/1000] train_loss: 23.44747 valid_loss: 14.42777\n",
      "Validation loss decreased (14.757953 --> 14.427773).  Saving model ...\n",
      "[ 692/1000] train_loss: 22.42536 valid_loss: 14.10659\n",
      "Validation loss decreased (14.427773 --> 14.106589).  Saving model ...\n",
      "[ 693/1000] train_loss: 23.03297 valid_loss: 13.78373\n",
      "Validation loss decreased (14.106589 --> 13.783729).  Saving model ...\n",
      "[ 694/1000] train_loss: 22.74637 valid_loss: 13.47051\n",
      "Validation loss decreased (13.783729 --> 13.470510).  Saving model ...\n",
      "[ 695/1000] train_loss: 21.52735 valid_loss: 13.16365\n",
      "Validation loss decreased (13.470510 --> 13.163648).  Saving model ...\n",
      "[ 696/1000] train_loss: 21.49171 valid_loss: 12.86234\n",
      "Validation loss decreased (13.163648 --> 12.862341).  Saving model ...\n",
      "[ 697/1000] train_loss: 20.97450 valid_loss: 12.57275\n",
      "Validation loss decreased (12.862341 --> 12.572748).  Saving model ...\n",
      "[ 698/1000] train_loss: 21.63940 valid_loss: 12.30113\n",
      "Validation loss decreased (12.572748 --> 12.301133).  Saving model ...\n",
      "[ 699/1000] train_loss: 20.93192 valid_loss: 12.03952\n",
      "Validation loss decreased (12.301133 --> 12.039521).  Saving model ...\n",
      "[ 700/1000] train_loss: 19.93679 valid_loss: 11.76331\n",
      "Validation loss decreased (12.039521 --> 11.763309).  Saving model ...\n",
      "[ 701/1000] train_loss: 20.82540 valid_loss: 11.47062\n",
      "Validation loss decreased (11.763309 --> 11.470618).  Saving model ...\n",
      "[ 702/1000] train_loss: 20.03650 valid_loss: 11.18433\n",
      "Validation loss decreased (11.470618 --> 11.184325).  Saving model ...\n",
      "[ 703/1000] train_loss: 21.19344 valid_loss: 10.91457\n",
      "Validation loss decreased (11.184325 --> 10.914565).  Saving model ...\n",
      "[ 704/1000] train_loss: 19.50245 valid_loss: 10.66161\n",
      "Validation loss decreased (10.914565 --> 10.661608).  Saving model ...\n",
      "[ 705/1000] train_loss: 19.22042 valid_loss: 10.40716\n",
      "Validation loss decreased (10.661608 --> 10.407159).  Saving model ...\n",
      "[ 706/1000] train_loss: 19.73830 valid_loss: 10.15922\n",
      "Validation loss decreased (10.407159 --> 10.159220).  Saving model ...\n",
      "[ 707/1000] train_loss: 19.12002 valid_loss: 9.92614\n",
      "Validation loss decreased (10.159220 --> 9.926140).  Saving model ...\n",
      "[ 708/1000] train_loss: 19.33031 valid_loss: 9.69887\n",
      "Validation loss decreased (9.926140 --> 9.698874).  Saving model ...\n",
      "[ 709/1000] train_loss: 18.82160 valid_loss: 9.46952\n",
      "Validation loss decreased (9.698874 --> 9.469516).  Saving model ...\n",
      "[ 710/1000] train_loss: 19.00369 valid_loss: 9.24105\n",
      "Validation loss decreased (9.469516 --> 9.241048).  Saving model ...\n",
      "[ 711/1000] train_loss: 18.32729 valid_loss: 9.01990\n",
      "Validation loss decreased (9.241048 --> 9.019898).  Saving model ...\n",
      "[ 712/1000] train_loss: 18.56025 valid_loss: 8.81371\n",
      "Validation loss decreased (9.019898 --> 8.813713).  Saving model ...\n",
      "[ 713/1000] train_loss: 18.39902 valid_loss: 8.61050\n",
      "Validation loss decreased (8.813713 --> 8.610501).  Saving model ...\n",
      "[ 714/1000] train_loss: 18.46719 valid_loss: 8.40829\n",
      "Validation loss decreased (8.610501 --> 8.408289).  Saving model ...\n",
      "[ 715/1000] train_loss: 17.84808 valid_loss: 8.20884\n",
      "Validation loss decreased (8.408289 --> 8.208840).  Saving model ...\n",
      "[ 716/1000] train_loss: 17.78544 valid_loss: 8.01740\n",
      "Validation loss decreased (8.208840 --> 8.017397).  Saving model ...\n",
      "[ 717/1000] train_loss: 17.25986 valid_loss: 7.83756\n",
      "Validation loss decreased (8.017397 --> 7.837558).  Saving model ...\n",
      "[ 718/1000] train_loss: 17.68788 valid_loss: 7.65888\n",
      "Validation loss decreased (7.837558 --> 7.658883).  Saving model ...\n",
      "[ 719/1000] train_loss: 17.09660 valid_loss: 7.48770\n",
      "Validation loss decreased (7.658883 --> 7.487695).  Saving model ...\n",
      "[ 720/1000] train_loss: 17.71739 valid_loss: 7.32245\n",
      "Validation loss decreased (7.487695 --> 7.322448).  Saving model ...\n",
      "[ 721/1000] train_loss: 16.77074 valid_loss: 7.16487\n",
      "Validation loss decreased (7.322448 --> 7.164874).  Saving model ...\n",
      "[ 722/1000] train_loss: 16.94199 valid_loss: 7.01545\n",
      "Validation loss decreased (7.164874 --> 7.015445).  Saving model ...\n",
      "[ 723/1000] train_loss: 16.30581 valid_loss: 6.86703\n",
      "Validation loss decreased (7.015445 --> 6.867029).  Saving model ...\n",
      "[ 724/1000] train_loss: 17.13458 valid_loss: 6.72444\n",
      "Validation loss decreased (6.867029 --> 6.724440).  Saving model ...\n",
      "[ 725/1000] train_loss: 17.00635 valid_loss: 6.58853\n",
      "Validation loss decreased (6.724440 --> 6.588531).  Saving model ...\n",
      "[ 726/1000] train_loss: 16.49154 valid_loss: 6.45769\n",
      "Validation loss decreased (6.588531 --> 6.457694).  Saving model ...\n",
      "[ 727/1000] train_loss: 16.06741 valid_loss: 6.32417\n",
      "Validation loss decreased (6.457694 --> 6.324173).  Saving model ...\n",
      "[ 728/1000] train_loss: 16.25040 valid_loss: 6.19604\n",
      "Validation loss decreased (6.324173 --> 6.196044).  Saving model ...\n",
      "[ 729/1000] train_loss: 16.30369 valid_loss: 6.07793\n",
      "Validation loss decreased (6.196044 --> 6.077926).  Saving model ...\n",
      "[ 730/1000] train_loss: 16.36393 valid_loss: 5.97123\n",
      "Validation loss decreased (6.077926 --> 5.971227).  Saving model ...\n",
      "[ 731/1000] train_loss: 16.30918 valid_loss: 5.86088\n",
      "Validation loss decreased (5.971227 --> 5.860884).  Saving model ...\n",
      "[ 732/1000] train_loss: 16.31320 valid_loss: 5.75261\n",
      "Validation loss decreased (5.860884 --> 5.752607).  Saving model ...\n",
      "[ 733/1000] train_loss: 16.47890 valid_loss: 5.65206\n",
      "Validation loss decreased (5.752607 --> 5.652064).  Saving model ...\n",
      "[ 734/1000] train_loss: 15.86465 valid_loss: 5.56217\n",
      "Validation loss decreased (5.652064 --> 5.562166).  Saving model ...\n",
      "[ 735/1000] train_loss: 16.30139 valid_loss: 5.47117\n",
      "Validation loss decreased (5.562166 --> 5.471167).  Saving model ...\n",
      "[ 736/1000] train_loss: 15.93629 valid_loss: 5.37626\n",
      "Validation loss decreased (5.471167 --> 5.376259).  Saving model ...\n",
      "[ 737/1000] train_loss: 15.57156 valid_loss: 5.28615\n",
      "Validation loss decreased (5.376259 --> 5.286150).  Saving model ...\n",
      "[ 738/1000] train_loss: 15.35976 valid_loss: 5.20440\n",
      "Validation loss decreased (5.286150 --> 5.204400).  Saving model ...\n",
      "[ 739/1000] train_loss: 14.73851 valid_loss: 5.12680\n",
      "Validation loss decreased (5.204400 --> 5.126803).  Saving model ...\n",
      "[ 740/1000] train_loss: 15.16475 valid_loss: 5.05363\n",
      "Validation loss decreased (5.126803 --> 5.053634).  Saving model ...\n",
      "[ 741/1000] train_loss: 15.01780 valid_loss: 4.98304\n",
      "Validation loss decreased (5.053634 --> 4.983041).  Saving model ...\n",
      "[ 742/1000] train_loss: 15.40018 valid_loss: 4.90865\n",
      "Validation loss decreased (4.983041 --> 4.908647).  Saving model ...\n",
      "[ 743/1000] train_loss: 15.36986 valid_loss: 4.83578\n",
      "Validation loss decreased (4.908647 --> 4.835778).  Saving model ...\n",
      "[ 744/1000] train_loss: 15.32684 valid_loss: 4.75224\n",
      "Validation loss decreased (4.835778 --> 4.752237).  Saving model ...\n",
      "[ 745/1000] train_loss: 14.85199 valid_loss: 4.66619\n",
      "Validation loss decreased (4.752237 --> 4.666188).  Saving model ...\n",
      "[ 746/1000] train_loss: 15.00412 valid_loss: 4.58738\n",
      "Validation loss decreased (4.666188 --> 4.587379).  Saving model ...\n",
      "[ 747/1000] train_loss: 14.94786 valid_loss: 4.51227\n",
      "Validation loss decreased (4.587379 --> 4.512269).  Saving model ...\n",
      "[ 748/1000] train_loss: 14.58991 valid_loss: 4.43484\n",
      "Validation loss decreased (4.512269 --> 4.434839).  Saving model ...\n",
      "[ 749/1000] train_loss: 14.76843 valid_loss: 4.37045\n",
      "Validation loss decreased (4.434839 --> 4.370446).  Saving model ...\n",
      "[ 750/1000] train_loss: 14.39410 valid_loss: 4.30648\n",
      "Validation loss decreased (4.370446 --> 4.306483).  Saving model ...\n",
      "[ 751/1000] train_loss: 14.27193 valid_loss: 4.24292\n",
      "Validation loss decreased (4.306483 --> 4.242915).  Saving model ...\n",
      "[ 752/1000] train_loss: 14.26006 valid_loss: 4.18898\n",
      "Validation loss decreased (4.242915 --> 4.188978).  Saving model ...\n",
      "[ 753/1000] train_loss: 14.66114 valid_loss: 4.13044\n",
      "Validation loss decreased (4.188978 --> 4.130437).  Saving model ...\n",
      "[ 754/1000] train_loss: 14.21143 valid_loss: 4.07024\n",
      "Validation loss decreased (4.130437 --> 4.070242).  Saving model ...\n",
      "[ 755/1000] train_loss: 14.56956 valid_loss: 4.02076\n",
      "Validation loss decreased (4.070242 --> 4.020765).  Saving model ...\n",
      "[ 756/1000] train_loss: 14.26234 valid_loss: 3.97491\n",
      "Validation loss decreased (4.020765 --> 3.974906).  Saving model ...\n",
      "[ 757/1000] train_loss: 14.19968 valid_loss: 3.93807\n",
      "Validation loss decreased (3.974906 --> 3.938070).  Saving model ...\n",
      "[ 758/1000] train_loss: 14.66980 valid_loss: 3.88545\n",
      "Validation loss decreased (3.938070 --> 3.885452).  Saving model ...\n",
      "[ 759/1000] train_loss: 13.81203 valid_loss: 3.83907\n",
      "Validation loss decreased (3.885452 --> 3.839067).  Saving model ...\n",
      "[ 760/1000] train_loss: 14.31698 valid_loss: 3.80439\n",
      "Validation loss decreased (3.839067 --> 3.804385).  Saving model ...\n",
      "[ 761/1000] train_loss: 14.40139 valid_loss: 3.76265\n",
      "Validation loss decreased (3.804385 --> 3.762651).  Saving model ...\n",
      "[ 762/1000] train_loss: 13.16644 valid_loss: 3.69549\n",
      "Validation loss decreased (3.762651 --> 3.695493).  Saving model ...\n",
      "[ 763/1000] train_loss: 14.52007 valid_loss: 3.62386\n",
      "Validation loss decreased (3.695493 --> 3.623863).  Saving model ...\n",
      "[ 764/1000] train_loss: 13.74304 valid_loss: 3.56438\n",
      "Validation loss decreased (3.623863 --> 3.564381).  Saving model ...\n",
      "[ 765/1000] train_loss: 13.66846 valid_loss: 3.51400\n",
      "Validation loss decreased (3.564381 --> 3.513997).  Saving model ...\n",
      "[ 766/1000] train_loss: 14.15438 valid_loss: 3.46365\n",
      "Validation loss decreased (3.513997 --> 3.463647).  Saving model ...\n",
      "[ 767/1000] train_loss: 13.87751 valid_loss: 3.41977\n",
      "Validation loss decreased (3.463647 --> 3.419773).  Saving model ...\n",
      "[ 768/1000] train_loss: 13.18906 valid_loss: 3.40207\n",
      "Validation loss decreased (3.419773 --> 3.402067).  Saving model ...\n",
      "[ 769/1000] train_loss: 13.52649 valid_loss: 3.37122\n",
      "Validation loss decreased (3.402067 --> 3.371219).  Saving model ...\n",
      "[ 770/1000] train_loss: 13.07626 valid_loss: 3.33459\n",
      "Validation loss decreased (3.371219 --> 3.334588).  Saving model ...\n",
      "[ 771/1000] train_loss: 13.15625 valid_loss: 3.30095\n",
      "Validation loss decreased (3.334588 --> 3.300949).  Saving model ...\n",
      "[ 772/1000] train_loss: 13.31983 valid_loss: 3.27534\n",
      "Validation loss decreased (3.300949 --> 3.275339).  Saving model ...\n",
      "[ 773/1000] train_loss: 13.45234 valid_loss: 3.25511\n",
      "Validation loss decreased (3.275339 --> 3.255114).  Saving model ...\n",
      "[ 774/1000] train_loss: 13.37820 valid_loss: 3.23008\n",
      "Validation loss decreased (3.255114 --> 3.230083).  Saving model ...\n",
      "[ 775/1000] train_loss: 13.25349 valid_loss: 3.19211\n",
      "Validation loss decreased (3.230083 --> 3.192110).  Saving model ...\n",
      "[ 776/1000] train_loss: 13.13409 valid_loss: 3.15763\n",
      "Validation loss decreased (3.192110 --> 3.157635).  Saving model ...\n",
      "[ 777/1000] train_loss: 13.23235 valid_loss: 3.12700\n",
      "Validation loss decreased (3.157635 --> 3.127004).  Saving model ...\n",
      "[ 778/1000] train_loss: 13.75883 valid_loss: 3.09650\n",
      "Validation loss decreased (3.127004 --> 3.096497).  Saving model ...\n",
      "[ 779/1000] train_loss: 12.39690 valid_loss: 3.05405\n",
      "Validation loss decreased (3.096497 --> 3.054053).  Saving model ...\n",
      "[ 780/1000] train_loss: 12.40857 valid_loss: 3.01443\n",
      "Validation loss decreased (3.054053 --> 3.014427).  Saving model ...\n",
      "[ 781/1000] train_loss: 13.80157 valid_loss: 2.97652\n",
      "Validation loss decreased (3.014427 --> 2.976516).  Saving model ...\n",
      "[ 782/1000] train_loss: 13.31202 valid_loss: 2.94309\n",
      "Validation loss decreased (2.976516 --> 2.943094).  Saving model ...\n",
      "[ 783/1000] train_loss: 12.69445 valid_loss: 2.90949\n",
      "Validation loss decreased (2.943094 --> 2.909489).  Saving model ...\n",
      "[ 784/1000] train_loss: 12.95670 valid_loss: 2.87909\n",
      "Validation loss decreased (2.909489 --> 2.879089).  Saving model ...\n",
      "[ 785/1000] train_loss: 12.52975 valid_loss: 2.85509\n",
      "Validation loss decreased (2.879089 --> 2.855093).  Saving model ...\n",
      "[ 786/1000] train_loss: 12.54364 valid_loss: 2.83576\n",
      "Validation loss decreased (2.855093 --> 2.835759).  Saving model ...\n",
      "[ 787/1000] train_loss: 12.37938 valid_loss: 2.81767\n",
      "Validation loss decreased (2.835759 --> 2.817670).  Saving model ...\n",
      "[ 788/1000] train_loss: 13.19208 valid_loss: 2.80858\n",
      "Validation loss decreased (2.817670 --> 2.808580).  Saving model ...\n",
      "[ 789/1000] train_loss: 12.92570 valid_loss: 2.79917\n",
      "Validation loss decreased (2.808580 --> 2.799169).  Saving model ...\n",
      "[ 790/1000] train_loss: 12.37504 valid_loss: 2.77865\n",
      "Validation loss decreased (2.799169 --> 2.778649).  Saving model ...\n",
      "[ 791/1000] train_loss: 13.43456 valid_loss: 2.75256\n",
      "Validation loss decreased (2.778649 --> 2.752556).  Saving model ...\n",
      "[ 792/1000] train_loss: 12.92540 valid_loss: 2.72199\n",
      "Validation loss decreased (2.752556 --> 2.721989).  Saving model ...\n",
      "[ 793/1000] train_loss: 13.01274 valid_loss: 2.68436\n",
      "Validation loss decreased (2.721989 --> 2.684365).  Saving model ...\n",
      "[ 794/1000] train_loss: 12.75686 valid_loss: 2.65998\n",
      "Validation loss decreased (2.684365 --> 2.659981).  Saving model ...\n",
      "[ 795/1000] train_loss: 12.16158 valid_loss: 2.64067\n",
      "Validation loss decreased (2.659981 --> 2.640675).  Saving model ...\n",
      "[ 796/1000] train_loss: 13.17707 valid_loss: 2.61891\n",
      "Validation loss decreased (2.640675 --> 2.618909).  Saving model ...\n",
      "[ 797/1000] train_loss: 12.35028 valid_loss: 2.59729\n",
      "Validation loss decreased (2.618909 --> 2.597291).  Saving model ...\n",
      "[ 798/1000] train_loss: 13.14752 valid_loss: 2.57561\n",
      "Validation loss decreased (2.597291 --> 2.575608).  Saving model ...\n",
      "[ 799/1000] train_loss: 12.62016 valid_loss: 2.55665\n",
      "Validation loss decreased (2.575608 --> 2.556646).  Saving model ...\n",
      "[ 800/1000] train_loss: 13.27661 valid_loss: 2.54067\n",
      "Validation loss decreased (2.556646 --> 2.540673).  Saving model ...\n",
      "[ 801/1000] train_loss: 12.70589 valid_loss: 2.52286\n",
      "Validation loss decreased (2.540673 --> 2.522858).  Saving model ...\n",
      "[ 802/1000] train_loss: 12.42003 valid_loss: 2.50819\n",
      "Validation loss decreased (2.522858 --> 2.508189).  Saving model ...\n",
      "[ 803/1000] train_loss: 12.93187 valid_loss: 2.49313\n",
      "Validation loss decreased (2.508189 --> 2.493130).  Saving model ...\n",
      "[ 804/1000] train_loss: 12.58277 valid_loss: 2.47430\n",
      "Validation loss decreased (2.493130 --> 2.474301).  Saving model ...\n",
      "[ 805/1000] train_loss: 12.70573 valid_loss: 2.45082\n",
      "Validation loss decreased (2.474301 --> 2.450818).  Saving model ...\n",
      "[ 806/1000] train_loss: 12.55917 valid_loss: 2.41663\n",
      "Validation loss decreased (2.450818 --> 2.416633).  Saving model ...\n",
      "[ 807/1000] train_loss: 12.80099 valid_loss: 2.39168\n",
      "Validation loss decreased (2.416633 --> 2.391675).  Saving model ...\n",
      "[ 808/1000] train_loss: 12.32881 valid_loss: 2.37665\n",
      "Validation loss decreased (2.391675 --> 2.376647).  Saving model ...\n",
      "[ 809/1000] train_loss: 12.71751 valid_loss: 2.36163\n",
      "Validation loss decreased (2.376647 --> 2.361628).  Saving model ...\n",
      "[ 810/1000] train_loss: 12.33552 valid_loss: 2.34331\n",
      "Validation loss decreased (2.361628 --> 2.343313).  Saving model ...\n",
      "[ 811/1000] train_loss: 11.76468 valid_loss: 2.32643\n",
      "Validation loss decreased (2.343313 --> 2.326428).  Saving model ...\n",
      "[ 812/1000] train_loss: 12.83113 valid_loss: 2.31367\n",
      "Validation loss decreased (2.326428 --> 2.313670).  Saving model ...\n",
      "[ 813/1000] train_loss: 12.27605 valid_loss: 2.30685\n",
      "Validation loss decreased (2.313670 --> 2.306849).  Saving model ...\n",
      "[ 814/1000] train_loss: 12.79589 valid_loss: 2.30642\n",
      "Validation loss decreased (2.306849 --> 2.306423).  Saving model ...\n",
      "[ 815/1000] train_loss: 12.20136 valid_loss: 2.29910\n",
      "Validation loss decreased (2.306423 --> 2.299095).  Saving model ...\n",
      "[ 816/1000] train_loss: 12.24527 valid_loss: 2.28626\n",
      "Validation loss decreased (2.299095 --> 2.286262).  Saving model ...\n",
      "[ 817/1000] train_loss: 12.79679 valid_loss: 2.27117\n",
      "Validation loss decreased (2.286262 --> 2.271167).  Saving model ...\n",
      "[ 818/1000] train_loss: 12.18917 valid_loss: 2.24887\n",
      "Validation loss decreased (2.271167 --> 2.248872).  Saving model ...\n",
      "[ 819/1000] train_loss: 12.41484 valid_loss: 2.23886\n",
      "Validation loss decreased (2.248872 --> 2.238862).  Saving model ...\n",
      "[ 820/1000] train_loss: 12.60100 valid_loss: 2.23078\n",
      "Validation loss decreased (2.238862 --> 2.230778).  Saving model ...\n",
      "[ 821/1000] train_loss: 12.47240 valid_loss: 2.21019\n",
      "Validation loss decreased (2.230778 --> 2.210189).  Saving model ...\n",
      "[ 822/1000] train_loss: 11.76462 valid_loss: 2.18537\n",
      "Validation loss decreased (2.210189 --> 2.185372).  Saving model ...\n",
      "[ 823/1000] train_loss: 12.14228 valid_loss: 2.17324\n",
      "Validation loss decreased (2.185372 --> 2.173239).  Saving model ...\n",
      "[ 824/1000] train_loss: 12.03057 valid_loss: 2.16539\n",
      "Validation loss decreased (2.173239 --> 2.165387).  Saving model ...\n",
      "[ 825/1000] train_loss: 11.59010 valid_loss: 2.15413\n",
      "Validation loss decreased (2.165387 --> 2.154134).  Saving model ...\n",
      "[ 826/1000] train_loss: 12.22011 valid_loss: 2.14402\n",
      "Validation loss decreased (2.154134 --> 2.144018).  Saving model ...\n",
      "[ 827/1000] train_loss: 12.24401 valid_loss: 2.13325\n",
      "Validation loss decreased (2.144018 --> 2.133249).  Saving model ...\n",
      "[ 828/1000] train_loss: 11.78570 valid_loss: 2.12112\n",
      "Validation loss decreased (2.133249 --> 2.121116).  Saving model ...\n",
      "[ 829/1000] train_loss: 11.68213 valid_loss: 2.11025\n",
      "Validation loss decreased (2.121116 --> 2.110251).  Saving model ...\n",
      "[ 830/1000] train_loss: 11.59052 valid_loss: 2.09947\n",
      "Validation loss decreased (2.110251 --> 2.099467).  Saving model ...\n",
      "[ 831/1000] train_loss: 12.18916 valid_loss: 2.08723\n",
      "Validation loss decreased (2.099467 --> 2.087232).  Saving model ...\n",
      "[ 832/1000] train_loss: 12.34296 valid_loss: 2.07595\n",
      "Validation loss decreased (2.087232 --> 2.075949).  Saving model ...\n",
      "[ 833/1000] train_loss: 11.77173 valid_loss: 2.07039\n",
      "Validation loss decreased (2.075949 --> 2.070389).  Saving model ...\n",
      "[ 834/1000] train_loss: 12.55826 valid_loss: 2.05937\n",
      "Validation loss decreased (2.070389 --> 2.059366).  Saving model ...\n",
      "[ 835/1000] train_loss: 11.34461 valid_loss: 2.04836\n",
      "Validation loss decreased (2.059366 --> 2.048358).  Saving model ...\n",
      "[ 836/1000] train_loss: 11.83986 valid_loss: 2.04387\n",
      "Validation loss decreased (2.048358 --> 2.043868).  Saving model ...\n",
      "[ 837/1000] train_loss: 11.78841 valid_loss: 2.04110\n",
      "Validation loss decreased (2.043868 --> 2.041100).  Saving model ...\n",
      "[ 838/1000] train_loss: 12.35553 valid_loss: 2.03848\n",
      "Validation loss decreased (2.041100 --> 2.038480).  Saving model ...\n",
      "[ 839/1000] train_loss: 11.86156 valid_loss: 2.03618\n",
      "Validation loss decreased (2.038480 --> 2.036182).  Saving model ...\n",
      "[ 840/1000] train_loss: 11.43638 valid_loss: 2.03435\n",
      "Validation loss decreased (2.036182 --> 2.034352).  Saving model ...\n",
      "[ 841/1000] train_loss: 12.31210 valid_loss: 2.02435\n",
      "Validation loss decreased (2.034352 --> 2.024351).  Saving model ...\n",
      "[ 842/1000] train_loss: 12.47619 valid_loss: 2.02274\n",
      "Validation loss decreased (2.024351 --> 2.022735).  Saving model ...\n",
      "[ 843/1000] train_loss: 11.66300 valid_loss: 2.01352\n",
      "Validation loss decreased (2.022735 --> 2.013520).  Saving model ...\n",
      "[ 844/1000] train_loss: 11.06488 valid_loss: 2.00391\n",
      "Validation loss decreased (2.013520 --> 2.003908).  Saving model ...\n",
      "[ 845/1000] train_loss: 12.03464 valid_loss: 1.98827\n",
      "Validation loss decreased (2.003908 --> 1.988265).  Saving model ...\n",
      "[ 846/1000] train_loss: 11.16964 valid_loss: 1.98615\n",
      "Validation loss decreased (1.988265 --> 1.986146).  Saving model ...\n",
      "[ 847/1000] train_loss: 11.02410 valid_loss: 1.99061\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 848/1000] train_loss: 11.93910 valid_loss: 1.98585\n",
      "Validation loss decreased (1.986146 --> 1.985851).  Saving model ...\n",
      "[ 849/1000] train_loss: 11.49372 valid_loss: 1.98021\n",
      "Validation loss decreased (1.985851 --> 1.980211).  Saving model ...\n",
      "[ 850/1000] train_loss: 11.81629 valid_loss: 1.98412\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 851/1000] train_loss: 12.02562 valid_loss: 1.98420\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 852/1000] train_loss: 11.94610 valid_loss: 1.98102\n",
      "EarlyStopping counter: 3 out of 20\n",
      "[ 853/1000] train_loss: 11.64886 valid_loss: 1.97227\n",
      "Validation loss decreased (1.980211 --> 1.972270).  Saving model ...\n",
      "[ 854/1000] train_loss: 11.89069 valid_loss: 1.96582\n",
      "Validation loss decreased (1.972270 --> 1.965819).  Saving model ...\n",
      "[ 855/1000] train_loss: 11.23685 valid_loss: 1.95945\n",
      "Validation loss decreased (1.965819 --> 1.959446).  Saving model ...\n",
      "[ 856/1000] train_loss: 11.67823 valid_loss: 1.95236\n",
      "Validation loss decreased (1.959446 --> 1.952363).  Saving model ...\n",
      "[ 857/1000] train_loss: 12.04000 valid_loss: 1.93876\n",
      "Validation loss decreased (1.952363 --> 1.938761).  Saving model ...\n",
      "[ 858/1000] train_loss: 11.33974 valid_loss: 1.92788\n",
      "Validation loss decreased (1.938761 --> 1.927882).  Saving model ...\n",
      "[ 859/1000] train_loss: 10.92058 valid_loss: 1.91781\n",
      "Validation loss decreased (1.927882 --> 1.917808).  Saving model ...\n",
      "[ 860/1000] train_loss: 12.03980 valid_loss: 1.90934\n",
      "Validation loss decreased (1.917808 --> 1.909341).  Saving model ...\n",
      "[ 861/1000] train_loss: 11.01418 valid_loss: 1.91302\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 862/1000] train_loss: 11.54378 valid_loss: 1.91630\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 863/1000] train_loss: 11.35405 valid_loss: 1.90922\n",
      "Validation loss decreased (1.909341 --> 1.909223).  Saving model ...\n",
      "[ 864/1000] train_loss: 11.77162 valid_loss: 1.89212\n",
      "Validation loss decreased (1.909223 --> 1.892123).  Saving model ...\n",
      "[ 865/1000] train_loss: 11.45494 valid_loss: 1.88059\n",
      "Validation loss decreased (1.892123 --> 1.880586).  Saving model ...\n",
      "[ 866/1000] train_loss: 11.01945 valid_loss: 1.87479\n",
      "Validation loss decreased (1.880586 --> 1.874785).  Saving model ...\n",
      "[ 867/1000] train_loss: 11.60345 valid_loss: 1.87158\n",
      "Validation loss decreased (1.874785 --> 1.871581).  Saving model ...\n",
      "[ 868/1000] train_loss: 11.42339 valid_loss: 1.87133\n",
      "Validation loss decreased (1.871581 --> 1.871334).  Saving model ...\n",
      "[ 869/1000] train_loss: 10.94190 valid_loss: 1.86705\n",
      "Validation loss decreased (1.871334 --> 1.867047).  Saving model ...\n",
      "[ 870/1000] train_loss: 11.47502 valid_loss: 1.85044\n",
      "Validation loss decreased (1.867047 --> 1.850439).  Saving model ...\n",
      "[ 871/1000] train_loss: 11.16142 valid_loss: 1.84285\n",
      "Validation loss decreased (1.850439 --> 1.842853).  Saving model ...\n",
      "[ 872/1000] train_loss: 11.67023 valid_loss: 1.84193\n",
      "Validation loss decreased (1.842853 --> 1.841926).  Saving model ...\n",
      "[ 873/1000] train_loss: 11.26635 valid_loss: 1.83762\n",
      "Validation loss decreased (1.841926 --> 1.837617).  Saving model ...\n",
      "[ 874/1000] train_loss: 12.11457 valid_loss: 1.83476\n",
      "Validation loss decreased (1.837617 --> 1.834759).  Saving model ...\n",
      "[ 875/1000] train_loss: 10.73273 valid_loss: 1.84440\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 876/1000] train_loss: 11.07598 valid_loss: 1.85950\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 877/1000] train_loss: 11.15724 valid_loss: 1.86378\n",
      "EarlyStopping counter: 3 out of 20\n",
      "[ 878/1000] train_loss: 10.96053 valid_loss: 1.85199\n",
      "EarlyStopping counter: 4 out of 20\n",
      "[ 879/1000] train_loss: 11.11595 valid_loss: 1.83409\n",
      "Validation loss decreased (1.834759 --> 1.834085).  Saving model ...\n",
      "[ 880/1000] train_loss: 10.83900 valid_loss: 1.82770\n",
      "Validation loss decreased (1.834085 --> 1.827704).  Saving model ...\n",
      "[ 881/1000] train_loss: 11.91919 valid_loss: 1.82944\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 882/1000] train_loss: 11.38829 valid_loss: 1.82407\n",
      "Validation loss decreased (1.827704 --> 1.824067).  Saving model ...\n",
      "[ 883/1000] train_loss: 11.09613 valid_loss: 1.82437\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 884/1000] train_loss: 11.48181 valid_loss: 1.81496\n",
      "Validation loss decreased (1.824067 --> 1.814955).  Saving model ...\n",
      "[ 885/1000] train_loss: 11.24012 valid_loss: 1.80698\n",
      "Validation loss decreased (1.814955 --> 1.806976).  Saving model ...\n",
      "[ 886/1000] train_loss: 11.28964 valid_loss: 1.79975\n",
      "Validation loss decreased (1.806976 --> 1.799749).  Saving model ...\n",
      "[ 887/1000] train_loss: 10.86377 valid_loss: 1.79413\n",
      "Validation loss decreased (1.799749 --> 1.794128).  Saving model ...\n",
      "[ 888/1000] train_loss: 11.08990 valid_loss: 1.79125\n",
      "Validation loss decreased (1.794128 --> 1.791255).  Saving model ...\n",
      "[ 889/1000] train_loss: 10.79922 valid_loss: 1.78056\n",
      "Validation loss decreased (1.791255 --> 1.780558).  Saving model ...\n",
      "[ 890/1000] train_loss: 11.40373 valid_loss: 1.77083\n",
      "Validation loss decreased (1.780558 --> 1.770827).  Saving model ...\n",
      "[ 891/1000] train_loss: 10.92930 valid_loss: 1.76753\n",
      "Validation loss decreased (1.770827 --> 1.767532).  Saving model ...\n",
      "[ 892/1000] train_loss: 11.47744 valid_loss: 1.76016\n",
      "Validation loss decreased (1.767532 --> 1.760160).  Saving model ...\n",
      "[ 893/1000] train_loss: 11.73097 valid_loss: 1.76081\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 894/1000] train_loss: 10.98192 valid_loss: 1.75700\n",
      "Validation loss decreased (1.760160 --> 1.756997).  Saving model ...\n",
      "[ 895/1000] train_loss: 11.87083 valid_loss: 1.74894\n",
      "Validation loss decreased (1.756997 --> 1.748943).  Saving model ...\n",
      "[ 896/1000] train_loss: 10.84158 valid_loss: 1.74618\n",
      "Validation loss decreased (1.748943 --> 1.746184).  Saving model ...\n",
      "[ 897/1000] train_loss: 10.71580 valid_loss: 1.74642\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 898/1000] train_loss: 11.16919 valid_loss: 1.75207\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 899/1000] train_loss: 10.46044 valid_loss: 1.76124\n",
      "EarlyStopping counter: 3 out of 20\n",
      "[ 900/1000] train_loss: 11.05122 valid_loss: 1.76107\n",
      "EarlyStopping counter: 4 out of 20\n",
      "[ 901/1000] train_loss: 11.23094 valid_loss: 1.76716\n",
      "EarlyStopping counter: 5 out of 20\n",
      "[ 902/1000] train_loss: 11.19529 valid_loss: 1.75908\n",
      "EarlyStopping counter: 6 out of 20\n",
      "[ 903/1000] train_loss: 11.55850 valid_loss: 1.74965\n",
      "EarlyStopping counter: 7 out of 20\n",
      "[ 904/1000] train_loss: 10.89878 valid_loss: 1.74184\n",
      "Validation loss decreased (1.746184 --> 1.741836).  Saving model ...\n",
      "[ 905/1000] train_loss: 11.39393 valid_loss: 1.73594\n",
      "Validation loss decreased (1.741836 --> 1.735943).  Saving model ...\n",
      "[ 906/1000] train_loss: 10.80875 valid_loss: 1.72272\n",
      "Validation loss decreased (1.735943 --> 1.722717).  Saving model ...\n",
      "[ 907/1000] train_loss: 10.98759 valid_loss: 1.71067\n",
      "Validation loss decreased (1.722717 --> 1.710673).  Saving model ...\n",
      "[ 908/1000] train_loss: 10.48553 valid_loss: 1.70741\n",
      "Validation loss decreased (1.710673 --> 1.707408).  Saving model ...\n",
      "[ 909/1000] train_loss: 11.03882 valid_loss: 1.71019\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 910/1000] train_loss: 10.87392 valid_loss: 1.70933\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 911/1000] train_loss: 11.16439 valid_loss: 1.70639\n",
      "Validation loss decreased (1.707408 --> 1.706386).  Saving model ...\n",
      "[ 912/1000] train_loss: 11.03365 valid_loss: 1.70225\n",
      "Validation loss decreased (1.706386 --> 1.702253).  Saving model ...\n",
      "[ 913/1000] train_loss: 10.84848 valid_loss: 1.69683\n",
      "Validation loss decreased (1.702253 --> 1.696835).  Saving model ...\n",
      "[ 914/1000] train_loss: 11.36974 valid_loss: 1.70180\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 915/1000] train_loss: 11.10294 valid_loss: 1.70997\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 916/1000] train_loss: 10.42959 valid_loss: 1.71663\n",
      "EarlyStopping counter: 3 out of 20\n",
      "[ 917/1000] train_loss: 11.07524 valid_loss: 1.71940\n",
      "EarlyStopping counter: 4 out of 20\n",
      "[ 918/1000] train_loss: 10.84073 valid_loss: 1.71240\n",
      "EarlyStopping counter: 5 out of 20\n",
      "[ 919/1000] train_loss: 11.77146 valid_loss: 1.69646\n",
      "Validation loss decreased (1.696835 --> 1.696463).  Saving model ...\n",
      "[ 920/1000] train_loss: 10.83027 valid_loss: 1.68587\n",
      "Validation loss decreased (1.696463 --> 1.685868).  Saving model ...\n",
      "[ 921/1000] train_loss: 10.80220 valid_loss: 1.67698\n",
      "Validation loss decreased (1.685868 --> 1.676980).  Saving model ...\n",
      "[ 922/1000] train_loss: 10.81930 valid_loss: 1.67224\n",
      "Validation loss decreased (1.676980 --> 1.672239).  Saving model ...\n",
      "[ 923/1000] train_loss: 11.13029 valid_loss: 1.66572\n",
      "Validation loss decreased (1.672239 --> 1.665716).  Saving model ...\n",
      "[ 924/1000] train_loss: 10.73759 valid_loss: 1.66322\n",
      "Validation loss decreased (1.665716 --> 1.663218).  Saving model ...\n",
      "[ 925/1000] train_loss: 10.20952 valid_loss: 1.65460\n",
      "Validation loss decreased (1.663218 --> 1.654603).  Saving model ...\n",
      "[ 926/1000] train_loss: 11.34186 valid_loss: 1.65148\n",
      "Validation loss decreased (1.654603 --> 1.651477).  Saving model ...\n",
      "[ 927/1000] train_loss: 11.24355 valid_loss: 1.64348\n",
      "Validation loss decreased (1.651477 --> 1.643479).  Saving model ...\n",
      "[ 928/1000] train_loss: 10.98651 valid_loss: 1.63741\n",
      "Validation loss decreased (1.643479 --> 1.637412).  Saving model ...\n",
      "[ 929/1000] train_loss: 10.97528 valid_loss: 1.63895\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 930/1000] train_loss: 11.09783 valid_loss: 1.63951\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 931/1000] train_loss: 10.91594 valid_loss: 1.64180\n",
      "EarlyStopping counter: 3 out of 20\n",
      "[ 932/1000] train_loss: 10.90386 valid_loss: 1.65006\n",
      "EarlyStopping counter: 4 out of 20\n",
      "[ 933/1000] train_loss: 10.49740 valid_loss: 1.65072\n",
      "EarlyStopping counter: 5 out of 20\n",
      "[ 934/1000] train_loss: 10.76764 valid_loss: 1.65255\n",
      "EarlyStopping counter: 6 out of 20\n",
      "[ 935/1000] train_loss: 11.20102 valid_loss: 1.63929\n",
      "EarlyStopping counter: 7 out of 20\n",
      "[ 936/1000] train_loss: 10.75545 valid_loss: 1.63210\n",
      "Validation loss decreased (1.637412 --> 1.632100).  Saving model ...\n",
      "[ 937/1000] train_loss: 10.46135 valid_loss: 1.62193\n",
      "Validation loss decreased (1.632100 --> 1.621933).  Saving model ...\n",
      "[ 938/1000] train_loss: 10.45621 valid_loss: 1.61626\n",
      "Validation loss decreased (1.621933 --> 1.616261).  Saving model ...\n",
      "[ 939/1000] train_loss: 10.41343 valid_loss: 1.62954\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 940/1000] train_loss: 10.99277 valid_loss: 1.62326\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 941/1000] train_loss: 10.70434 valid_loss: 1.62006\n",
      "EarlyStopping counter: 3 out of 20\n",
      "[ 942/1000] train_loss: 11.23271 valid_loss: 1.61766\n",
      "EarlyStopping counter: 4 out of 20\n",
      "[ 943/1000] train_loss: 10.80357 valid_loss: 1.62515\n",
      "EarlyStopping counter: 5 out of 20\n",
      "[ 944/1000] train_loss: 11.01623 valid_loss: 1.64287\n",
      "EarlyStopping counter: 6 out of 20\n",
      "[ 945/1000] train_loss: 10.63581 valid_loss: 1.63716\n",
      "EarlyStopping counter: 7 out of 20\n",
      "[ 946/1000] train_loss: 11.11350 valid_loss: 1.63239\n",
      "EarlyStopping counter: 8 out of 20\n",
      "[ 947/1000] train_loss: 11.19640 valid_loss: 1.61762\n",
      "EarlyStopping counter: 9 out of 20\n",
      "[ 948/1000] train_loss: 10.83529 valid_loss: 1.60098\n",
      "Validation loss decreased (1.616261 --> 1.600981).  Saving model ...\n",
      "[ 949/1000] train_loss: 10.60437 valid_loss: 1.59905\n",
      "Validation loss decreased (1.600981 --> 1.599049).  Saving model ...\n",
      "[ 950/1000] train_loss: 10.51591 valid_loss: 1.59279\n",
      "Validation loss decreased (1.599049 --> 1.592787).  Saving model ...\n",
      "[ 951/1000] train_loss: 10.82631 valid_loss: 1.57875\n",
      "Validation loss decreased (1.592787 --> 1.578745).  Saving model ...\n",
      "[ 952/1000] train_loss: 11.00266 valid_loss: 1.57910\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 953/1000] train_loss: 10.46559 valid_loss: 1.58093\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 954/1000] train_loss: 11.04589 valid_loss: 1.59976\n",
      "EarlyStopping counter: 3 out of 20\n",
      "[ 955/1000] train_loss: 10.14995 valid_loss: 1.59511\n",
      "EarlyStopping counter: 4 out of 20\n",
      "[ 956/1000] train_loss: 10.40323 valid_loss: 1.57867\n",
      "Validation loss decreased (1.578745 --> 1.578669).  Saving model ...\n",
      "[ 957/1000] train_loss: 10.82858 valid_loss: 1.57726\n",
      "Validation loss decreased (1.578669 --> 1.577264).  Saving model ...\n",
      "[ 958/1000] train_loss: 10.10235 valid_loss: 1.57455\n",
      "Validation loss decreased (1.577264 --> 1.574551).  Saving model ...\n",
      "[ 959/1000] train_loss: 10.58710 valid_loss: 1.57547\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 960/1000] train_loss: 10.57327 valid_loss: 1.57028\n",
      "Validation loss decreased (1.574551 --> 1.570283).  Saving model ...\n",
      "[ 961/1000] train_loss: 10.27904 valid_loss: 1.56978\n",
      "Validation loss decreased (1.570283 --> 1.569783).  Saving model ...\n",
      "[ 962/1000] train_loss: 10.36585 valid_loss: 1.57209\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 963/1000] train_loss: 10.63902 valid_loss: 1.57563\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 964/1000] train_loss: 11.09154 valid_loss: 1.57718\n",
      "EarlyStopping counter: 3 out of 20\n",
      "[ 965/1000] train_loss: 10.12446 valid_loss: 1.57705\n",
      "EarlyStopping counter: 4 out of 20\n",
      "[ 966/1000] train_loss: 10.45988 valid_loss: 1.57087\n",
      "EarlyStopping counter: 5 out of 20\n",
      "[ 967/1000] train_loss: 10.67912 valid_loss: 1.56239\n",
      "Validation loss decreased (1.569783 --> 1.562392).  Saving model ...\n",
      "[ 968/1000] train_loss: 10.51831 valid_loss: 1.55617\n",
      "Validation loss decreased (1.562392 --> 1.556173).  Saving model ...\n",
      "[ 969/1000] train_loss: 11.46425 valid_loss: 1.55492\n",
      "Validation loss decreased (1.556173 --> 1.554924).  Saving model ...\n",
      "[ 970/1000] train_loss: 10.42272 valid_loss: 1.54899\n",
      "Validation loss decreased (1.554924 --> 1.548986).  Saving model ...\n",
      "[ 971/1000] train_loss: 10.26248 valid_loss: 1.53601\n",
      "Validation loss decreased (1.548986 --> 1.536006).  Saving model ...\n",
      "[ 972/1000] train_loss: 9.40211 valid_loss: 1.52984\n",
      "Validation loss decreased (1.536006 --> 1.529844).  Saving model ...\n",
      "[ 973/1000] train_loss: 11.23357 valid_loss: 1.53476\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 974/1000] train_loss: 10.99737 valid_loss: 1.53991\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 975/1000] train_loss: 10.37390 valid_loss: 1.53393\n",
      "EarlyStopping counter: 3 out of 20\n",
      "[ 976/1000] train_loss: 10.19925 valid_loss: 1.53478\n",
      "EarlyStopping counter: 4 out of 20\n",
      "[ 977/1000] train_loss: 10.14452 valid_loss: 1.54783\n",
      "EarlyStopping counter: 5 out of 20\n",
      "[ 978/1000] train_loss: 10.83810 valid_loss: 1.54691\n",
      "EarlyStopping counter: 6 out of 20\n",
      "[ 979/1000] train_loss: 10.39181 valid_loss: 1.54604\n",
      "EarlyStopping counter: 7 out of 20\n",
      "[ 980/1000] train_loss: 9.99826 valid_loss: 1.54419\n",
      "EarlyStopping counter: 8 out of 20\n",
      "[ 981/1000] train_loss: 9.92006 valid_loss: 1.54234\n",
      "EarlyStopping counter: 9 out of 20\n",
      "[ 982/1000] train_loss: 10.62580 valid_loss: 1.53806\n",
      "EarlyStopping counter: 10 out of 20\n",
      "[ 983/1000] train_loss: 10.51370 valid_loss: 1.52823\n",
      "Validation loss decreased (1.529844 --> 1.528234).  Saving model ...\n",
      "[ 984/1000] train_loss: 10.44661 valid_loss: 1.52577\n",
      "Validation loss decreased (1.528234 --> 1.525765).  Saving model ...\n",
      "[ 985/1000] train_loss: 10.53605 valid_loss: 1.52080\n",
      "Validation loss decreased (1.525765 --> 1.520798).  Saving model ...\n",
      "[ 986/1000] train_loss: 10.59689 valid_loss: 1.51502\n",
      "Validation loss decreased (1.520798 --> 1.515015).  Saving model ...\n",
      "[ 987/1000] train_loss: 10.61427 valid_loss: 1.51845\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 988/1000] train_loss: 10.29615 valid_loss: 1.52221\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 989/1000] train_loss: 10.95677 valid_loss: 1.52412\n",
      "EarlyStopping counter: 3 out of 20\n",
      "[ 990/1000] train_loss: 10.58625 valid_loss: 1.51735\n",
      "EarlyStopping counter: 4 out of 20\n",
      "[ 991/1000] train_loss: 9.75329 valid_loss: 1.51148\n",
      "Validation loss decreased (1.515015 --> 1.511480).  Saving model ...\n",
      "[ 992/1000] train_loss: 10.64435 valid_loss: 1.50233\n",
      "Validation loss decreased (1.511480 --> 1.502332).  Saving model ...\n",
      "[ 993/1000] train_loss: 10.38254 valid_loss: 1.50016\n",
      "Validation loss decreased (1.502332 --> 1.500165).  Saving model ...\n",
      "[ 994/1000] train_loss: 10.57789 valid_loss: 1.49921\n",
      "Validation loss decreased (1.500165 --> 1.499209).  Saving model ...\n",
      "[ 995/1000] train_loss: 10.95706 valid_loss: 1.49246\n",
      "Validation loss decreased (1.499209 --> 1.492464).  Saving model ...\n",
      "[ 996/1000] train_loss: 9.94945 valid_loss: 1.48990\n",
      "Validation loss decreased (1.492464 --> 1.489902).  Saving model ...\n",
      "[ 997/1000] train_loss: 10.74488 valid_loss: 1.48425\n",
      "Validation loss decreased (1.489902 --> 1.484248).  Saving model ...\n",
      "[ 998/1000] train_loss: 10.39341 valid_loss: 1.48679\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 999/1000] train_loss: 10.92426 valid_loss: 1.48318\n",
      "Validation loss decreased (1.484248 --> 1.483180).  Saving model ...\n",
      "[1000/1000] train_loss: 10.54859 valid_loss: 1.47378\n",
      "Validation loss decreased (1.483180 --> 1.473784).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# define the structure of the Recurrent Neural Network model\n",
    "\n",
    "model_PINN = RNN(4, 2, 32, 3)\n",
    "model_PINN.to(device)\n",
    "\n",
    "print(model_PINN)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_PINN.parameters(),lr=1e-3)\n",
    "\n",
    "std_physics = torch.from_numpy(np.array([std_CA_input, std_T_input])).float()\n",
    "mean_physics = torch.from_numpy(np.array([mean_CA_input, mean_T_input])).float()\n",
    "\n",
    "n_epochs = 1000 #1500\n",
    "\n",
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 20\n",
    "\n",
    "model, train_loss, valid_loss = train_model(model_PINN, patience, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945bec05",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc0c530c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 out of 560\n",
      "2 out of 560\n",
      "3 out of 560\n",
      "4 out of 560\n",
      "5 out of 560\n",
      "6 out of 560\n",
      "7 out of 560\n",
      "8 out of 560\n",
      "9 out of 560\n",
      "10 out of 560\n",
      "11 out of 560\n",
      "12 out of 560\n",
      "13 out of 560\n",
      "14 out of 560\n",
      "15 out of 560\n",
      "16 out of 560\n",
      "17 out of 560\n",
      "18 out of 560\n",
      "19 out of 560\n",
      "20 out of 560\n",
      "21 out of 560\n",
      "22 out of 560\n",
      "23 out of 560\n",
      "24 out of 560\n",
      "25 out of 560\n",
      "26 out of 560\n",
      "27 out of 560\n",
      "28 out of 560\n",
      "29 out of 560\n",
      "30 out of 560\n",
      "31 out of 560\n",
      "32 out of 560\n",
      "33 out of 560\n",
      "34 out of 560\n",
      "35 out of 560\n",
      "36 out of 560\n",
      "37 out of 560\n",
      "38 out of 560\n",
      "39 out of 560\n",
      "40 out of 560\n",
      "41 out of 560\n",
      "42 out of 560\n",
      "43 out of 560\n",
      "44 out of 560\n",
      "45 out of 560\n",
      "46 out of 560\n",
      "47 out of 560\n",
      "48 out of 560\n",
      "49 out of 560\n",
      "50 out of 560\n",
      "51 out of 560\n",
      "52 out of 560\n",
      "53 out of 560\n",
      "54 out of 560\n",
      "55 out of 560\n",
      "56 out of 560\n",
      "57 out of 560\n",
      "58 out of 560\n",
      "59 out of 560\n",
      "60 out of 560\n",
      "61 out of 560\n",
      "62 out of 560\n",
      "63 out of 560\n",
      "64 out of 560\n",
      "65 out of 560\n",
      "66 out of 560\n",
      "67 out of 560\n",
      "68 out of 560\n",
      "69 out of 560\n",
      "70 out of 560\n",
      "71 out of 560\n",
      "72 out of 560\n",
      "73 out of 560\n",
      "74 out of 560\n",
      "75 out of 560\n",
      "76 out of 560\n",
      "77 out of 560\n",
      "78 out of 560\n",
      "79 out of 560\n",
      "80 out of 560\n",
      "81 out of 560\n",
      "82 out of 560\n",
      "83 out of 560\n",
      "84 out of 560\n",
      "85 out of 560\n",
      "86 out of 560\n",
      "87 out of 560\n",
      "88 out of 560\n",
      "89 out of 560\n",
      "90 out of 560\n",
      "91 out of 560\n",
      "92 out of 560\n",
      "93 out of 560\n",
      "94 out of 560\n",
      "95 out of 560\n",
      "96 out of 560\n",
      "97 out of 560\n",
      "98 out of 560\n",
      "99 out of 560\n",
      "100 out of 560\n",
      "101 out of 560\n",
      "102 out of 560\n",
      "103 out of 560\n",
      "104 out of 560\n",
      "105 out of 560\n",
      "106 out of 560\n",
      "107 out of 560\n",
      "108 out of 560\n",
      "109 out of 560\n",
      "110 out of 560\n",
      "111 out of 560\n",
      "112 out of 560\n",
      "113 out of 560\n",
      "114 out of 560\n",
      "115 out of 560\n",
      "116 out of 560\n",
      "117 out of 560\n",
      "118 out of 560\n",
      "119 out of 560\n",
      "120 out of 560\n",
      "121 out of 560\n",
      "122 out of 560\n",
      "123 out of 560\n",
      "124 out of 560\n",
      "125 out of 560\n",
      "126 out of 560\n",
      "127 out of 560\n",
      "128 out of 560\n",
      "129 out of 560\n",
      "130 out of 560\n",
      "131 out of 560\n",
      "132 out of 560\n",
      "133 out of 560\n",
      "134 out of 560\n",
      "135 out of 560\n",
      "136 out of 560\n",
      "137 out of 560\n",
      "138 out of 560\n",
      "139 out of 560\n",
      "140 out of 560\n",
      "141 out of 560\n",
      "142 out of 560\n",
      "143 out of 560\n",
      "144 out of 560\n",
      "145 out of 560\n",
      "146 out of 560\n",
      "147 out of 560\n",
      "148 out of 560\n",
      "149 out of 560\n",
      "150 out of 560\n",
      "151 out of 560\n",
      "152 out of 560\n",
      "153 out of 560\n",
      "154 out of 560\n",
      "155 out of 560\n",
      "156 out of 560\n",
      "157 out of 560\n",
      "158 out of 560\n",
      "159 out of 560\n",
      "160 out of 560\n",
      "161 out of 560\n",
      "162 out of 560\n",
      "163 out of 560\n",
      "164 out of 560\n",
      "165 out of 560\n",
      "166 out of 560\n",
      "167 out of 560\n",
      "168 out of 560\n",
      "169 out of 560\n",
      "170 out of 560\n",
      "171 out of 560\n",
      "172 out of 560\n",
      "173 out of 560\n",
      "174 out of 560\n",
      "175 out of 560\n",
      "176 out of 560\n",
      "177 out of 560\n",
      "178 out of 560\n",
      "179 out of 560\n",
      "180 out of 560\n",
      "181 out of 560\n",
      "182 out of 560\n",
      "183 out of 560\n",
      "184 out of 560\n",
      "185 out of 560\n",
      "186 out of 560\n",
      "187 out of 560\n",
      "188 out of 560\n",
      "189 out of 560\n",
      "190 out of 560\n",
      "191 out of 560\n",
      "192 out of 560\n",
      "193 out of 560\n",
      "194 out of 560\n",
      "195 out of 560\n",
      "196 out of 560\n",
      "197 out of 560\n",
      "198 out of 560\n",
      "199 out of 560\n",
      "200 out of 560\n",
      "201 out of 560\n",
      "202 out of 560\n",
      "203 out of 560\n",
      "204 out of 560\n",
      "205 out of 560\n",
      "206 out of 560\n",
      "207 out of 560\n",
      "208 out of 560\n",
      "209 out of 560\n",
      "210 out of 560\n",
      "211 out of 560\n",
      "212 out of 560\n",
      "213 out of 560\n",
      "214 out of 560\n",
      "215 out of 560\n",
      "216 out of 560\n",
      "217 out of 560\n",
      "218 out of 560\n",
      "219 out of 560\n",
      "220 out of 560\n",
      "221 out of 560\n",
      "222 out of 560\n",
      "223 out of 560\n",
      "224 out of 560\n",
      "225 out of 560\n",
      "226 out of 560\n",
      "227 out of 560\n",
      "228 out of 560\n",
      "229 out of 560\n",
      "230 out of 560\n",
      "231 out of 560\n",
      "232 out of 560\n",
      "233 out of 560\n",
      "234 out of 560\n",
      "235 out of 560\n",
      "236 out of 560\n",
      "237 out of 560\n",
      "238 out of 560\n",
      "239 out of 560\n",
      "240 out of 560\n",
      "241 out of 560\n",
      "242 out of 560\n",
      "243 out of 560\n",
      "244 out of 560\n",
      "245 out of 560\n",
      "246 out of 560\n",
      "247 out of 560\n",
      "248 out of 560\n",
      "249 out of 560\n",
      "250 out of 560\n",
      "251 out of 560\n",
      "252 out of 560\n",
      "253 out of 560\n",
      "254 out of 560\n",
      "255 out of 560\n",
      "256 out of 560\n",
      "257 out of 560\n",
      "258 out of 560\n",
      "259 out of 560\n",
      "260 out of 560\n",
      "261 out of 560\n",
      "262 out of 560\n",
      "263 out of 560\n",
      "264 out of 560\n",
      "265 out of 560\n",
      "266 out of 560\n",
      "267 out of 560\n",
      "268 out of 560\n",
      "269 out of 560\n",
      "270 out of 560\n",
      "271 out of 560\n",
      "272 out of 560\n",
      "273 out of 560\n",
      "274 out of 560\n",
      "275 out of 560\n",
      "276 out of 560\n",
      "277 out of 560\n",
      "278 out of 560\n",
      "279 out of 560\n",
      "280 out of 560\n",
      "281 out of 560\n",
      "282 out of 560\n",
      "283 out of 560\n",
      "284 out of 560\n",
      "285 out of 560\n",
      "286 out of 560\n",
      "287 out of 560\n",
      "288 out of 560\n",
      "289 out of 560\n",
      "290 out of 560\n",
      "291 out of 560\n",
      "292 out of 560\n",
      "293 out of 560\n",
      "294 out of 560\n",
      "295 out of 560\n",
      "296 out of 560\n",
      "297 out of 560\n",
      "298 out of 560\n",
      "299 out of 560\n",
      "300 out of 560\n",
      "301 out of 560\n",
      "302 out of 560\n",
      "303 out of 560\n",
      "304 out of 560\n",
      "305 out of 560\n",
      "306 out of 560\n",
      "307 out of 560\n",
      "308 out of 560\n",
      "309 out of 560\n",
      "310 out of 560\n",
      "311 out of 560\n",
      "312 out of 560\n",
      "313 out of 560\n",
      "314 out of 560\n",
      "315 out of 560\n",
      "316 out of 560\n",
      "317 out of 560\n",
      "318 out of 560\n",
      "319 out of 560\n",
      "320 out of 560\n",
      "321 out of 560\n",
      "322 out of 560\n",
      "323 out of 560\n",
      "324 out of 560\n",
      "325 out of 560\n",
      "326 out of 560\n",
      "327 out of 560\n",
      "328 out of 560\n",
      "329 out of 560\n",
      "330 out of 560\n",
      "331 out of 560\n",
      "332 out of 560\n",
      "333 out of 560\n",
      "334 out of 560\n",
      "335 out of 560\n",
      "336 out of 560\n",
      "337 out of 560\n",
      "338 out of 560\n",
      "339 out of 560\n",
      "340 out of 560\n",
      "341 out of 560\n",
      "342 out of 560\n",
      "343 out of 560\n",
      "344 out of 560\n",
      "345 out of 560\n",
      "346 out of 560\n",
      "347 out of 560\n",
      "348 out of 560\n",
      "349 out of 560\n",
      "350 out of 560\n",
      "351 out of 560\n",
      "352 out of 560\n",
      "353 out of 560\n",
      "354 out of 560\n",
      "355 out of 560\n",
      "356 out of 560\n",
      "357 out of 560\n",
      "358 out of 560\n",
      "359 out of 560\n",
      "360 out of 560\n",
      "361 out of 560\n",
      "362 out of 560\n",
      "363 out of 560\n",
      "364 out of 560\n",
      "365 out of 560\n",
      "366 out of 560\n",
      "367 out of 560\n",
      "368 out of 560\n",
      "369 out of 560\n",
      "370 out of 560\n",
      "371 out of 560\n",
      "372 out of 560\n",
      "373 out of 560\n",
      "374 out of 560\n",
      "375 out of 560\n",
      "376 out of 560\n",
      "377 out of 560\n",
      "378 out of 560\n",
      "379 out of 560\n",
      "380 out of 560\n",
      "381 out of 560\n",
      "382 out of 560\n",
      "383 out of 560\n",
      "384 out of 560\n",
      "385 out of 560\n",
      "386 out of 560\n",
      "387 out of 560\n",
      "388 out of 560\n",
      "389 out of 560\n",
      "390 out of 560\n",
      "391 out of 560\n",
      "392 out of 560\n",
      "393 out of 560\n",
      "394 out of 560\n",
      "395 out of 560\n",
      "396 out of 560\n",
      "397 out of 560\n",
      "398 out of 560\n",
      "399 out of 560\n",
      "400 out of 560\n",
      "401 out of 560\n",
      "402 out of 560\n",
      "403 out of 560\n",
      "404 out of 560\n",
      "405 out of 560\n",
      "406 out of 560\n",
      "407 out of 560\n",
      "408 out of 560\n",
      "409 out of 560\n",
      "410 out of 560\n",
      "411 out of 560\n",
      "412 out of 560\n",
      "413 out of 560\n",
      "414 out of 560\n",
      "415 out of 560\n",
      "416 out of 560\n",
      "417 out of 560\n",
      "418 out of 560\n",
      "419 out of 560\n",
      "420 out of 560\n",
      "421 out of 560\n",
      "422 out of 560\n",
      "423 out of 560\n",
      "424 out of 560\n",
      "425 out of 560\n",
      "426 out of 560\n",
      "427 out of 560\n",
      "428 out of 560\n",
      "429 out of 560\n",
      "430 out of 560\n",
      "431 out of 560\n",
      "432 out of 560\n",
      "433 out of 560\n",
      "434 out of 560\n",
      "435 out of 560\n",
      "436 out of 560\n",
      "437 out of 560\n",
      "438 out of 560\n",
      "439 out of 560\n",
      "440 out of 560\n",
      "441 out of 560\n",
      "442 out of 560\n",
      "443 out of 560\n",
      "444 out of 560\n",
      "445 out of 560\n",
      "446 out of 560\n",
      "447 out of 560\n",
      "448 out of 560\n",
      "449 out of 560\n",
      "450 out of 560\n",
      "451 out of 560\n",
      "452 out of 560\n",
      "453 out of 560\n",
      "454 out of 560\n",
      "455 out of 560\n",
      "456 out of 560\n",
      "457 out of 560\n",
      "458 out of 560\n",
      "459 out of 560\n",
      "460 out of 560\n",
      "461 out of 560\n",
      "462 out of 560\n",
      "463 out of 560\n",
      "464 out of 560\n",
      "465 out of 560\n",
      "466 out of 560\n",
      "467 out of 560\n",
      "468 out of 560\n",
      "469 out of 560\n",
      "470 out of 560\n",
      "471 out of 560\n",
      "472 out of 560\n",
      "473 out of 560\n",
      "474 out of 560\n",
      "475 out of 560\n",
      "476 out of 560\n",
      "477 out of 560\n",
      "478 out of 560\n",
      "479 out of 560\n",
      "480 out of 560\n",
      "481 out of 560\n",
      "482 out of 560\n",
      "483 out of 560\n",
      "484 out of 560\n",
      "485 out of 560\n",
      "486 out of 560\n",
      "487 out of 560\n",
      "488 out of 560\n",
      "489 out of 560\n",
      "490 out of 560\n",
      "491 out of 560\n",
      "492 out of 560\n",
      "493 out of 560\n",
      "494 out of 560\n",
      "495 out of 560\n",
      "496 out of 560\n",
      "497 out of 560\n",
      "498 out of 560\n",
      "499 out of 560\n",
      "500 out of 560\n",
      "501 out of 560\n",
      "502 out of 560\n",
      "503 out of 560\n",
      "504 out of 560\n",
      "505 out of 560\n",
      "506 out of 560\n",
      "507 out of 560\n",
      "508 out of 560\n",
      "509 out of 560\n",
      "510 out of 560\n",
      "511 out of 560\n",
      "512 out of 560\n",
      "513 out of 560\n",
      "514 out of 560\n",
      "515 out of 560\n",
      "516 out of 560\n",
      "517 out of 560\n",
      "518 out of 560\n",
      "519 out of 560\n",
      "520 out of 560\n",
      "521 out of 560\n",
      "522 out of 560\n",
      "523 out of 560\n",
      "524 out of 560\n",
      "525 out of 560\n",
      "526 out of 560\n",
      "527 out of 560\n",
      "528 out of 560\n",
      "529 out of 560\n",
      "530 out of 560\n",
      "531 out of 560\n",
      "532 out of 560\n",
      "533 out of 560\n",
      "534 out of 560\n",
      "535 out of 560\n",
      "536 out of 560\n",
      "537 out of 560\n",
      "538 out of 560\n",
      "539 out of 560\n",
      "540 out of 560\n",
      "541 out of 560\n",
      "542 out of 560\n",
      "543 out of 560\n",
      "544 out of 560\n",
      "545 out of 560\n",
      "546 out of 560\n",
      "547 out of 560\n",
      "548 out of 560\n",
      "549 out of 560\n",
      "550 out of 560\n",
      "551 out of 560\n",
      "552 out of 560\n",
      "553 out of 560\n",
      "554 out of 560\n",
      "555 out of 560\n",
      "556 out of 560\n",
      "557 out of 560\n",
      "558 out of 560\n",
      "559 out of 560\n",
      "560 out of 560\n"
     ]
    }
   ],
   "source": [
    "y_test_error = list()\n",
    "total_batch_num = y_test.shape[0]\n",
    "\n",
    "for batch_num, x_batch_test in enumerate(dataloader_physics_test, 1):\n",
    "    print(f\"{batch_num} out of {total_batch_num}\")\n",
    "    model.eval()\n",
    "    model.to(\"cpu\")\n",
    "    \n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    x_batch_test = x_batch_test[0]\n",
    "    NN_output = model(x_batch_test)\n",
    "\n",
    "    loss1 = torch.mean((NN_output[:, 0, :] - x_batch_test[:, 0, -2:])**2)  # use mean squared error\n",
    "\n",
    "    # compute the \"physics loss\"\n",
    "    C_A0 = x_batch_test[:, :, 0] * std_CA0 + mean_CA0 + C_A0s\n",
    "    Q = x_batch_test[:, :, 1] * std_Q + mean_Q + Q_s\n",
    "\n",
    "    NN_output = NN_output * std_y.to('cpu') + mean_y.to('cpu') + torch.from_numpy(np.array([C_As, T_s])).float().to('cpu')\n",
    "\n",
    "    dCA_first = (NN_output[:, 1:2, 0] - NN_output[:, 0:1, 0]) / (t_step)\n",
    "    dT_first = (NN_output[:, 1:2, 1] - NN_output[:, 0:1, 1]) / (t_step)\n",
    "\n",
    "    dCA_center = (NN_output[:, 2:, 0] - NN_output[:, :-2, 0]) / (2 * t_step)\n",
    "    dT_center = (NN_output[:, 2:, 1] - NN_output[:, :-2, 1]) / (2 * t_step)\n",
    "\n",
    "    dCA_last = (NN_output[:, -1:, 0] - NN_output[:, -2:-1, 0]) / (t_step)\n",
    "    dT_last = (NN_output[:, -1:, 1] - NN_output[:, -2:-1, 1]) / (t_step)\n",
    "\n",
    "\n",
    "    dCA = torch.cat((dCA_first, dCA_center, dCA_last), 1)\n",
    "    dT = torch.cat((dT_first, dT_center, dT_last), 1)\n",
    "\n",
    "\n",
    "    # Physics-based Concentration loss\n",
    "    loss3 = dCA - F / V * (C_A0 - NN_output[:, :, 0]) + k_0 * torch.exp(-E / (R * NN_output[:, :, 1])) * NN_output[:, :, 0]**2\n",
    "    loss3 = torch.mean(loss3**2)\n",
    "    \n",
    "    # Physics-based Temperature loss\n",
    "    loss4 = dT - F / V * (T_0 - NN_output[:, :, 1]) + delta_H / (rho_L * C_p) * k_0 * torch.exp(-E / (R * NN_output[:, :, 1])) * NN_output[:, :, 0]**2 - Q / (rho_L * C_p * V)\n",
    "    loss4 = torch.mean(loss4**2)\n",
    "\n",
    "    # backpropagate joint loss using appropriate scaling factor\n",
    "    loss = 1e3 * loss1 + 1e-1 * loss3 + 1e-5 * loss4 # add all loss terms together\n",
    "\n",
    "    # record validation loss\n",
    "    y_test_error.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f2cad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean error is 1.709591373920973, std is 2.684643091688111\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of the Test dataset\n",
    "\n",
    "print(f\"mean error is {np.mean(y_test_error)}, std is {np.std(y_test_error)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d7d8bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbe1a707a00>,\n",
       " <matplotlib.lines.Line2D at 0x7fbe1a7079d0>,\n",
       " <matplotlib.lines.Line2D at 0x7fbe1a7078b0>,\n",
       " <matplotlib.lines.Line2D at 0x7fbe1a707790>,\n",
       " <matplotlib.lines.Line2D at 0x7fbe1a707670>,\n",
       " <matplotlib.lines.Line2D at 0x7fbe1a707550>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD4CAYAAAAEhuazAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAACGw0lEQVR4nO2ddZiU1RfHP+8m3d0hiIQFAiJ2oWKAit2K3f5UxO7uxELFQCUMQgXFFqQbpLtZNtic+f7+OO84s8vsssmC3M/zzLO789Z935k9995zz/keTxIOh8Ph2LuIKe8GOBwOh2PX44y/w+Fw7IU44+9wOBx7Ic74OxwOx16IM/4Oh8OxFxJX3g0oLHXq1FGLFi3KuxkOh8OxxzBlypRNkupG27bHGP8WLVowefLk8m6Gw+Fw7DF4nrc8v20ldvt4ntfU87yfPM+b63neHM/zbvbff9DzvNWe5033XydHHDPA87xFnuct8DzvxJK2weFwOBxFozRG/jnA7ZKmep5XFZjied4P/rYXJD0bubPnee2Bc4EOQCNgnOd5bSUFSqEtDofD4SgEJR75S1oraar/ewowD2hcwCGnA59JypS0FFgEdC1pOxwOh8NReEo12sfzvBbAQcBE/60bPM+b6Xnee57n1fTfawysjDhsFfl0Fp7n9fc8b7LneZM3btxYmk11OByOvZpSM/6e51UBhgG3SEoG3gBaAwcCa4HninpOSYMkdZHUpW7dqAvWDofD4SgGpWL8Pc+Lxwz/x5KGA0haLykgKQi8Tdi1sxpoGnF4E/89h8PhcOwiSiPaxwPeBeZJej7i/YYRu/UBZvu/fw2c63leoud5LYE2wKSStsPhcDgchac0on0OAy4CZnmeN91/7x7gPM/zDgQELAOuBpA0x/O8z4G5WKTQ9WUZ6TMC2A9oV1YXcDgcjj0Qb0/R8+/SpYuKk+TVFtgMjMNWoh0Oh2NvwfO8KZK6RNv2n9f2ORPYAhwLTCvntjgcDsfuwn/e+F/u/9yKdQBTy7EtDofDsbvwnzf+bYAjgQpAZeA4XAfgcDgc/3njD3AFkAE8DFTFdQAOh8OxVxj/M4HqwI/ABFwH4HA4HHuF8a8EnA98CdTEOoBq2BrAlPJrlsPhcJQbe4XxB7gSc/18ArTEOoDq2AzAVQlwOBx7G3uN8T8YExl61/+7BeEO4HhcB+BwOPYu9hrjDzb6n0o43r8F1gHUwHUADodj72KvMv7nA4mER/9gHcBPWAdwHPD3Lm+Vw+Fw7Hr2KuNfEzgLGAKkR7zfApsB1MRmAK4DcDgc/3X2KuMPFvO/DRie5/3mWAdQC+sAnMyow+H4L7PXGf8jsQoz70TZ1hxzAbkOwOFw/NfZ64x/DKb3MwErHpyX0AygNq4DcDgc/132OuMPcAl24+/ls70Z1gHUwTqAifns53A4HHsqe6XxbwycDAzGqslEoxnmAqoDnIDrABwOx3+LvdL4gy38rgXGFLBP5AzgBOCvsm+Ww+Fw7BJKo4ZvU8/zfvI8b67neXM8z7vZf7+W53k/eJ73j/+zpv++53ney57nLfI8b6bneQeXtA3F4RSgPrlj/qPRFOsA6uI6AIfD8d+hNEb+OcDtktoD3YHrPc9rD9wNjJfUBhjv/w1wEiaz3wboD7xRCm0oMvGY7/9bbAZQEE0xF1A9XAfgcDj+G5TY+EtaK2mq/3sKMA9zq58OfODv9gFwhv/76cCHMv4Canie17Ck7SgOVwAB4MNC7BuaAYQ6gD/LrlkOh8NR5pSqz9/zvBZYnfSJQH1JoUH1OszLAtYxrIw4bJX/XrTz9fc8b7LneZM3btxYmk0FrLj74ZjrpzBl7JtgHUB94ETgj1JvkcPhcOwaSs34e55XBRgG3CIpOXKbJFE4+5oLSYMkdZHUpW7duqXU0txcCfwD/FrI/ZtgLqD6QC9cB+BwOPZMSsX4e54Xjxn+jyWFlBPWh9w5/s8N/vurMS9KiCb+e+XCWVhhl50t/EbiZgAOh2NPpzSifTzMds6T9HzEpq+xNVX8n19FvH+xH/XTHdgW4R7a5YSqfH2Baf4UlsZYB9AA6wB+L/WWORwOR9lRGiP/w4CLgGM8z5vuv04GngSO9zzvH0wt+Ul//9HAEkxd4W3gulJoQ4m4AlP5/LSIx4U6gIaYC8h1AA6HY0/BM3f87k+XLl00eXLZlFsRVuUrnuIVdFkNHE04aaxnqbXM4XA4io/neVMkdYm2ba/N8I3EwxZ+pwDTi3F85AzgJOC30mpYHrYBdwGjgOwyuobD4dg7cMbf5wJ2rPJVFBphHUAjzAVUFh3AZuAFoLd/nRsx1dE9Y+7mcDh2J5zx96kF9GXHKl9FoREWBtoY6wAKGz5aWJpg1cYANgGvAt2AfYGHgcWlfD2Hw/HfxRn/CK4AkoARJThHaAbQGHMBlWYH8C4WL9vd/7sy8IB/rQeAfYAemF7G5lK8rsPh+O/hjH8ERwMtKb7rJ0RDrANognUAv5TwfAAZwGNYaNUfWJ2BNOB1rC7BciycKhkLn2qA6Wh8QfFnMg6H47+LM/4RhKp8/UjJXSgNMRdQU6x2QEk7gLewqKJHsAXqr4FOwEZsJpCJLQbPAqYBN2OF6PthHcGVWIcULGE7HA7HfwNn/PNwKfZQ3i+FczXEOpKmlGwGsB14ApuZHO2/VwGTSm2EdQCHAXOxjuFA4FlMQOkHTFFvqH9sc0xedXYx2+JwOP4bOOOfhybYYu1g8q/yVRRCM4BmWAfwczHO8RqwHhv1R1IXM+6Vga1YcfqZEdtjsey6DzBlvU+A/bGOoRPWSTwHrClGmxwOx56NM/5RuBJzsXxXSudrgHUAzTEXUFE6gBTgKUxC4rAo29tjC9RBIBUb3U+Jsl9l4DwsR2AN8DKQANyBdXjHY51EShHa5nA49lyc8Y9Cb0y3v6QLv5E0wFxAoQ5gQiGPexmL3Hm4gH2OA97EFoWzgWMpuOBMPcI5AvOBe7E1jksxsbrzMA0Ol0jmcPx3ccY/CvHAxcA3mLultIicAZzCzjuAJMxFcyrQdSf7XgXcjo3cE7CRfGHCTCNzBH7HOoDv/fY1Bm7CJZI5HP9FnPHPhyswn39hqnwVhfrkdgFNKGDfF7AOoKBRfyRPYeGdm4Aa2NrF+EIe62E5Aq9jGkUjsTWEQeROJFtSyPM5HI7dG2f886Ed5mN/h9If9YY6gJZYB/BTlH1CUg5nYguzhSEW+Njffws2cj8FE5srCgmEcwTWYc8glEjWGnsuLpHM4dizcca/AK4EFlI2Us31sTWAlpiB/jHP9mexBdyHinjeypi7qqZ/fFss1POrAo4piBrYLOgnLJHsCWw2ch0WyXQ68CW23uBwOPYcnPEvgLOBqtjItyyInAH0JtwBbABeAc4FOhTjvI2xDiAZ+4APwCqWfVHC9jYjnCMwDVsP+Bt7TvVxiWQOx56EM/4FEAqPLGqVr6JQD+sAWmEdwHjgaUyS4YESnPcgrDjNTGyhuTvWmQwpSWN98iaSfY/NLj7DQk1bYJ3EnFK4lsPhKBuc8d8JV2AZtp+V4TXqYaP+1lgH8DJwIbbIWhJOBZ7HZgGHAEdhUUzvlfC8kcQSzhFYjyWSdcI6ho5YJ+QSyRyO3Y/SKuD+nud5GzzPmx3x3oOe563OU9oxtG2A53mLPM9b4HneiaXRhrLiEMyYlWbMfzTqYaP+ylh8/XGldN6bgWuxxeOzsGSxK7AF29ImMpFsNfASFjZ7ByZxcQIWPeUSyRyO8qe0Rv6DscjCvLwg6UD/NRrA87z2hN3ZvYDXPc+LLaV2lDoeZiz/Jrd0QlmQifnpawL9gXGlcE4Pm0mcgPnob8ZmBNdhHUJZUZ9wjsB8YCBWtPkSf9v5uEQyh6M8KRXjL+kXLLqwMJwOfCYpU9JSzCbsLIepXLkQC38s69H/Y/7PcUAbzEiXRgcQB3yOuZHOAx7FZgG3YTLQZU1kItlvWAcwlnAiWUiB1CWSORy7jrL2+d/ged5M3y0UKkLVGFsnDLHKf28HPM/r73neZM/zJm/cuLGMm5o/tYE+wEeUXUjjUqxzuQo4GHMBhTqAH0rh/NWBb7FOrA8WTXQ+MAB4kF1jeD3COQLrCCeSvYX1/u0w8TqXSOZwlD1lafzfwNYwD8SSRp8r6gkkDZLURVKXunXrlnLzisYVmHLmyDI6/yPY4uk9/t91CXcAp2ERNSWlBRbvvxob+b8NXIblEtzDrh15500kexvLG7gf+9L0xPSKXCKZw1E2lJnxl7ReUkBSEPvfDrl2VmPrfyGa+O/t1hyLSTKUhevnH2wh9FpyT4HqYlFAbTFDWRodQHcsMud3bF3hbeAazP1zG+XjeqlBOEcglEi2FXseDbEwUpdI5nCULmVm/D3PaxjxZx/C9UO+Bs71PC/R87yW2OB2Ulm1o7QIVfkah7loSpOHgEQsNj4vdbAZQFtKbwZwDub3/xhbZ3gd87u/CFxP+SZpRSaSTcXURydiiWQNMLfYz7hEMoejpJRWqOenwJ/Avp7nrfI87wrgac/zZnmeNxPL/bkVQNIcbP1xLrbud72kQGm0o6y5DPNbl0aVrxBzsdj4G7AomGiEOoB2WAdQGnUG7sFi/h/AksFewMpAvoEZ2PL+QDzCOQKrsE7vdKytR2EurAG4RDKHo7h40p4RY9GlSxdNnjy5vJvBSdiodBnmoy8p/TDhtaWYkS+ITVj8/3zMd1/SBIlMLAT0LyzL+FBsFvIQcAEWvxtXwmuUNmnYvQ/BOoQAtqh0ERbJ1DDfIx2OvQ/P86ZI6hJtm8vwLSJXEh6JlpQZ2ILnzezc8EPuGcDp2LSpJCQCwzFXyxlYB/Qg8DjmEjqP3S8OvzLhHIHIRLLbscUjl0jmcBQOZ/yLyKnYQmxpiL09iIVg3l6EY2pjHcB+mMEuaQdQG8vIzcGkJZIwd8rz2CLrWdgMYXckbyLZPdjieSiR7AJsVlUatZgdjv8azvgXkQTMV/41pr5ZXKZgYaO3YRm9RaE2tvBcWh1AW6wO8CJsYTUbW6B5DbvPMzChud2ZfQnnCIQSycZg9RJcIpnDsSPO+BeD0qjydT9QC7ilmMeHOoD2mAuoqAVb8hKq2jUOi/gRJgHxDrbA3Bvzt+/uRCaSrcU6tcOxnIGuWIf5KKUfseVw7Gk4418M9sNKHr5L8UaSf2I+6/8B1UrQjlAH0AEbnZe0A7gUc/m8jbl9wDq6D7EY/F6Y9tCeQiLhHIH12H01AO7DJLRDiWSF1SVxOP5LOONfTK7A/Mx/FOPY+7F1gxtKoR21yN0BjC7h+UK6P/8jnM18ISZp/Re2oJpUwmuUBzUIJ5Itwxa1t2CJZA2wRJRhuEQyx96DM/7FpB9QhaJn/P6CGeu7/eNLg1AH0BEzYqNKcK4YbKR/CLZgOsV//2xsBD0Vy3bek2UXmhPOEQglkv2FdXoukcyxt+CMfzGpgulSD6XwrhBhLoeG2IizNKmFCcB1BPpSsg6gIhZLXweLblrlv3+6//4cLGuvJAveuwPREslOI5xI1hKLIJpbTu1zOMoSZ/xLQKjK19BC7j8eG/nfgxnY0iZyBtAXU/EsLg3841OxDiDVf/8krGNZhC0SR1boWrQI3n0X1q8vwYXLiVBFsg+x9YGPMVfa0/7Pg7F1kLXl1UCHo5Rxxr8EdMMMQ2FcP6FRf1PMrVBW1MQ6gE6UvAPohOlwzMQSq0KSD4duh2dnw9IsaLcefl1m748fD1deCeecA8E92GcSLZEslnAi2YmYvHdqfidwOPYAnPEvAaEqXxMJq9blxxjMr3wvFoVSltTEXEAHUPIOoGcqPJ1udYAv2QAdO0LVqnB9J8g8AlIrQL/6FjrZrx88/zz8/DO89FIp3MhuQCiR7G9gHjZrW4jlerhEMseejNP2KSGbgEZYbHx+ZRGFLaBuARZgcgS7gq1YdM4MLJLl1J3sn5MDf/4JU6aEX/Pnw7PPworbbATc8TXouwEOPhg6d4Z1jeEEz0bLPwL7CE4/Hb7/HqZOhfbty/YeywNhUV4fYTOjrVgN5vOwyKjO2MDA4ShvCtL2cca/FOiH+fPXEH1UPxKLwnkPUwbdlSRhvuy8HUByshnnqVOhYUM47zxIT7dRfSAAjRqZcT/4YDj1VDiwc1hPaBS5ReVm+NeIxZ5D7fU2QzjnHHj11V11p+VDJjbyH4LNjrKwbOMLsVlBy/JrmsPhjH9Z8z1mDIdiHUEkQUx1MgOLGikPlcwNWdA7AaYDx70Ji56Hf/4Jb+/XD4b6q9Y//wz77gsNGux4nhQsW3YpVgymY8S2uVgIaABbc6iwEPbZB2L2IsfiVqyD/Qhb2AdLJLsQC5WtVU7tcuy9OONfxgSxEd6+7Kj2+QXWIQzBRoJlzdatNpoPuW2mToXERPhttrmApuTAIc9A70B4ZF8/v0ICUViJLXQnYGsdkYf+AxyDRUB9j7k/1q61Nv0X3T8FsRyr0/ARtlYQjxWsv9D/WaH8mubYi3DGfxcQ0sFfghUaARsFd8L8vzMpHf3/SLZsMQM/cybcdht4Hlx8MXz0kW1v0cKMe9eucOedsM2zGco0rFM6vZjXnQwcAeyP1QGIDFtdinUAW4ExgusOhsxMa2fFsohv3c0RNuP6COsM1mPZxmdjHUFPXNSFo+woyPgjqcQvzJ29AZgd8V4o7+gf/2dN/30PeBkLFZ8JHFyYa3Tu3Fm7M8sleZLuj3hviKzxn5fidX78UTrzTKlFCwnCrxUrbPvff0vffy9t2hT9+K2SukqKlzSyBO0YLrvfs2WFmiNZLmkfSVUkPTfJ2nfrrSW4WBnym6QDJQ2UNL+Mr5Ut6TtJF0mqLPtuNJM0QNKcMr62Y+8EmKz87HZ+G4rywgaCB+cx/k8Dd/u/3w085f9+MrZG5mH1xCcW5hq7u/GXpBMkNZWUI/tHbyNpf+1oHHfG+vXS6NHSI49IZ5whNWsm/fmnbRs6VGrdWurXT3rqKemHH6TNm4t2/iRZBxAnaUQR2xbJ07IPZ2CUbasltZNUSdJpL9k37ccfS3CxMmK+pESFv2hdJb0qaWMZXzdVNjjoJSnGv/bBkp6XtKaMr+3Yeyhz42/XoEUe478AaOj/3hBY4P/+FnBetP0Keu0Jxv9zWWPHSHrP/31no+u1a6VRo6R58+zv33/PPaJv00Y691xp6lTbHgyWTluTJHVTyTqAoKQrZfc5OMr29ZI6SUoMSo2ukJo3l7ZtK+bFypDXZPfQQdZeZDOj0yUNk5RRxtdfK+lFSV38a8fIBhIfSkop42s7/tuUl/FPivjdC/2N5Rz1jNg2Huiys/PvCcY/Q1JtSX0ltZD9M+e11du3Sw8+KJ16qtSoUdjI33efbU9Olp59VvrpJykpqWzbG9kBDC/mObIkHSMzlj9H2b5JNqKND0iHPytt2VLMC5UhQUlnyO5hiqTpkm6X1ED25asp6RpJf2jHz7O0mSubSTX3r11J0gWyAUV2GV/b8d+j3I2///dWFdH4A/2x9cXJzZo1K8tnVGrcKilWdgOj82ybPVt65RWpUiVpv/2kCy+UXnhB+vlnM/rlQZKk7rIOYFgxz7FF5uKpJWlhlO1bFe5kSnP9ozTZLKmJzFUXGm1ny4zueZIqyj7TfSQ9JGlxGbcnIOlXSVfLOh8k1ZN0s6TJKvtOyPHfwLl9diFTZA2uKulH5f4nHTjQnniDBtJ770nZu8lQbptK3gEsklRHZjyjLUFsk9RTUkxQOuAZacOGYl6oDJkgc7lcEmXbNpkr7yiFv5SHSxok69zKkgzZzKyvpAT/2u0kPSppaRlf27FnU5DxL8sos6+xUqr4P7+KeP9iz+gObJP0nxFL/N3/GY+FPB6BlUEU8MgjMHYsNG4Ml19uWbDDh5dXS8NUw9rYBTgHS1QqKq2xTOblmJ5QVpRrjAW6pMGM26DX5+bw2p04EtNe+gBT9YykGpad/RNWDOYxLLytP6aAeg42pc0ug3YlEi42sw4rt1nPb2tLLPFuEBZe63AUmvx6haK8MAn0tdh3fxWmd1Ybc+n8gyV91lLY//8asBiYRSH8/dpDRv5pMj/xEf7vr8qif5B0iKSvZTOBYFAaPlzq0EG68047NhgsvcXc4rJN0qGyGcCXxTzHx7L7vUTRXRPbJbVd7O/zVzEvUoZkSzpMNnNbtJN9g5ImSbpBttaDpLrada6ZZZIek80CkM0K+mjXLFI79gzYFW6fsn7tCcb/WVljIxc+M2WugZb+tgMkfSHz6ebk2AKwJI0ZI3XrZqGb5dkJbJPUQ7ZuUdwO4AHZvT6ez/a0HKnmr7bP/VuLeZEyZJmkGrIOO7OQx2RK+krSmQq7ZtpLelLSyjJoYyRBmbvxVkn1/WvXkHSVpF9U9FBjx3+Hgoy/y/AtJXIwdc+KmPxvvTzbs4Eb/oC364LaQOVlcOBoOHQF3H4LTJoE110Hq1fDEUfAE09Ajx679h5CJGNFWyZitXvPKuLxwqQsPsVUL8+Oss+8xdBpJgT6wBNYIsjuxDDsvu/Aai5nYvpMmXl+j/beZuzZ/YW5wQD2AQb65yyt8p3RyMGm20OA4ZjURnPs87gQ2K8Mr+3Y/XDyDruAHOwf+ytM9+YcrEB714h9Jk6EIZ+atML6KzFltIXwTC24uQ48+Qjcf394/5o1oVs3+OQT+33qVFi50tYMGjWCevUgroyU4lKAXhS/A8jAhN6mYkXTu0XZ55c/4PWuMDQOHsBeHiaLkUnhDG1hthX3vbIo5l4JWxO5GFsTKm3Jj0hSse/jEExrKYhlYl6IyU9H0e5z/Mdwxn8XMh94HRiMGdBDsALh/QjLPY8bB/2vgaX7Q8LDkNURmgfh0nXQ8HtYvcT08DMzzbj//DMsXw6vvWavEDEx1gksX26/f/qplVIMdQ6hV506O2+3sNlJpPHbDFyOaXA8CPSgaAZ0K2Z8srFFydgo+2UA6wTpni2SBwlXDCspCdgzT8SE1CJ/Rnsv77YY4H1s9DwAqFuMcyRiNYIHYAb4S0z9dRs2U7wA6wgiFVLLgnVYJz4EmOLf2/FYR3AGZTsbcZQfzviXA8mYmNerWIdQFyvfeC1WCjAQgBEj4KnnYHJDSHgQsva30djlmO6+hxnHEaPhpTeh+1HQ/UjIjoVNKbAlDdICcNwptt9XY2HBUnJZosTq0PNY2754JWwXxPjbgwkQjIfsODPGpUEMuQ1gDGZ4ErCSl5XIYygFY7+C7ftDeivrLE/G3GclMdwJlI5g2iysTUdjdQyKc85bgXewwQDYZ/EN9v0IVQE7EOsEdsWIfB4WzTQEc0tVwqKJLgSOo3xkxx1lgzP+ZcgHmI+/INfBZkzNMVTzNR4zItnYSLc0SCBsTOMCEJsNXhbE5UDD2mYUF82GresgIwlyUq1x9WvCxefYsR8OgrQtUKMi1KwEdapA2+bQvQc8CswT3JcNpyfkb3SjGY7xmAvpOMzo5d3n00/h/POhx0T4o6t1kK+y+6hdvo5VanseM+RF5TwsU/GfKNs2YCPyj/x9YjHp7Ysx1dWyFEINkrsiWRIm0R2qSHYwriLZno4z/mXI0ZhPO5K2WB3dkFHctg4m/QrEQrvesDTBOoc6gu6erQtU8/fdshbuexeCpwIHQOUcOD/OCooHN8J7r8PQD6FTG/hprB1TnFFuaqpp7WdlQYcO9t6DD1rZxjVrbOF5zRro3Ru++MJGrbX+hJxDoPLV0GKiuZTOOMMWqgG+/tpqAzRqZMVg4iPqVb6NxcRfhxn2SKMiwbnnwvARcOFKGFzfZj+DKFufeGGYPh3StsMzh8JozxZxDy7iOY7B8h5+28l+czFDPASLl66GLZZfhLnNyrIzzMQK1g/B8hWygHaEK5K1KMNrO8qOMpd03hWv3TXUM5SGf4PCYXaVZZIAI2Xx1pFibXFx0o13SkcMlphq+8elSF1+kZ7y02vXrrVs4KonSHxr+1TLMbnozZKWLZOmTLF9t2wxXaCy0MwJBqX09PDvr38kNV8heTnSIc9aaOrDD9v27dvD9wiS50n165tOUWh7j9/sXvrPNqG6deukgB+HuGmTZT536CgNzLb9LlD569m0a2f3U6WZVHmL1DhVSi5iKG57WXZuYQlIGi/LlagiexbNtWtkpyWT6xgky2AO/QMeLuktf5tjzwEX579ryJFJOvRXOOmnuuyfeFRQ+muydMst0ssvWzz/Hf+TOFTiE4ksiYB0skwT6MKLpcMOk3r0kBqdKnVfY+erlCPdkWWKmZL02WdmaKtXN0Nc1hpBKTJDECPps4j3s7PNoH/7rfTWW9IDD0hXXWXJbJK0eLFEjMRwiRyJU+zb98ILtn3ZMunQQ03sbsAA6azJdr+nZph4XHkxapS187jjpMQTrO2X+Nv+/jucp1EQtSRdV8zrh6SfT1BY+rmbTIk0n5INpcpSuUSyPRln/EuRTFnizHxJ6QXslyVprKRLZR0AMiNwlaRxso5i4kSpYkX7FBp3kTp/LdX1R701N0otX5RqtAiPpg+6WKr4lXUSMelS55+l10dIX3xhKqEg1aljo+2yTBSL7AA+LcJxWVnS/JVSuxSpQrZ01yfSjBm2bdo0y3iuUSN8v9xiz+I0Sd//LLVqJfXsKZ1zjhWHeeaZcBGb7dul1NRSvMkopKVJ1220Nr2fJVWtaiJ9Z54pDRkibd264zGZsv0fKoXrr5b0jHLLTp+hXWOIQ4lktyh3Ill/uUSy3Rln/EuRL5S7YQ1komjnSrpb0psyox/ZOWTIpB3OV3gaX0/S9ZLGpknvDZaOPdZG8MRLTy4ziQFkkr6XbJcGTzb1zwkTpCP7SwyWyJZIl3hFoqkVeDnxRKl3b+ndd23/tWvLpiNIkclYFLUDkMyINfFfq6NsX7xYOu88aexY6clkew49kqR+l0hHHmk1DipVsm/vX75ExODB9ne1auaqOeYY6aKLpJV+eu3SpdIff9gMI6MYlnLJErv2nAW+/ENQ+uA36dprpYYN7drx8dKgQbmPWylr/1tFv2SBTJd0m8KGuKaka7VrZKezZd/xC2XfT2QS5gMlzSvjazuKhjP+pUhQVopvoGz0G0rlz+8Vs1Zqslw6YZN0R6b0iqR7ZRWcQjLBjWWp+V+vl159LWysz3tGavaDFJ9j+x0lk1zIljRnjtTnDsl7R4rJlmJzpGMXm9Tw8uXK5X+vVEnq3l363NdTzsyUFiwouapoZAfwSRGPnSZbG+ksc21E8ssv1hFec439/Y6sZOQxEfsGg1bvIMv3Cc2cKT35pHTTTdJZZ5kLqXlzadUq2/7II7mfSZ060v77h9dKfvpJeuMN6auvzJ2zZo3Jb4RYtUqqXVs68EBpfnpu+YdAwDqW//0vvBYzfrzNUm75yD67kUV8PoUlUna6guxa+0h6WNKSMrpmJCkyt9SJCrulOkt6QVakxlG+OONfhqTLpr2PyvyyoZH9Dq9smV8/z/tVs6WawfA/Tm1ZXdwRku573B9V1pIq3C9V8V0OTWR+2A2ykf3SgPmUY7PsOkctl8atlF57TWra1D7lihVtvSEQMN98aKTavr3Ut690zz3hamJFIVXhDuDjIh77jX/cGdrRbXDHHdbGMWPs74/8fXvK9IeKyooVVhrznXdsbeSaa6TTTgsb+Kuvzt05gFShQnhB+rXXpJNPtvePPVYa6C/W35XP9UaNkg44QOJk26/txXbdsnRN5Sc7/basbkNZs1Zm9Dv7146RDXKGaMcO3rFrKMj4u1DPUiYHmA78Avws+DkA20KB7WuhRwNoKhj6NJAONANaQNy+kNOQHWIbqwjqpUBwNqz8A/Y9BBKPhGlAvOA8z2QkDgGe/QQe3g4p5wMVoMtSeKMxrBwD990Hc+bAIYfAV1/Bd99ZWGfotWgRjBkDxx8P334L11wD7drBfvvZz3btoHt3qFx5x3tOA04BfsVCFc8vwvN6CbgF+B9W9DlERgZ06QJbt8KsWVCrlslinA90xuShaxThOjsjJwc2bLDw1lCoa0oK3Hmnbb/hBvjsM9i8OXxMlSGQeoFl7n56OSxYYGGuoQzrffeF+T3g7rpw8Jmw+nc7b2ysPeO6de3ziCmDGM7lWCLXh1jBjEQsb+BiLI8gPv9DS4W8iWSVCSeSHYtLJNtVuFDPEvCnpFaywt7nyEZ6b8im2vNkEsUFEZA0W9Lrsql5I0Xc1CaJERK3SnSWSJBWSBqeJTUcLcUskQjavrE5kheI8mD892oGpFMlvZQj3fSbVHuYhO8v7yNpUo708cfS4xFSm3//HXYxZWaGXSh//ildfLHUtav50EMj4blzbfuwYVaF7NFHpS+/NBfUlkwbccbIRnqFJSibtSAboUYyZYqFxl57bfi9kbKFzoO1a6Jd8pKUJHXqZKP6b3+0MM76km553NYZ9t3XFoLBorUel93bduUe9bdpY/s0bixdf725icqiuE9Q0kTllp0OVQSborJfHwjIZsb9Za4yZOtkt+6i6+/t4Nw+xWetLEKnm/J36TQISt2DuRd9v5O0QDtGBAVlfvn3JV0uqWV2+DyVcsx1dMlCicMlEiXqSVwn8XN4vzaSWk6SmgyVKn0pMV8iM3rbKuWEy0pW3yBdvcXa9slkO/+RR0q//Zb//QeD5v/+8cdw5xDpTgq9YmOldSl+BxCUrpxg+Q2bo5X1ykO2zD0QJ4uEimT4cMsBiGS0pERZ1Mt67XpWrbLIH0ma6bell3K7rpKT7bmdtFBKiBIWtmWL9NFHUp8+4Yivq66ybZH5FaVJpqzzjJSd7qBdIzstRa9Itp/Mhbl0F1x/b6Qg4+/cPkUgCKwEZgNzgAeGQsY+mE5upfyPq5MNTXNgn3hoE2fZkqFXM2AL5jL5xX/N8o/zsoCJIH/D87cAJ8FHmTAtpBL3F6YUNgyqdoOun8GPsdbYRsnQqRqkxcCUDNieyA75+jHrILgEGmZBr3bQvUHutlUo4HmkpsLCheY2WrEC7r7bXEAtZ8PG/bD6bR+be6N7d8sABnPjVK0KzZqFXR7JwGH+8/2THaWHc3LsejVq2N/jMf2j5v7vjQpoZ1mRmmoCfOv65i//0GoSrKgNOa3zP8/27eaGa9LE3EBz5pia68knQ58+cMopUK1a6bZ9Cybp8CH2vD3MHXMRpjpa1kJvWzCRuyHYdx8si/kiTEG2Zhlff2/ByTuUARJ8842l/z/wEGYtOwIdoMqh4HWE1MaghILP4wHV06D+dmgmaBMPLSpBRqL5SmcEYZoHAQ9iBAd5kPMjzPgMqu4HOSdBejs7V7sN0L8GtNwIfT4GrgRqQdxc6PgzXNMIBiXA1G72fuI6OLgSrNkEKz0INmEHZ3CdLDOwbROgURYEFkO97VA3DepsN+2gDh2gZcvwMSkBOCETJlWEfqOh8ggz8oMG2fauXeHvv6FiRWjb1tYTjjoKTrrGpC6qCP7yTAwv9KyPOQYqVTJfued3YL9gaw0NgB+BpkX7CEvMwIFWd+H7H+C1Y034La/8Q+1ZkJkNqUXQhFi0CJ59FkaOhPXrISEBjj0WXn0VWrUq3XsA0xwagq3XLMXGMWdihrisZafBymJ+4l9/PiZX0htbHziZsBquo+iUq/H3PG8ZJg0TAHIkdfE8rxY2Xm2Bffb9JBVYgnRXGf+l7KjVE8mXX8Kff9ooLTYWFIR69eG88yA9HYYMgS1bIDsLVq2DrBpY+frGEa8GFP4/KgheMlT0ILMCBNKxYVloxWw1sALiK0CtA2G9Z8cwD9PujcWGVM0gJgOCEzCr2QGr2FILKq2FTstgvyAMHo4V4KwTftXrAKoNmwXBvEpfSVAtC46sZ31fC/9VH7jZv9SH2D9yiL/+gtmzYd688IJz587w+edWP6B7BiTOgiMegQ6trXNYuhSeeso6kKuuijgXJhpXE+sAIvqgMictzUbqW7fChJlwXF0TYptKeORccSlUXwXrDi/6+QMBe1YjRsCoUVYPolo1GDrUdJn69IHmzUvvfoTVoP4QmxWEZKcvxDqCspadFvbshmCFgNZji/r9/Ov3YPcR+9tTKNcFX8y418nz3tPA3f7vdwNP7ew8u8rnH1p8dK+ivSrJfPZ53w/5drtLGiBLdoq2HhIKqQwEpH5f2DE1R0kVfH/4jTdaiGWVKlLnztJll0lPPWVx+cOXW7hsE0kL8/tgy4iZM6XERKlXL+nHgC14XxKxPWartO8PpXvNiy7Sv2stBx9sOQxz5pTuNdIlfS6pt8JrRgfJQjnXle6louISyUoHytPn74/8u0jaFPHeAuAoSWs9z2sITJC0b0Hn2VUj/yxMonkqNnCegqkthqSXPUGjFKixGDL/gjXfwPa5FgbYvTtMmQJjx8KBB8L++9sMwcs7WgaSk2HGDBv9zp1rP/9ZAa+Nhczm8PV8mJwMwdaQWjviwFBDIoZAsWngpUGgAqgqeSQz/VeMHVthIwS2QnZDoDp4y6Dpz1A5HpYeDRkNwVsAegXzY/jXq1kLhg0zt8PatWZ6Gja0ezsNm9B8A6zBevvQa5H//tZwE3LRkNxrIKHXWOAF4IEgXLYyvDbQoYP9XqGCuURC3PI+fHwpeAE4/RU4oo7NGPbd19YXypI334Rrr4Xnn4ekW+FhbPR6pmzG1vM7+PXE0r3mokU2Ixgxwmaixx0HP/xg2+bNs3uP9r0rDiHZ6Q8JTyZPxMJGT6NsZafBpNBHYs/0B+w71BmbDZyLzTId0Snvkf9Swra0v/9eUsR2L/LvPMf2x2TOJzdr1qyM+sadkyYL+XxVFqFzgHKPcqvnSMfkWBjold9J8e3CI7O6dS05KBT1UlA4X0ZGeAT8xRfSUUdJNWtKVJKFgl4q3ZghnSSpVlqUBxaQErdKLVZLJ6bYqK1tphQXGQkUzHNM6O9siblS3HjJ80XkGqdLvT6TGjSxhKfffrOkshtusHurXl2qVUuqMENqvyzc9mjP71hZlu7zsgL3H8j0bi6TdLSswH20mQOy6KbzJd0j6bI/pKaXS7+uk9ZtNXmHwYNt5DtHUq1MifUSncKfQePGFkopSatXm6jeypWlJ3sRDEoPPmgSEtny5R9kIY5IequM4xlXr5ZmzbLf16+XYmKkJk1stvTjj6UbQjpHFtHWRHZv1SRdIftMd4W+T95Esli5RLKCoJxH/o0lrfY8rx7Wcd8IfC2pRsQ+WyUVuMC/uy34ZmBROaHZwVT/72x/e9UcaLQO4mdBxu/w7cPQJgauvdoiRA45xBY+DznE/N1V8gmvkKxu74wZNtq71Q8n6dsXRowD2kPF7pBwEmzvCNn1yLVomwC0xvy18cAmYFoQNkY6T4PgBUGxRK/ekY0N4f8ClgDL4ODa0DYehr8GWRMgNgneXGHJTR98ANu22cynUyd7NWsH51SwyJz3sUCgvOSQe+awCKsDsB5bJtno7xNJI3LPGCoCLwkygzDwN9CftqYwYIDNAt57D664wo6tUiWcwPbEEzZLS021BdaEnSzU54cECzOhewVr21zgaywyaVeQlmYztOHDLYIoIwNq17Y60CecUHrXCWBrYx9hUTtpWGDARf6rbeldKl/mYolkH5M7kSy0UO0SyXajaB/P8x7EZnFXsZu6fUpCJhYGGukymom5ksCKczRaD8G/Ycs42DQWWAitW5lhB8uyrVvXDGdBBiglxUImZ8yw1/TpFg0zbhx8Nh+u+R3SD4ecNtg8PUguV1E1zOWSCWyQ1anFw1xEQf+YFOy/qjG2oprDjv9RWf7+obn5MmAptIqFFb9Azgo7Z9eu8NNEyzIdL7h+Ktxc3dxIBWW4bgYOxdxGv2Ohp8uA+RkwdCI07QkrYu29FexY/7cOZoha+K+6aZD9j7nqNkyGf2Zb5zBlij33hx+2V6tWuTOcL7hg5x2CBGeeaXWX+w2Fs/2O9PPlcHYpLswWlrQ0c0EOHw6PP26Lw198YUELfftaKGlpuMTSsI/+Q2Ac9nXohrmFzsHiB8qSIPbdGEK4IlkDwhXJDmLvrUhWbm4frDOuGvH7H1hwxjPkXvB9emfn2l21fXZGlkzE7B3ZYnI3hQW4kFQxW2q3wTIuP5RU9yiJGCkhwTJsb7ghrG9TFB55xNxNDdtInCYxSGJd2D0Uu0GqvlWqkpP7QccqimsoIFVPkuqk+H+vlxp/KzV7RuIxiU1StYBUN2fHDy4+Rxrwc1jXPy0oxf1o5+QSE53r0kV65ZVw2zdsyO2SWSiTw24naav/3rffmkvnvvvC+2VLWi5zQTzvHxMnW6hsqfDCZeSrkaQeCruV7vxHOv996cQbpPYH2ecQqfFz553SEUdI/ftLzz9vekFLl4bb8OST1q633pL2W2/XeHx50T67suTNN63IDti9nXKK6R3l57IrKiHZ6Y7yP3+ZdtNw7Rr9/3SZxHUf/9rIsrAfl7RsF1x/d4Pycvt4ntcKGOH/GQd8Iukxz/NqY510M2xs2U/SloLOtSeM/AtLDuFIzJDLaDr+6BtIzIHaq2zDpu/gzBbw4QBQNpx6Khx0kLmLDjkk/wXlSDZtstlBTBzMTYTBm2ByfUwQCGA1JM6G9rVg/84w1zMXVkbehePQ39mYDykZK3B7PBzVHn6qCBMmwpBf4N1fIXEoeBVgkgedIk61MRV6B+DvanDcZxB8F3r1gjvuMHdRjRpQp07YZdSpE1Q+GS5pBEdgRc/jgUsvtdDa33+3pKi8rMESl1Zgrpcj2XFBOvIVbebQSJYAt2+izRwWfAfzx5pOT9IMIAtatw7P3B5/HN5911x1Hb6A6adDnQDMiYV6BX9Mu4xAwBaJR4ywWUGVKjaLBBg9Gjp2tAS8kiBgBjYb+ARz3dXCZgIXYzODsh6NR0skOwKbDewtiWRO22cPIEemAfShbBbQUyZ5HHoAFSQdlCnVHSrFXCVxgESclT4cOtTOkZ4eXlh+/fXwImA0MjKk72dJl/0m1flVIjSqT5Xq/ylxpdTodKnr49L+0/0F5rwzgtArKJEhNZ4gcaZEJanyZ7btraTo198u6XjZIvC7Ee8nJUkvvihdcYXNfCpXtlHq669Lg/3rNftOumeg1Sxo0MB0ckJyC3lZL5OBSJTJQhRE5Mwh2oJ0tJlDnQyp/ZbwzKHt0xLHSewj8arENikmMyz/MHSo1VlYv75sC+4UlmDQZlqSfSeqVLHn3aWL9NhjxVN6zUu27NlHyk630a6TnZZ/nUcl7etfP0EmczFC5V8qtCzBafvsmeTIYpqHyAp3HCmLIgk9lLgcqfZi6fS1VnP1lT8kEqR99rEInZaLpcuXS4+OkW67wzTrQ/o8eVm7VRowQdr3eylmefgaibMkHpLoIn06yoxi29SIjiBYQKcgiXelU4ZKU2UunyVLwkZvu0zLKG8HEEkgYMVdQvo+l66083r/Uy5tobPOsu1LlphLaNmy8HU2yYTgElQyXf3idA5kSXW22e/HBKS4a8OdQ436VncgVAAmGJQWLiwbgbfCsmCBua66dQs/26eesm2BQMk7rG2yz/pIhZ/REdp1stNBSZMVrkhWU//tcpTO+P+HCMh84J9KukNW4KSGwg8qNkeqsUiqOFxisf5V/WSNxNtSwjnSMadLH3yQ/z9ydo40erkJfvWUrOaupJgNUvs/pD4fShXbSNwhEaE86u2kI/CCEsulxDFS+2HSJWOkn5KjzwAKuv9+/rmeX2LiaMcdZyJyktUEDhmtatWsBvLVV0uLNtl6S1zQkpfKgrydQ9VNUvxmE7urpCjPJiAlbJBarraZw80pEldKsb2k1r2k0/tZnYVQgZjSYuRI8/2HRvyRfCepi6RHJE1aK736qjR7tm37/nupWTPp5putolxksZvisFQ2Gm8rex4VZMq532rX1G3OloWu/pcpyPg7bZ//AMIiMCOjjKZiPk8wTaCqguQAKB7z2f8CLWbBUWlwfEvzq3fsGH39YMYqeGkBfBcHaw4AakBMNhyyHaY9AlkDITYTEupDeuh4YQ70P8HbBhUOhfTaEdtCiWf+j4pY1MhphKUE9iF6uF46cDQWSfUrlvADZvI3bTJd/dmzzY89a5Yl0a1YATmVoP0SWN0cOj0HJ24M3/dBB5VeUlSIfYEDgtDhEeh9MZzR0tYqXscSp5blea2QaTj9SxBYC/vEQdf6UGEdDHsOWnrQoTJ0rgv772t1D6pXL3y7TjjBEsJiYy0hrE2b8La3gGv83z3gOOAy4Axg2h/w5JMWqpyZadFRp51mOkQhwb3iICyx8kNM1mELtj5yPrY+cCB7b7ROSdltQj1LgjP+RUPYSnqoI5gCTBZsjjTOod83AOOhwpdQexr07Gohiz17WhZvJNu2wxszYE4L+KsuLMpjnRMC0DgLllaIOH8QmAQ8AiRA4vEQewxs98NQO+VAszjT5kmPPBdmQH29vH9/tsTyFbphkaYTgSZ+mGW1ajB4cJ5nobBh/3gk3NEG1u0HcddCziCLg9+40fZ5+WVISrIOoVMnC/mMLaayWQ2gbwqMaAb77ANP/AEnxlvnNjjK/nnzHJYBSwJ+xxCbf+fQvjIcWANiVsA/P8B+FaFzbejZDNq33jFE9bnnbHH92mvhtdfsvm+8Edatg87XwYCjTaE0yW/nCqA6Fjp5GbBfKowdYwvGkydbBxIbCx9+aJnXJ51U/BDSLGxB/0MsMzwb+8wvBi7Aoo4dhcct+Dokmb9zhWyR616ZO6JqXldEjsQyiY8lTpWOPi1cBH3yZFuQDfHbb1JcO4kfon9oTbNzh7UiiY0Sj0hcLpEqsUJifws5TJe5gJB0drL0v6B0sqTmec5RQebD7y1byG0paZake+83d8+IEQU/h+2yBVgk3b/WsmBD9O5t9YNDrqOKFa2YfIhJk0ynf2e+7+3+c+07xYrfgHT77dL9/nWLUvAmRKRbaXBQuiNZ6rVGOjzbnkFM3mI/AYmVUuft0gWSLlkhXfKrNPAnidbSsxHhtffeK9WrJ8smD0gHDjPXTkBWY+EChT/L9rJwzrXK/Rw6d7b7TEy05/juu9LGjcW4UZ/NssJJh/rX9SQdJwuKSCn+afcqcG4fR34IG23+DgwH/gjCKg8UOUPYBFUXQOo3oInQOgXaNYQff4R69eDoOTC4Mpw3EWZVgtmdolwolCAWOeNIBe8JODsZDmtj+jSNW0OdXyDneKh4I3SZAQccAMeeAQ2PtToKoXoKczBR0xCVBZoNwVkw4FToVtVGjY3Z0W2QCZyNjS7z6vCnpZmrKOQ2ql/fahVI5urYvNlmC6Ew1FNOgRPzaPfMSYWOVeCssfBFL7j+enj9dfh6NDx1krmspmHZ16VFaOYwdztM2gAzk2FRNtQ8CJbHwLKAn8UdIggJm6FLbWgZA5U3WgLc0HaQOQ2u+gbeeBGCQVMUbdcNvoixDO2/sDzAk7DZQG8gNmBht6EQ0hUr4JJLbCYmmSZUo2IWXgjJTn+IzYQqY3UHLsZcgGUtO72n4tw+jiIhLFPzQ2B8DqyNJvuwCJgMrfaDJQfA1Zng3RKWrEisAncshq+P3cmFPCAAMT+D9zlU/g6O6Q4zH4Al7aD1k7DuUbjpJouhT0uzmP7997dOoXUXGN8N3qxivuHEVJiYRi61r+pYJxDpOuqAxXlfiMWCP4FlGxb4XAQ//ZR7PWH2bDPsTz1lRVk6dTLxubgeMOJuuOhzeLePxdZ362ZrEj8tgW6J0Ab4DXNv7QoyAzBpFfyxBqYnmQzFpirQ+jjfzRSlc6ieBs2CMOsbqLYVDm0IZxwIzVvDeM+kFdZhmdQXYB3BAf6zmjbN3EDt29uz2n9/+3706WOvfQvM6Y9OKJv3I8Ky0439a1+Mfa6OMM7t4ygRG2TRKydlSgl+2cmYKJE93mKJzyXukir0lg7tbdLLINW9wnfzKJ9XVsTPUeYWqvirvfdGIBzHv2qVdOqpO5aRPGaG7ftksrlparaRPl1jYnzXysIJa+W5Zm1Jh8tizpFJMRfVSxEISNv9Qs7r1knnnCN16CDFnB764lregiT98otlbH/zjfTGBtt+VxGvV1pkZ5t4YGQJz78mS09+KnX4yz7bGq9Kdb+VDtgixa+UvDwZ3F7QMqT3k9RMJmeNpH0k3SfL9g2xdq3Vjz7kkPBn1r59OJKoOGyXNFS5ZacP1q6Tnd4TwIV6OnbGsElS8yekW56TJk6Mnu6/dq10yZXSqDRL2c/1Aa2XmC/FrM39frUtUsN5UoUsie1Sp6el2x+Xmo/O54MOSvhx8aRL1Xz10pejtHnzZstdePFFadZcMwIxAYkT9a/vuXNn6fLLpX/+sTWPNUHzYb8oq818qHLnTiArcH6MpJtk9Qd+U1hWorBcOdHO9b+XLXZfMpmOyA4r/h3b59UFtj0pacd6xWVFICA1bBjOj4jkK1m7vt0sXXyxtbVhQ+mnX6Wpm6VbR0oHPCfdly1dKmnf1VKtJH9AoNyvCrLOIVKhZMUK6eWXpeOPl1J85/2gQdItt1gHWZwQ0vWyzzRS7fMUSZ/JOom9FWf8HTvljnkRD3uGVPFF6cRHpDV+LPjmzVKnTqbFM2Gq6ezUlvSDpJckHZ5mOQZIit8uddos9QtKdZZqx/j2FRIjJO6VXlwk9fzGDH3UDz5itvD4Tu4hRdIBQalyjvT419Jtt9nosk4dM/6Sxa03by6dfrp0//2mObRosbTMX1xGtqDZRbkzrJHUWNKJsoS7dyVNVP4LjxfMt2OW5xmCJiVJAwea7HLvflKlJVKdbDNer78eNrQnnGALxIMHhw1kaXP11ZbRm5Eny2mp/3lVuEWKjzc9o+Tk/M9z1ll+hxYndT9HuuoHqVV27mdXUzbLmZ/POf73P+uswRaer7rK8gqKw2z/Wo39a1eTdKV2nez07oQz/o6dEpRlEz+UKrVfK3n+P2+toHR2llT3ZimmjvT6W1KvoAmmTchzjhRJj8ySEj+UvHURH942ieelVk9IvX6QTtkqNUvzk778fSpukfha4quCvwiXby74PlZKaihzQwz62r7hL70UjkoZPdpcM+3amQEGi+5JTrZncOpiu86Z66Xk7SYL8I0s4e1CmUhc3gim5rJR5p2ySJQpkq6WGbxoJCdbFnaTJtIvWy1iqZekmbOlZ56RLrnEKnRVqGDt27bNjnv5ZenMM612wLBhlo1bkkSr0aPt/KPz6F4EJcWnSk2/lubnZ63zMHeBdMFHUpUx+teF11MmaPh+Rm7XzKGyjPRtec6RnGzyF+eeK1WtKp14YnjbuHFF7wRzZLO8ixXuyFvIIt0WFO1UeyzO+DuKTJKkoUHpwhwpdrP/IeRIrLLfD/pKGpXHaLzxhhQXJ7U9WDre1wo6OCi1T1F49L9KOmGZhZv+tUx66g/p6Uzp7HSp8Vb9m02MVLBshKRr/5HSoxi/ybKM2q5B6cQzzIhGM2JpaRa6+fHH4ffO6CPxhH+Nd6V2HUzBM0RyspQdtCzrEbIM1XNl+kEhFcnQK17S6TLNn08kzVBYSmDKFBtVn3aa9Kp/n8/naV9OTnjGIpnsQps2uUNRGzQId2xjxkhjx1pxl8LIMIS0fK6+2lwx55wjTZ1q2w4LSD0KcY5lkh6Q1NS/5zpB6cJ10nBfE+iff2y2eM450vtjpMezzQ2EpIqyDnW8dhyRZ2SEQ4zXrrV7rlDBntf77xfdPZYq6SOFs8mRlRZ9XRZS+l/FGX9HsZk4UapcVXpglHR8HtdM4jrp/GRpZI4UX92+TUdfInXMscW/F2WjSMncGoMlnbpdquYbu9hsibFS/G3SMVdJjz4q1eohNfelkBsEwqn/O3vVnyBNiVitHSH7Jz91u1SztonEFUYzJxAwN1C/uXbexj9J/c4Pbz/oIKvOdvzx0h13mLzEAn8YmSVprqQvJFXZJCUmmaGL1PyJlbnMzpR04l8SZ0l3fySdJussCqPkkJYm/f239N574cVkycTYQp1CrVrSkUfaImuI9PQdTqW+fU2yoVIlM64ffmjv3yCpiqK7STL9ezxR9ow9//cv/G2RLF0qXXutVLu2tatGDenyK6Sv10vXSKruP5fmshyIaEJv2dmWc3DTTeGF/thY6csvbXtR9YZWSXpauWWn+8i+M3nbv6fjjL+jRKxbJy2WFB+0iI9K70u8LfGjRLL/AW2X2Br+wP43Ln+lzSxJP0q6OVtqnBzxIa+WyJAqBKXXJK1eY8Y4TdKvks4LSAl5E5nyeV37rfTwdvu9z1z7pj/ySNHu+3H/XGcprDXzxhu2gNy5c9hHfc454WNuvNEMcsJSqe44ey9D0kyZHtNA2WL5Psrt9orzX1VkbonhMtdEUbw6GzdawtpLL5nPvHt36dJLw9sbNrT1jt69pQEDzM8eMqZ9++auS/C2365FEeefJ9OTqutvayob9S8rRNuyssy9dNFF1hGE1GfH/S49/I90fDA8Ij9KFl0WrSxjMGgd3z33hGcGgwebEN1TT4UX1wtDUFZr41bZIj+yiLDrJP2l8MBlT8YZf0eJ2SATVNtXO6pXVs2UYvO6aBZIcc9J+98gvfehqW1mZeXO+Jw0ydwYdJNahWoS++epLqnm91LV66Vzb7RR3rZtZgw/yHP9nbmHkLTP29Jbb+ffIeXH8/7xp2lH9cfsbAtVDIUrJiXZrABk6xwv2Kj67bdte1aWuUFCkVTbZWqnH8oWKEOZrJGvCpIOlGXYPiHpa1lHXNSFy5wck2g+7zypY0dzz4HViP7+eykzUzr/fJspfP21NGK1Xf9T2Yytp8KdVF+ZRHPejiklJXcHkh+RM7CjjrJ2tGolXf+UdMM6qbV/raqy+sC/qWBD/MUXuWc9HTtakZ+irIeEZKfPVXhNp61M4K4Qt7Tb4oy/o1TJlPT6j1KFc6Umr0sdQm4cRTfKMdkSM6WqH0gcIVVvKLVsaQuulS+TqmTYoufzskXAEbLojBqhDiEg8acUc7905WtmCNLzXstfK/BWSPtsyOdLlCIxQarzgXTU29LYZYUbWb/mH99LOw8bDAalRWts/+ZvmLH96ivbNnWq/cdVqWJqo9ddZxW/Vq2yalpXXSXd5z+zhyW9J+l2/7qhgumhVyVZRNKlMqmF0TLpjoKMZGqqjfi/+caM/eTJ0ty5tsjcrZvNCv4NRT3Unnuc/1z3yZH6/yPNKcDX/sAD4cpqmzdLW7bs5GFJ2rrV3FfHHx9egL/4EovMuVThhdq2spnYqgLOtWyZzbqOPNIWzEO8/77066+F7wySFF12+h3tGtnp0sQZf0epEQz6pQqPlGqfK3XdZB/Q9bJOYZ0sCuhNWXLVgZIqBZR7dB6UuYg22t/10mx0OfRnqddJ5o4YPFiaNFn6bbv0YI603zb9K0/dSNIFqVKjG6WGU3f8stTIMldRVpaUGZR+kVQvx0pKVpwveRFrF1UktVknNRwqnTBEeupbad7CHf3I78jcEscoujsiktl+p3XmqNzvb9hgs4AbbpAOP9wkp8FG3g89ZL8fcqjUfIVUMUv6fV3udiRJ+l0WKXOzpGMlNchz79VkC5lXypKdvpe0Oih9+plFF4F1AJG8+KK9P2Wx9Ox2qW1K+HOqn2HPb3REjkKDBmasb7vNai2EuPpqC6uVTCuoYkVzO/31V+H88uvWWTnPUMnPLVukHidI530vdcv0BxKyznCobACQH5m+8z4729Y/wMpX9u9vi+KZhXTuL1V02elR2jOKwOyWxh+r5bsAEwq4e2f7O+Nf/qSnSxdeaN+a+OlhA9ElYO6BiQttdBmNVElDAtIZWVLFtIgPNo/LJmab5E2UGCxxt0Qfaexy61hGT5au+VM6Pimi9nC6wklhQameH2ZYT9LBn0rH97XImsWS6kiqvk6invTe39J7QVvYbLtRitke0Y5UKe4v6Zos6X1JXy+TVqyxaJEYmQskb5hiJGOT7DwPTCz4eQaD5g5LS7NRaYcOfiRPM4ktEhOl2g1tRCuZy2jmzB0L8mySjZRfl/mrj5TlYOT6B9osVZ4inbFGekXSTzJXXlDSZ2skPpDi/Gd3oGy200dSK/8aSUnSd99Jzz5roaidO9sC8YwZtv3996UqzaSKlSx/4tlnbS0kVIntwANtdlMUZsyQDjjAjo+JkXpcLJ0yTWrsDwJqygYdk1XwjGfbNunTT6V+/cLtCc1QsrLy/85GEpStA1yvcKZ4fVlRmKk7uX55stsZf0yHaTHQCpM2mQG0L+gYZ/zLn6ws6eijpcs/lGoEbUR/0AYzqkgiW/J+kdoMku78QJq/wAzc1q3m/rjrcZvKI0ukekgW9ZKYGT6e5VLMP1LFbbm/ALHyDfdXEk9JMddJ1YZKMUm596uTbX7bfbba395miYHSaRdJQ5ZIiUGpwl9S8za5E5dyJE3Plh5dLh07S2qyNE+S13YpYYpUb6rkBaSWKWY8ozHSP2ZyEZ/v6tU2cu7YUbrfl6s4YHTYR37DDfYfm5BgxvSSS6yIfLRs7KBsFnbnWKnSXVLPORa6WUM7PlckeZlStWk2wwl5ax71t+XX0eXkhK/97i8y19s8ybtVwh9tL11qi+QtW1qSYCgUNRQhVRjmzLGZROvWds75/1jRmdPT7PNEFmr7vCyqrCDS083tFQqhHTPGZihnnGEFjgrjqsqUuSb7Khze21HSUyrYLVUe7I7G/1Dgu4i/BwADCjrGGf/yY/LkcNWn13Ns0W8/hSNBciT9IemCpVLdVREf2lKp5idS3YulmNOkaltt5DxQuUPqMmXRP9enS40ijH5bSWdnSo0/kHr+KnVeIiUulMjI/eWoJhuF7fClSda/eQkkSTwiXfq7//f70pVXFXzfObIIl4cXSUdNlurNkbyU8Pk92Si58yTpnPHS4JlSUrq5vFDxDMGoUTb6HzbMEsWQuW4kc7F88oll3J54orkxmjQJH3vddSaNfcopFl4ZKgkZiqzJkbkrTlLY6NeXje7j8zzTRv69IVuM/lMFz3Y+8/et6CfJJQSkw5aYyygoi5CKlLYAqXp18/cXdhE+GLSZT4izzpIqNZYOeVdqu8WuGyfLrRipwlUDmzPHOtXGja1NcXFWGS5albNobJLNuCJlp4/X7iM7vTsa/7OAdyL+vgh4Ncp+/YHJwORmzZqV2QNy5M/HH9v0/ryLpBtlH8zJKnjha5WkR9ZJcaN2/PAbfyc99F7u5KW8LJJJRpwgKT4U2pkkMUxipORlSImBcATMlTJXTNWMAr50ocXnLKlRKPt4gC22FoWApElbpas2mwGtHJS8yFlKphTrn/+JbSYBkVrELNy5c+1nmmyGVF/5j2gj6ytccIGNYiMN7BFHWCjm/ZJq++62OkEL2YyszT5jpnTJ/dKHm2wEe7FsNJ33OTaTdR53yNZp/pa59B7xt/e5wJLZrpd1ykhqkyVdNEU6/4bwom6ozGaoE7jxRvP5F4Wff7ZF8po1/fMdKvX4PTwQqCeT4phViHMFApbTcvfdth4TmtG8+KL09NMFf19DLJQJ2rXwr19Z9hx/UNFCdkuTPdb4R77cyH/XEgjYwiBIh54sHeG7Zm5X4b7IM2ZIsUdKlTaHP8TKSREf6Ayp5hvSoDkFn+/PWVKVCyXGKZz9G5T2TTe30ZuTpMeflGbNspHhaplSJ5KOzJKOTTXVzh1Ex/y/jx4aVuUsKuNkWartgtKIddIdf0ndf5Ji1yhcO1kSWVLlhVKHidLlU6Rhq6XUQjiJ//hDGrs6LP+QX3jn+vXSFVfo38XYd9+V/pgsXTte2n+Nn4gVlGK/k+grkWAhtmefHU6UknZclA3K/OrnyUbSj8lqDe8vKUHh+/NkC+cVglL/jdK1v0kXPCPNWGURSy386CfSpVqjpNOekd4aZG63n382l2D16uEZypIlO+oNFURmprlyzjvPQlWzJA3LktrOkmL9z6GLbB2jEF6dXJx6ariz2n9/i2jamRJpQDbjuVLhDrCxbAZVAhHTYrE7Gn/n9tmN2b49LL3b7z6pRaYleL27E4MVDFpIXbpsZOgFrcLUz6HtsgzYuzdKbVZG6AdJ6rZIOvBp6am3w4ugkjQxS6rni84dJAvBeyhH6h6RFMQaiXek2ldKl90kDftGOsM37i/5186QFeseJumKZKnKQr8DCEpdJxX/Wf0sM3z7yEItJanVVClmnkWKDNosdRkrVf9DIiIENSZgfuJ+aVL/2dI3W3JHEaWkWJRKjx7Sy36n93x+bfjZpCLuuEOalGIddGgd5t9ErKD530eONG2gPn0stv7BB+0cW7bY9Q4+2EbhH3wgTZ8uHRGw6KG8ZORIMzKkLyVds0aK2yoT4cuKuMegue5OSpf6bpSOD9izQuY2fEFhaYVI10/XrpYIdscdRUvaimTSJHsm1JHqPCrV99VmE2XROmNV+NH40qXSCy/YLMrzzIUl2ff9r7+ir7mECMlOn6LcstMvaufrE6XB7mj847Ca4y0jFnw7FHSMM/67jgkTzPd53UipelDy0iROlmLjbWTZqZNlWIZ4+mnpiSf8RJvOUkPf/3qNCvZ7Jkn6XDY1rhqKtsmR+EXyHpVaT7UOpI4svDHvP+sGmW/1tDSpYsjlk2l1AJ4JhktCDszjv/16gsS9UmxIMTRVevnr4j+vP2VJaS1k8gS150qV/txxv4xM6dsZ0tVjpBuTzH0WkqwO3Xu1FdLBs6VHU6X7x0lUkQbcY37sSPmHCRNssVeyTuP5LdJh/nniZAvpY7RzAxca7a9ZI510klmE+Hj9O9o9eqa5tsZ8Z9E7p59uA4NKlUwhVbK1iLgtUtVPpfMvlYYtkJ5bKd2WahFDbRXW+sf/vWJEW4+Q9LFsxB4MmojbWWeFE9GOPdbeKypbtlho7THHSHgSB0n91oWjdZrIdJeK0r+sXx+Ovpo2Tf/Otq65xqKh8kZiRbJOZvQP9q8fkp0eqrKTnd7tjL+1iZOBhX7Uz8Cd7e+M/64lKWnHxCIvICUmS5VXS01Xme/3gqAU84rEwxJjfOO9UTp0qMkTrMuRWraRmr4gHXq7dMHFJlX8ww92nexsmy3MmSfdNVyq+LxM8tm/ZlVJl0u64BPp5bel5cujtzdLll9wW460T6Tv31+grfie1LWXdOlCUypFlrX7ebYUmyzFzJXmro5+7sIwWeYiaSIpYa1Ur5DGKiVV+vJP6bLhUvvPpYrjFF6klsx9NE9qPU2qki01yJZOu8b+cxufJl2ZHXYt7CtL+CpuIZPMTPPDn3aaSWH873/S034uBvso11pC5crSoYeaYN5W//rx91jsf1qarT0895ydNxCw2eA0We3iuyWdqrDk8r/fL9lC85kyF9P7m6VbX5WaNje9f8lcRSHjWxTWrLGoo0DAZoEnvSfV/MO+08jWjN6VVIBy9Q4kJ9sC/FlnWWcY0i76+++dH5uf7HQhDi0Su6XxL+rLGf9dzyKZnMBgmcvhXlkc+bmyxdgukmpski3GaievkM89W2K1VHOu1C9ZunGbxK0Sl0jcJ7HE9mu9TrovIPVKl+JCSVnbJb6Vag2ULrzHopDySx5aKunlgLTvYuVWCpVUZ5VN+7OzrTMZvFIiS6o9WcoqQcD2dPm6N0Gp806cu9nKP1ls0yYrjv6tpKPGS4zM3SEiXxRPUlxQOi5oUTxFbXrks7vjDhvRRy7I9uljxghJd060ql9jxlhG8IUX2gxwxQppUqhdp1vUzMkn2/FPP23P+PHHbRH18y+ljdn22UyT5Rp8LItqaibl+92pGJTe8cNdX3nFXC8nn2wyFMWVtH7tNcuroJHkDZAqrbRrVZJVdJugoklobN9umdyXXx7OG3juOXuGH31k4c7RyFFu2emni3c7+VKQ8Xc1fB3FJgCcOxmGHwi1Y+E5D7oDW6K81gBzBdO2QVoVwsXcYcf6wBFUDEDGWlAm5iCsCVSxbVVSoUMWTHkZKkyHBrHQuCI0SIATj4AnXoJ/jgPvXlB1OyY+B7Lj7Pd9U2DB89BqHiR0hfm3weGz4OdOBTapQCYDh2AFxv8A9s9nv1OA0QCrIGEZdEqEDgnQtQYc2RD2TYB4f9+Zs+CeMTCqFdAHiIVKQAXs2Yaovhnap8ORVeHo6tAZqO1vy8iwOrpTpsDUqfYzIQH+/NO2n3EGpKRApUrw7bfw6afQrx9kxtjjHgjcDyQDSXle3wGDAF6HahUhNR6CVYEa0OlwWJMGW4KgqkBMwc+vEvbs07F6vVWAdsDjwPHAypXw9tvwzjtWEL5JE7jqKrj3XojZybmjMWuW3esnn0Kn/tBwAHwmSPGgZRAujYFLgOZFPzXPPQfPPw9r1kBcHBxzDJxzDlx+efT9U7F/iarFuFZ+uALujlIjGISXX4aMZvBVX/gLOBN4A6hbyHPkCK7/AAYlAzfl3lZvMhwUC92bQ3ytcOexWbAxBzYGYbMHyfEQLMBCewGQhxmbLcAcoAm2yjQVGnSAdYkRB6wDNgKd4IhR8HVPqF69kDcUwWLBPp71UR7wPWaE8zIduGsJfB8PNN1xeyxQJw2SN4LXELYnQoUAnCGYF2cLZtOAccPg29UwLQbWNAQdCLQOn6dWMrRLh6TxMPdjYDFUqwztusM+XaDPZVYEPcl/rc+Ed76EVgdDw/3svfn+uXJ2dvNBqJgNOZsgYTu0rgMta0INYOJYmD8xfKHYVDisA7z4oG1PXQmt60El/zNJAz7DOpVJWEfXD4v97gHkZFsn9eabkJkJEybYcTNnQseORe8IJEhNhapV4fdp0PM5iL0SAkeBJzhacHkM9AUqFuG8wSD8/TcMH26vNm1g9GjbNmQI9OgBrVoVra1FwRl/R6mwZg1cchmM2xdin4FqifAacC6FHyn/8w/ccBN8XxfinoWcenAM0BP4cSv8XgFUEUiDqhOh+xZ4rAcc0ij3eQRsB5YBo4Pw/kqYVxOo5m/0oHIA2ghytkFyLKQkQFIF0M4Mg+CCFBhSrbBPJsywtXBWQ7jydxh3GGwFxmIzomikpMLbE+G9DTCnBdCNHUfH/v2EiMuBYJx1BgdsgiaVITsJ5s+CNZshuQLW0TXAeqFKhKcRBRADVAcqZkK9eKgZY4Z5OrAZuA37O/RKATYBE4BJOVC7M9x0A/TvD888A3fckedeU2yk/c039j1o1Ahuvx2uuQbmzLHvV7t2Zrw7doTjjoPu3e36g4Ah/jU7YJ3ARf7tZWZCYiJs2GAzgaZNrQ2XXQb16u38vvOSkwM//WQzgi8nQ0ofiLkcgs3t63UucBn2URVlhhjZwWzcCPXr23sHHAB9+kDfvnbfXnGnnVEoyPiXuy+/sC/n8y9fRoyQahwoxUywD+QUP6a+sASDFiMd10OKnWTnOCRoeimRpAWlN5ZLPaZJFSKKwR8o6YIl0v+GSYuXhc/53XfSIYdL3BBOrmoWkBpF1AmovEniDanlTdJtD0lH+IuYd8hCNUfIFvueDko3bJcqppjOzbRiPKcnF9i5X/xZWi4LAa0SlD5bLc36R/plofT1Qumjf6Qhm0wT6dUc6YKVUsetUmxosTpH4VyBoHLnDRT0CvhJcMlS/RSpc5Z0yAqp0VdStS+k2K+l6rOlyluUS1cpfpNUb5LUaZh0xLPSqddIU/0HcM1y2+fYvibodswxppzZzX/G8TlS4jKpY0/zozdtamGk8/wsssGDbS2gQQOTvK5Vy+L6lyyxKJ6qVcPrDJGvk07K/WxTZPITh/htriDzlf8uW+/IzDQNnyOP1L9RS+eeW/xwUcnyDUaMkC64SBqTIV0kKd5fb2mZLj0ZlNYU89xLlti6QM+e4eps779f/LZGA7fg6ygJM2ZKXCHFpFpx9HdV9MXFNZLa/GYfZr0cW0Te2YJaUNLENKuf21PhyAzWS9VHSPs9InGbn1Ql6fCALSJGXvMdScdukxJCRnW7xGipol+p61nlFvYKBqWTr5BYYSJxRQ0Auma6nXfUNPt7tfzKZXmTzKK9gjLpitUSk6S606Umi6QKC2VCb6H9lkt8JlXwO9HjR8uSt261To7xEiujnH+55I2T4t6SEu+SLvxG6vislHiPxIcSs5VrcbzSNunIVOlov1NtfafUrbt02GFStzMtDLePwolUselSi7HScbeYiFpI2//HHy0JrX9/C4m8/nqryhWSUBg3zkJIGzXSv5FEN90U7jyiMVUWSlzVb2tHSS8rnMQ1d650yy3W0SxZYu8tWhROJCsJV/9Piukv4X+fYwLSUamW81DcSmBr15q899q1JW9fJM74O4rF+vVmvE6SfQhHBQpXtSnEkiVS7zOlG1dack980IqcF6QRUxCbgtL1v0ltJ+dW4awWtIiRt36RlubTwAxZmv216VKD5NxfroTfpH2vkgY+YFm1a9dKNY+yzu6gwM4lnCPpPVkiIC2KCEl9MSjFRhjVuFVS2+VSVf+9atukM+dK59xpCU4hIwhW2jIzU/pymNT1IqnDe1Kj+bKoKVlltfhs6bH10uJNFtu+bZt1aF+Mka58Ver2rFT3Jcn7SKo4fUdxt5gcqeY6qek0qfX30tlpUo1xUoVfJGbl7hCqZViy1tH+34/5AnTnrpaqfyHF+J3sUbKEuqLKHv/5p0XihLjvPumnn/KP6kqRVR2LnA1cotyzgX8/m95Wfe3ii+1zLmr5x0g2bpTefFM65AKJx6XETXb92pKu2l68WWNZ4Iy/o0jk5EiPPibFXyJVy7aEnFdU+NC3jAzp4Uek+D6S9499gL1V+GSaKZKOSZYeWiOt8v9BJ02STjhF4gqpoj8SbZ8j9UoK12JFEoukmh9Kfd+URo/PXyZggSwmvl6kO2WLxKdS5WukqwZInGyzjdNV+GzQQyZLbNixXu4/2VL1DIVnAEHJWyvFPSwRb3kPFSpYQZU+fczojxkTvVB5ICAd3Veqd6MUN1a5wmgP3CQ9599fXrKzrWMLStoo6ZyXpFYPSwnPSnwhMSNsvEMvL1WKWeh3AOnaQVQv9Gq2WVb4/jKp0stSTV/vqKmsCEshddJysXFjuPbv/vubbEW0OsQh8psNbPW3T59ugnchN1OnThanX1JWrpRmzrGkut5p4WfUdKP0eKo96/LCGX9HoUlKkqjjGwNJXbPNaL/7rk1L33tPGjJEGjo0dzLLzz9bHPirr0oNj5YYbce3zpJGB+0fOSnJ4qGzswsedX0rqXJkNa4ZyuXG6JJj/2iRp1gWlO5fK7VfHGHAUqR959jIcGlm9OSgbJnrAkkHZkvV/RmFF5Tq/iN539rfh/5uwl8FpfJLUo91Uh3fCbx+gzRkvs1KQgaJ7b4h/Urm3pFUP1O6dYv025KCzx1JWpo9w+xs6aaFdp4Kv0uNI9xDtTdJF22UxgULVrgMBi0J6scfpV9+s7WK1xdKXCvxgsQoiTQVznUVem2VYmZLtfxOIEHmny9qEtP27Zal27GjWas6daTffy/4mNBsoIvfloqy2cAfsu9MSooljR10kHWyks0QpkyJeroisWGDdNdTUr2HJP72v0tZ0vHJ0jfa9QVgCjL+LtrHkYuvZ8Pp9YB6QACOjoVEYPxYyE7H4v0C9rN1Cziqp4XsD3rdD608BDgIyIamS+Hy/SA2B+6/xz82J3yOU06Es/tCZhr871Y7T+hFAJocBpOOAVrkbmPDSVBjOlzfEU7uAZvWwZuvQmIsJMRCbAVY0RTWdISl7WBtKNJlClT5BQ5YCcfVgEsvhhYtYONWOC8HxteFm9bAYRnwW3X4pRrMiIyS+RkqvwWnVoIXH7Nojbz0AOKz4Y+bIac/cKCFBvYDKn8KddbCU4dD5sFw2W9wyuEwKMZCQsmBJlPhlopwS0eILUTUx6JF8OUw+OsuGC34PQh1Y+GRqfDeeiyUKhES0qFHMlxSB3rHQp2dnFeCli1hv/3gkUfg3sowvjW88Bv8eAx8JQh+CByJBcEXJkLFj1pqCJwBXINF7sQW5lBZBM6bb1qcf/XqFt5ZsSJ065b/cVOxSKGPsTj6Tlik0IVAdUEgYDH4n39uMfhdulj00bnnQuXKhWhYAe2dPh1e/gmGV4P4y2BzLNTKhCNWwIPN4YCE4p+/sLhoH0ehWS5LvT86KPUMmk55V0n7Z0n7ZUltMqWWmVKTTKlJttQ4KFVO9RdU/YVNL2CLYOX+pQm9gjIfeU7E6DVgo9ka2VKdNIl/FK4IttpGbe2SwsXEc73SJb6SxkUZKTbPkU7NUljgbIl092CbNW3zFzv+nisdkW1yBu/6x41ZKHX+QfL8GU/8cunsyTsX/7r/fhsRv/WlSUu0UVhPac0a6aV3pc4PSzHvygTwZNftnC7dn2Hyy/lNwgYONP94MGgVzZCJ4zWW1HmVXbftVKnWFmn/wyQOkg56ytRWj18vMVE7zf6OD1rU182SPpDJHhTWxdazp7WhWzdz3xSkq5Mi04eKnA1cqvBsICnJZq2hGUa1arYwXZgqXzsjGAwXgGk4Uf+u19RdJN0yT9pchnrPOLePoyyYMEFqcYHEZPuQDlPu6lVB2T9yhmzRNEmm4rhetpC8XFZecYGsVOOp90rcI3nL7Hz7yHTiv5e5eb6R/QO9lyqdOkVq+LfkrQ5/SapnWAnDq3Kkh7KlRwJmiO7JkW5IlvonSedtkbolSXW3mLY/MhdP4hYpdpaU6EtQN9gqHZUhHSdpn5USv1sYpheQam+wSl+To4QCeWlSm6/NiPXeaguyZEi8Id3zTtj1tF0m04ysGEiILWlS/x+lqv4zjZfULyC9OT+6kc7ONuXPqlWlIStNNO2SKPulpEjfj7PP50FJ1RaEn1vNZOniFJOIyE9gbKq/7/3+z+t/NsmGg4PSibJOyPMs9FayOgmvvipdd710WB+Jw/TvegDD/M84J/o/eyXZoOMGWaczQ9HdJcnJ0ssvS/v4ukONG5sa6c6YInPFhRRGO8nWtLbKDPVvv5l8xQEHhN2TkyYVvN5QWLKypE/GS50/lmLm2PVjMkwq+wcVTVKiMDjj7yhV1q6V+twoMcQ+nNrbTaOluMETjz0hxZwneX74ZbtsixQpzD9CVpb0+STphK+ko9NNshdJXrLUfJJ0zV/S/C3Rj82W6a7fJanx1ogvmx8N1OZnG7Ft3y61ayfV72oLxC0VfQEzxT/+0BFWqvD88yVaSpU/kuJybDbkDZHOGGhlDDNksywUXa55RpbViK3sd1KJS6Vz/pBW5BmNLl9ugmKdO0v3+KPKITt5br/9Jl39oFTvbonhMjlmmY7OKZlmiEKf56ZNJswWK9PyrybrJAJBW8u4wd8vtJ6TH9u2mW993DiLakqsJHG4xOeyNQVJiRm+HHWemWMFSd0k/RrlvIGA9O23loMQMv5JSeH6wvmRLJsNdPavEZoN/Onfe0g3KC3N8hJq1TLhuvnzCz5vYUnbLj0xTjppaTgCK361dNh4aXQpXcMZf0epkS7prOlmLGKzpLuyihYKGWL1amlLkvSFpOa+u6Vtlkk8l2T0kyrpkRlSq3GSFyoiEpCqzJYeyC642PZSSc9nSAevD+cUJEjqG5RaPiLRQOpynRUt6SF7FpHM8Qve9B1pETuJiRaqmJZmM53+2yLKJY6Qjr9HmjJLOkv23hP5tGttknTRD1LFaf6x26V9/5J+SA3fy/Dh9t/86JM2A6uqcJnNgggGrZThQ09Jd/wgnbVWwk+ua50qnTRUqtnAjPp+kmKD0rHLzDCu89v9UsGXyJeQm2bhQsmrItW/V2q61c4ZlyTxjsRVEv+TLg9aeOmYQtyPZAXkwWpOjxy5cwG4yZL6Kzwb2F/Sq7LZajAojR9vxW9CMtNHH20BAKVFuqSX1km1/w53fJUmSRcNLXqFs0ic8XeUmN//kG75OVyirleqadcXlXXrpJtvleL6SfV8I9NOlula2q7P7Bxp8AzpqHFSnUXh4i8VN0utx0u3/CgtzucfK0nmQkJStchR6CTJm22/H70ud0cyfL29f84E6dZbrbpYXjZJui1FSvRHurUmm6rj+f75H1D+nVMwKL0/VWrzgxTjS1V3kjRgpbQhw7Jb09IsF6OGLPa9qElHixdLV98kJV4ti7KSdQaX/GNrP0jiKOmLL2wUjqTB6y17t3u0qi+FIDnZahO0aGEWqUE/af/51tF4QenYlNwzkcKwebP05JOWbQyWcfzCCzuP7U+W9JbCmvsVJV0my0QPyma9jz9ubZ3kFwFaujScSFYaTNsonfqnVGGFxG3Rv0eFxRl/R7HZsEHqc5+sjKKkjkFpfDHOs3Gj9L87pYSzJabauVpmmruoDNe7crFellncanJE3d10qdpv0nm/28g/kmyFjfINku7dLlWaqh1cEi+tlK6/W7r7NYVDUkdJdY+3kMJorpBkSQ+mSXV8N81BGVJt/7ncrZ0bui1Z5rI4KNSWVGn/idLIFebff2ervX9XcR6UzCC//obU7HK7F2SGGElNzraR9Hv+/T/6mVmSffYp5sV8cnJsBnPEESbN8PdaaaCkusHwIOFVFU1zPzvbOqqePa0oTIj1hSijNVnSVTKpZZR7NhAIhDuSyy+39Y5evWyWUZDrqygEJS1eWbJzOOPvKDKBgPTc+1LiIIkcqUKa9GxG8eKUg5J6PK5/456bZ1oFrl0d8xxJekB6Y4HU40+pwvLwF22/gNTuK+nuUdLy1dYxhTqAx2T/8BslfZA35j1T4geJO6Ta74RH9nwstTxeGjYs+qhzu2zBt34oY9nXHTp9qfnUd0YgIL36l9T8B/3rt4+ZIjV7ULrM7xi+L8FzCgalo46SGp6tXHH+B6+RzglYJa6+/czF1a1bCS6Uh8iiPUf3kg54Rto3ya5dVdKNkorqFg9F7qxcaZ3LySfbAnVhZgNvysqIIluQvlzh2cDKlaZbFcrMbtzY3E67A+Vi/IEHgdWYKN904OSIbQOARcAC4MTCnM8Z/11HtqS7lktsMsN/zmZzVxSFbdukhx6W3l0TTr1vmmnRGyU1+oGALRympdkINeTPTU21f8SlS03HZf5882eHUvxXr7a0/l9+McmAH36wLNrMTGleULp9jdRquW/IJbFVqvC9dMS3Uhe/oEy3adJVQ6SeD0rdnyzgC7tQqrVQikmXyJLqDJVWFbCYkSVpUIZUb3P4HNUmSylFeFhzV0snjwpHkSSkSzUCUs2cktWLffNNievD7aowXsLPso7NlipeLdVtbIJvpU0gYHWG69Uza9XyHKnbQinB74hOkEWBFWX2uHmznbN+fTvnfvtZla/IOsL5kXc2cICsMHySbMQ/cqTNAG6+2fYPlaUsbtGZklKQ8S+zJC/P8x4EUiU9m+f99sCnQFegETAOaCspUND5XJJX2ZOdDecPgj/6wZq6UGcWnPsXXHQAdO0K69bBCy+Y5G0gYD9zcuCii+Cww0ymd8A9MH8xzGsBwQHAIVAvA26rAM1nwVPPQMCD7BjI8SAnBq6/Fdp2gInT4K33IBBrr2AMBOLgimugYXP4ezp8NRor6hLv/0yA3n2hck277oy5ebbHQ/sDwUuEdVtgc3LubSRAbEVrU6kTgCYD4NQn4W0P4j3onw7Lr4eHb4VOnXY8JAh8ng1XZ0NyJVNjfhRoOwF6HQ6xhciIys6BU5+A71qAdy4oHuJT4aYl8FBHqFxErfv166FbENZsgwot4OwE6P0dnN8FMioAVSF2PezzPfx6AdQtRlGVnZGZCZ99Bi++aMlTjwwCroI3sRFmK+A64HJM5rmw5/z8czvnjBmwZAk0a2bf7Z0952TMiL2F1VWohEk9X43lOSpoNQUmTICjj7ZkwquuskIuDRoU7d5LQrno+Rdg/AcASHrC//s74EFJfxZ0Pmf8y5YgcNoWGFXLf2Mb8CcQD632hbpNICUD5i3iX6MbMqAVq9vv6TmguLJtZ0wOxAYhNmA/44JQvRJUiAVlQGaqvRcviJP9bFgXKsVBVgqkJ5vNj8cylxOAJg2gYixkpkD2dkjwINGDCjEQ58GWSrAgEf4WLI80bF8CE+CeK+HxA/33roMbGsJdt0CdqlaEBKxQ9QPAJwK2gZ6E8zfD4/dC8yhlooLAxVhmKgAbofZH8GA96N/PqnAVREoKdO4M2+Kg6iew2G9fzFY4Yhm8eRDsW8hn/g/QFngyCD94sM2zAitVgaMWw6YPYOXZsKYTeOlw6EJ4oQV0LUYxnJ0hwa+/mgZ+9eow+GN4ZxNsvwymVbNs6guBG8i/ilq0cy5YYLUEAE45xSqa3XyzDWoK0tcXMAXrBD7FitAcgHUC5wMVs2DkSHjrLfjxR8sm7tMHXnkleoZ4aVMuGb6Y22cZMBN4D6jpv/8qcGHEfu8CZ+3sfM7tU7ZkyUIEWwekdjnS/jmmodMjYNm+J8rE2fpKOkema36FpP450k2SzpZU2XdZVM42vZxnZFPiQbKF1k9koZ1fSRoti3L5RRZXPUXSTJkfd4mklTJXxRZZ+GaWip9HUJosj6ZvkyYdHpTqh5KWfpBqHWk6SHm1gKZLOiHkVlotxV4v3XxH/m6Bx/1r1AjVMU6Sqr0iPbETkTNJmjrV/PUrVkq9/TyDGhMlz19kPjooPbZIytjJg71+oy32PvSOdHWKxdwv99v1asR+L42X6n2tsLDZtPyT00qLl16ybFyQ9r9IOmaR5Sog6QhZ6HBBukZ5CQSku++Wata0cx58sOUO5CcQGMk2SW/Iak+E1gaukDRR9gzmz7c8gf32C7si//orunhfaUFZ+fwxl83sKK/TgfqYbEcM8Bjwnopo/DEZjsnA5GbNmpXdE3IUmawskyyoe5Z0YJJ9YI2CZuwL8X+yR5MjEylDUvWZEi9JzbIjvrjZsoigYdJpV0X3Jf8qqYtvJCuvsVDXgKJHijzvn/fIoHTYGv/c26X+GWaEC1qwDG3bpLD8w/wsyylo6F8/dqN03N/S9Cha24GglLBcqvKnWYtLxtsxH/ht+iSK4Ro/W+r8jf5dvD5I0keSUotihYtAKNO3dWtr43H9rE5DS7+NjWWZ4kVZ90hNtbWO9u3tnM89V/hjg7Ki9lcqvDZwoGxhf5vCn0kgYCquiYmWUfzrryWTmY5GmRn/wr4waa7Z/u8DgAER274DDt3ZOdzIf/cgO9uqDTU8W/+Gf9bONOncUsh+32OI7ADOn2f/8HMl3blBarla4XDQgFR3mfT8GmnGMitQEvoHD8oUTDv5I9X90qU6F1qIZV6dmtf8a/WSNC0onZ1ikTZxQanhKOmmV6WtW6O3ddMmqW9f6a35ueUfklOlW8ZKNX7Sv5XDGs+S3t8aXpR/YoJd98rfLJSzx7X2d3+/PbSw0WujRjsayLVJ0pObLDkMWdJdz2+lSYXJPisGgYD0zTdhiYlNW6WTX5d6+DkRCZIulI3EC0swKH3/fbgIzGefSRddJE2eXPBxIfLOBirLOoVJss9/1izphhvCs5cOHSxbubQoF+MPNIz4/VbgM//3DsAMzOXaEqtFHbuz8znjX/4Eg1KHKyW+sw+lRob0fDB/PZj/OjkyQ4pML2ddxNByi6QnMqXa6RFf4myJ8VLNh6VzH5BGfmUx+QGZHEPjkDtogtSkn/T557lHgu/IEtWOkbnClstG/zEZZrjjvpQufzlcISvEpk1SkyZmvO/yR/t55R9GzZAO/kqK8bOiG0m6Zr1UY7TkpUubs6Xbb5fiKlmH012mjVS5mi8DjvREPinKAUlvr5TqT/fvL1VqOlJ67fvSi4mPxnffWY0EkHpeIZ22VKrqd7RdZeHGRZ2lvviiVKWKnfOww+wzKsw9hGYDV8jcQaHZwBuyDiI1VXrnHemQQ/4bxv8jYJbv8/86T2cwEFsDWwCcVJjzOeNfPoRGU7/nmN8fWTWnZ4JSISLj/vPkyPRgkMSP0jHXm6bMihWm3/PPP7bG0SDkFop0Dy2UeEH6dLqdK1PSq8EI//5I6aCLcq8HfCQbvfdUuCLaOkmXr5Pi/dyCmDHSoNm52/nrr1JMjHTeRdJhwfzlH7ZnWXtPCoZnL9387NVffjGL0WyrVF9S/D/SqadaZwPmetkZ4zdIB0+TrQsEpJMyrI5ydhmFQm7cKD32WDgGv83B0rPpljCGpLqyRLKi5FIlJVm2cKtWds4TTyxam7bJXEAHKPds4G9ZJ1Garp9yd/uUxssZ/11LMGgxy/ucK+EXNKkj6SkVT8unPAgEcvvb1661ak5//mmFS7791u4xxKjR0iNPS3c8IfV/TDrvUen8l2xheqSkM76Q9nlGavqCVO9VqfogqcGIcBJYrtd6iQ02at7Zl/u0PIYvVaZIWjHTFlovlmUf//OPbf9c5vLppnCVKvm/37zBCtAjk6d4cqq02Dfejzxi//HPfL5z+Ydg0ATzTvtVmuePbHNyLDZ+/+k2+ucr6ZVXLK8CbIG7sKzMlq5cbWUPkVRlntTtRemHCaXv95ZsgfXjj602sGRGdsA46fg0m03FyjSWflbhF6hzcqSvvpJGjbK/k5NNBnr27IKPCxGUuaBCs4E4lX7Vr4KMvyvm4siFBGPHwu0fw7x+wGlQJRMGxMNNMVClhOfOyID0dHtt324/27SxohxLl8K0aeH3Q69rrrGwvrFj4csv/WPTITVgBTreHAJeVfs5dBRkxEFWAuQkAlXgnidgeyx89zvMW2Hv/fuqCk3a2Xm2FTFUNTYTErOhfhU71YZkWF8tYoc1cFYWdGph21PWWgz5LzXgFz+G/5oAnDYb2jSAhg1zn38z8BTwCpAThJzXoO88ePZ/MLMlnI0VJ/keqB1xXBrwDvCMYLUHTIYj/4BXjoWbb4SVK+GReXBeHNwFPFn4WyY9HV6tCHcCvAYLT7B4+U6dLGb+7LOLcDIgHRgcgIGbYWs9YBXU+wxuqwbXnGOfe1mQlGTPOysLTrwaatwDYxvDVs9CRG/AQjWLUs/lxx8tTDQjA447zkJFTz7Z4v13Riiyulcx7qUgyiXOv7Rxxr/sSU+Hr5bDefOBMyAhFfoshftqQIemsGIFfPppbsOcng433gj77w9//AEDB+5ovId+CZ26w+Av4eZ7yG18q8D9z0Dt5jB+Inw9fsftXY6GQEVYlQSbM0BVQJWwOLJC4Akqe1AhC+IzoVIQKsv+sasCjWvYpRKz7WeNOKjq7dCMXK/K5F+B6jfgwmRYXg1OBl4WVNmQO657DnAbZrgTl4L3P7i/C9x6C1SokPt8q4GBmfBhHCgdYl6Cq5LhqHvg0uoWgz8OK74WSSbw8lZ4LADb6gBz4cCx8HxXOLqnxaIP8ttwfOEeJWC5BxcCJ86DMe1g2TK45x64/XarhFUcgsBXmXDvZpjbCEiFI5fCO52gVbBwBrSorFgBr71mlcG2boWDesApH8M3LWxRsgZwBZY81qqQ59y0CQYNsvOuWQP77AOTJkHNwmaelTKukpdjp8yUdMwW/2FnSAyS6CbRRbr7O+lrSffPk7hS4hYp9kEp8UWp0vvScass/r/rVqnaTKnKYqnyWqlCkhVAKcoHHRuUquVIDbOlNtnSwTnSUUHLMThX5hu9RdK9kp6UxZkPlvSlpLGSfpPF0i+S+cJTVfoFMgpDlixEs4qk+Gwp/hHp0WfD8d1SONqnZWihd4zU+HgTN4vm+lgg6dQIDaBaj0rf5JjyZDuZbHQ0siW9tU2q66uo1k+TXsmRPvxWai/z3xc2DDIYlDo/Yee5tpDHFJXpknpvtCpfnqQDFkvtr5Y++LB0CqrkJTXV5B06djRNoaCkj5dLp6ebK8aTff++U+G/S1lZprJ6/fXh9z7+2GRHdiU4t4+jID4ELinGcb5XhaoUPEouyiuBwpWD3VNYDVyTCt9WAf6BJk/B4PPg2GPD+2QBrwP3ZUOqB7wBQ9rABfn4AKYAN6fC71WgCXBuEF4JQJM4+MmDpvm0RcDQNHixEkz0gLVQ+wdIvhCO9WCUV7jJ1L4vwMJb4YxsGBG/8/2Ly1rgNeClTEhNBP6GKoOgf2247ipo3bp0ryeFs3l79TJphjOuhxp3wsj6sB6bZd2A/b9Uy/dMO5Kaam6mtDTo3RtuucVkHwrKHi4N3MjfUSDTZCqJd8mSYV6Q9LYs8egbST/JIhHmyaIitqpoWZMO06Nv7C/E8rl02X077rNR0jUBKyRTM2hFUn6YsGPoZogfZYu+SGKplZCsmyIt2snwNCjpuyypwi/+sb4a6EXzC7fY2mqERI7UxtdW/uQTC38MLUiXNmmSXg9KTfx2slxq8Wp4sbssFojnzpWuuUaqVEkC6cgTpAEzLcQV2YzuelluR2FZvVq6916pTh07Z8eOJjRYluCifRyO8idD0oNZUlyWVCHb5C9SM3O7giRplqx2MJJi50uV+lpoYbQC5UFJw4NS01DHki3FbZbe/23n7Vm8WKp8lFRxrH9sUGo/JX/3UYjqEyQ2mKJntizTG6xub1kSkA1GemSEDXD/7VKLY6zASmE0+ovK5s3SU09ZUZgnn7T3/syWzs+0pDFkn9VIFV5ZND3dIqMOOsg+g7LEGX+HYzdiicyHjKT6G6Sm51sWaSRB2TpL89CaySipRS9p9Ojo58yR9G5Aqh6qI5ApfVIIa/SZX4jlvNekhGzrAOIl9dsiPTdixwSmzZsl5krx0+06s2VVuCD/DOOyYKpMXyouaLMQvpBiD5fOPa9sZBKys61GsSQNGSJVrSr1HyjdsdlkM5DUXBYKXYZSPUXGGX+HYzfkK0UUcRks9b7CksMiyZT0bNAv4p4l8aL0UwGFyTMk3ZEdrrp1VJZ01r0Fu2QefFD6+29pgiyBrK1sVE+2VGW49OAXYWGzz76QyJAO9KuODQlIDz9sliTazKSsWSVpgKTqvqhe7CSJs6WFS8rumjNmSBdcYPV8PU86tY/04EwLTEAmfHe5zJ1a3jjj73DspqRJujNbis2R2CrF3ywN/mjH/TZIujJHiglKtWRRTsO/zn+0PVVSNdlInhzJe1e66J6dFwMf6BvRlwPS6YukGL9zqjBGGjBcuvlF+/u1HJsh3CXprrukhITi3H3pkSrTP2rtZyU3k/ScpHP6S9deK82cWfrXXL1aGjjQfPjt29tsY5ak/sGwhMNhkj5T+a2ROePvcOzmzJN0mG9o26WYDkw0zZgZMm0fJHlzpGpnWZhiNEnoJTIjmBD0ZZzTpbiXpDuejL7/7bdLx56YW/5hY1C6YJEUl2zXbO9fe4KkTjlS101WI/fGG0vjKZScgGxGdaSsnfHbpdiXJFpIhx9ui9OFkWcuCtu3m1yzZFm+bdpIdz4uPbhNau23o6Gkh1SyimrFwRl/h2MPICiLsGooiy3fb4J0yoXSsmU77jdSUtPQesDXUttTTbIiL8sl7SPTjzkyxSKJYlOlhyWlKLdvfNAgswh3vb6j/EOypCdyfE0fmRHrOEVi1a719ReFyZIukK0LeAGp8miJ7rY4XFYsX256R55ndYIvvFh6caEUEjD7tewuHRVn/B2OPYhtskS2mIDEein+KumhR3ZMcMqQ9FRQquivB8S8IM1Zs+P5VsuSwCpJelfS6b5rpHaOVO9R6f1PTAcpGJT69ZNiY6VH59s+d+U513ZJoQCVG5faPq9+VjbhlqXFStl91PB98p0zTR/py5FS7962iJ636E5JWbjQZkMhBdC5c20mtqsfkzP+DsceyHRJnUOj+1+kpidLkybtuN86SZf5i7y1ZYqR739kLogQ6yV1kpQoq6L2l6RDfFcOS6WmA6Vvx9govkULKzJyiX/tPIFI//K934nUPEvq2rVUbrlMSZH0isKumDopUpX7JKqaQudTT5kKaGmSlGSZveWFM/4Oxx5KQDZar5YpkS1duc1cMNFG2tMkHSX/H2amVKufNHhweFS7SdLBsvj0kbJR6NiA1MIvv8ksaf/7pN9+l2rUkEZPKFj+YYN/Le926YgjSvOuy5YcSSMkHS5rf8UsqclnEs0s8eq/REHGvwzkkhwOR2kRA1wOLEmAq2Lh3WrQTnDIM/DwI6YgGeJA4EdgGNCoDWwZCpfWgAPPhj//NOXP8cBBwFlY/fkTY2BxLfgkG+o1hpkPwx094JPVcGBb+AxIwuQMgnnaVheolQHqaOqYewqxwBnAL1gh+jPiYe05ELsM6oyz99LTTX5h0CCTZvgv4oy/w7EHUBsY5Jnsb13BlDvhgW7QtjeMGhXezwP6AosrwBNBqHAyzPoUjpkKa7ebUuX3QHfgXGAIZgTOi4fVNeFtYCVwciVoNAMuvRL6z4exwEtR2tUlHuIPgWbNyvDmy5BDgE+ApcBtHkyrD92AnsDSg+Dqa6FxY1OunTu3XJta6jjj73DsQXQDpsSYxn/lo2HVaOg9FU4+C9atC+9XAbg7BpbGw8UeZF4HB1SCN4Lw4RswfDscBVwMvOcfEwdcCfwDPJACiYfD1HfglWnQfCHcJZiapz0HxUL2PlC5Rtned1nTFHga6/heArZWhOXPQ6M0aPsqvPUJdOgAs2eXbztLkxIZf8/zzvY8b47neUHP87rk2TbA87xFnuct8DzvxIj3e/nvLfI87+6SXN/h2BuJxZQlF8XDebHAffDDi/BLFJnJBsAH8TDZg3bAdTFwYw/ocDlc8gWcKNOsfyPimIrAg1VhXWU4bRZwGixvCTnpcJas8E2I/QESYVa2FeLZ06kK3IR1gMOAlhVg8kVQcSOcMB2qdrD97r0X7r8fVq0qt6aWmJKO/Gdjs8xfIt/0PK89NqvsgBWned3zvFjP82IxldaTgPbAef6+DoejiDQAPo6Fn4C2jeGcSnBaEI65FL75Jve+BwM/A18ADfaFjZ/BJXGw7gI4PMkKlryQ5/w1gK86wc2vAG8BCbDUg0MFQ0ZZVbID/H2nZMOwYWV1p7ueWMyw/Qb8BZwUA+MPgNYenCeYGIRHH4UWLaBvX/jhBwjmXRTZ3clvJbgoL2AC0CXi7wHAgIi/vwMO9V/f5bdfQS8X7eNw5E+mTFSsYkDytkvcLZ10evTiIemSHg1IiVkSGVLN96TT/Rj4J6KcOztbOuwwqXInqX2oAH2qVP8FaegYKTEoNfnkvxcpk5dlkm6XL5shqUu61OdDqXY9i5u8997ybV80KIdon8aY+yzEKv+9/N6Piud5/T3Pm+x53uSNGzeWSUMdjv8CCVhd3fkxcHoi8ASMfRr2uwEeeMDq7IaoAAz01wMu8CDpUvjDgy4BG43dm2OFX0LExcEnn8Bnj8OMODhQEFMR1t8C5xwAmamQ1c784UuW7Lp73tU0B57FDNgLwMYKMOIiqLYOLp4EfS8q3/YVlZ0af8/zxnmeNzvK6/SybpykQZK6SOpSt27dsr6cw7HH0wwYEQOjgeatIHsMvHwobIhSmL4hMCQB/vasQtXkWGAzPBYHZy+EYEQP0KyZVaCKA15bA9ViLOS0pYCqENjX9vvqqzK+wd2AasAtwCJ8N5oHQw+BJm3LtVlFZqfGX1aroGOUV0Ef82pyV5Nr4r+X3/sOh6MUOQmYFwcPAuknQodYeCINzr0QFi3KvW9n4FdgKFC3kr03rC00+Q5m5YluGTcOjmwBV0+G+R6c3cg6mmcrwSmnQGJi2d7X7kQcli/xBzAfy3vYkyiVGr6e500A7pA02f+7AxY+2xVohOWWtMHCkBcCx2JG/2/gfElzdnYNV8PX4Sgei4AbsVj9mJkQcyPcdTjccw9UqpR733TgqQA8AgRjoeo/sLINVPe3Z2TAoYfCypXQaxl8XMXyBo7fdbfjKAIF1fAtaahnH8/zVmELuaM8z/sOwDfmnwNzse/c9bJM9RwsSu07YB7weWEMv8PhKD77YKPzYUD9DpDzMzzWAtr2gBEjcu9bEXgwFlbEQrscSGljUUVPb4EXX4HYWBg61DqBZX2hveAiYIN/fCAAmzbtuntzFJ9SGfnvCtzI3+EoOanAw8DzQSAZOnwM066PPgoU0B94J/TGVGj+Arx1IWzYABdfDNe+Du9dC0cDo4DOB0GrVv+tsM89mTIb+Tscjj2LKlgm64wYOKw6zLweegAjV8DAgZCWFt7Xw+QeHvP/TjgAln8EvbbB+z9ZfHvTJHiesPxD9+7w3Xe5NYccuyfO+DsceyEdgAkefITp2vRtAo83gH272ag90iFwD2bgs2KhbRDi+sBPr8Piy+HGAXAtcDpwF9DhIutAxo/f9ffkKBrO+DsceykecCGwALg+BmJugPUT4KzhcMKJsGBBeN9bsdT8hTHQMx7OEMw4xaI4LvoBVp8M9QQvdYcqDWDkyF1/P46i4Yy/w7GXUwMTipvkwUG1gY/hp/vg8ZG597sO8///DCRXNPno5sDHx8PkhyH5KVjiQZ1PTV5ij5M72MuIkvrhcDj2RjoDf3lm4O86DD7paZE+x/0JSavgrLPgCg8SMX3/LMzX/00QrmgKKXfbm8uOgis+ze06cux+uJG/w+H4lxgswmdhjElBPw2c0Qb6fQrHHQ/z5pmr6DNM8KwXcGoMLI6Hqi8AATvPkB6wLLZ87sFROJzxdzgcO1AXeBdTtdynNjAcfrkTOp0Bd94JvVKsEthULGOzUi0Y0xVi2kOnWZCZCE+WX/MdhcAZf4fDkS+HAVM8EzJLPA6YDc9UhPc/tQifr4A5WJx/m8Pg72EwoyNMBwaWW6sdhcEZf4fDUSBxmJDZwhg4Ox54CF6+yvz9sd/D68tNQuJIoMHB4HlQeRFkLijgpI5yxxl/h8NRKBoBnwI/AHGeicedJeh/FJz2OqySdQDLAnDSSXD22VYI3bF74oy/w+EoEscBM7DM35wTwJsHQ5dCYh9YkwVHx8DAd2DWLLjttnJurCNfnPF3OBxFJhHL/J3rwckVgGcg7TnYfhdszoH7joQrn4I334Qvvyznxjqi4oy/w+EoNi2wRd+vgPqtgBegVZzptQ+7CVr1hiuvhGXLyrGRjqg44+9wOErMadgsYKD/U8DWCrDkXWB/cIX4dj+c8Xc4HKVCJeBRYBa2LgBAPdg2Ag6/BaZNK6+WOaLhjL/D4ShV9sWqe30GNBRQG6Y9AwdfC8uXl2/bHGGc8Xc4HKWOB5yD1fm9FYitDhV+Ba9JOTfM8S8lLeN4tud5czzPC3qe1yXi/Rae56V7njfdf70Zsa2z53mzPM9b5Hney57neSVpg8Ph2H2phtUCmOrBNfFQzen97DaUVNVzNtAXeCvKtsWSDozy/hvAVcBErLRoL2BMCdvhcDh2Y/bHJCIcuw8lGvlLmiep0Encnuc1BKpJ+ktWPPhD4IyStMHhcDgcRacsff4tPc+b5nnez57nHe6/1xhYFbHPKv+9qHie19/zvMme503euHFjGTbV4XA49i526vbxPG8cVtMhLwMlfZXPYWuBZpI2e57XGRjpeV6HojZO0iBgEECXLl1caQiHw+EoJXZq/CUdt7N9ohyTCWT6v0/xPG8x0BZL/Itc72/iv+dwOByOXUiZuH08z6vreV6s/3srrM7zEklrgWTP87r7UT4XY5nhDofD4diFlDTUs4/neauAQ4FRnud95286Apjped50rODPNZK2+NtCdaAXAYtxkT4Oh8Oxy/G0h1RZ7tKliyZPnlzezXA4HI49Bs/zpkjqEm2by/B1OByOvZA9ZuTved5GoCyVQeoAm8rw/Hsi7pnsiHsmO+KeSXR2h+fSXFJUTdU9xviXNZ7nTc5verS34p7JjrhnsiPumURnd38uzu3jcDgceyHO+DscDsdeiDP+YQaVdwN2Q9wz2RH3THbEPZPo7NbPxfn8HQ6HYy/EjfwdDodjL8QZf4fD4dgLccbfx/O8ZzzPm+953kzP80Z4nlejvNu0O5Bftba9Ec/zenmet8CvQnd3ebenvPE87z3P8zZ4nje7vNuyu+B5XlPP837yPG+u/39zc3m3KT+c8Q/zA9BR0v7AQmBAObdndyFUre2X8m5IeeILFb4GnAS0B87zPK99+baq3BmMVeJzhMkBbpfUHugOXL+7fk+c8feR9L2kHP/Pv8gtPb3XUtRqbf9hugKLJC2RlAV8Bpxezm0qVyT9AmzZ6Y57EZLWSprq/54CzKOAglXliTP+0bkcpzbqyE1jYGXE3wVWoXM4PM9rARyE1Svf7ShpAfc9isJUJfM8byA2dft4V7atPClmtTaHw5EPnudVAYYBt0hKLu/2RGOvMv47q0rmed6lQG/gWO1FCRDFqda2F7IaaBrxt6tC54iK53nxmOH/WNLw8m5Pfji3j4/neb2AO4HTJG0v7/Y4djv+Btp4ntfS87wE4Fzg63Juk2M3w69Q+C4wT9Lz5d2egnDGP8yrQFXgB8/zpnue92Z5N2h3oIBqbXsVfjDADcB32CLe55LmlG+ryhfP8z4F/gT29Txvled5V5R3m3YDDgMuAo7x7ch0z/NOLu9GRcPJOzgcDsdeiBv5OxwOx16IM/4Oh8OxF+KMv8PhcOyFOOPvcDgceyHO+DscDsdeiDP+DofDsRfijL/D4XDshfwfKWEoirmAATcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data visualization between Test and Predicted data for the time-series data of a few sample datapoints\n",
    "\n",
    "y_pred1 = model(X_physics_test)\n",
    "\n",
    "\n",
    "# Reversing standardization\n",
    "y_pred1 = y_pred1 * std_y.to('cpu') + mean_y.to('cpu')\n",
    "\n",
    "y_pred1 = y_pred1.detach().numpy()\n",
    "\n",
    "plt.plot(y_pred1[0:50:5, :, 0], y_pred1[0:50:5, :, 1], color='blue', ls='--')\n",
    "plt.plot(y_test[0:50:5, :, 0], y_test[0:50:5, :, 1], color='cyan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42fcd1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PINN.eval()\n",
    "model_PINN.to('cpu')\n",
    "y_predict_test = model_PINN(X_physics_test[:1000, :, :]).detach().numpy()\n",
    "y_predict_test = y_predict_test * (std_y).detach().numpy() + (mean_y).detach().numpy()\n",
    "y_test = y_physics_test[:1000, :, :] * (std_y).detach().numpy() + (mean_y).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d04fc967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data visualization between Test and Predicted data for the time-series data of the first 10 sample datapoints\n",
    "\n",
    "y_test = y_physics_test[:1000, :, :]  \n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(10):\n",
    "    if i == 0:  # only add label to 1 data point\n",
    "        plt.plot(y_test[i, :, 0], y_test[i, :, 1], color='cyan', lw=2, label='Test')\n",
    "        plt.plot(y_predict_test[i, :, 0], y_predict_test[i, :, 1], color='black', lw=2, ls=':', label='Predicted')\n",
    "    else:\n",
    "        plt.plot(y_test[i, :, 0], y_test[i, :, 1], color='cyan', lw=2)\n",
    "        plt.plot(y_predict_test[i, :, 0], y_predict_test[i, :, 1], color='black', lw=2, ls=':')\n",
    "        \n",
    "\n",
    "plt.xlabel(\"C_A\")\n",
    "plt.ylabel(\"T\")\n",
    "plt.legend()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c036b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
